{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Lanczos.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "zzzB2uJwBC-m",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8e866ae0-62b4-45ea-8732-ab3fdef0fe19"
      },
      "source": [
        "!sudo apt-get install libmetis-dev\n",
        "!pip install metis\n",
        "import metis\n",
        "import random\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from scipy import sparse\n",
        "import scipy.io as sio"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "libmetis-dev is already the newest version (5.1.0.dfsg-5).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 30 not upgraded.\n",
            "Requirement already satisfied: metis in /usr/local/lib/python3.7/dist-packages (0.2a5)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j2yJoBMsBFa9"
      },
      "source": [
        "def mask_test_edges(A):\n",
        "  A_triu = sparse.triu(A)\n",
        "  edges = np.stack(A_triu.nonzero()).T # all edges of the graph\n",
        "  num_val = int(0.05 * edges.shape[0]) # 5% of the edges for validation\n",
        "  num_test = int(0.1 * edges.shape[0]) # 10% of the edges for testing\n",
        "  edge_ind = np.arange(edges.shape[0]) # indices of the edges\n",
        "  np.random.shuffle(edge_ind) # shuffling the indices\n",
        "  val_edge_ind = edge_ind[:num_val] # under 5%: indices for validation\n",
        "  test_edge_ind = edge_ind[num_val:(num_val + num_test)] # 5-15%: indices for testing\n",
        "  train_edge_ind = edge_ind[(num_val + num_test):] # over 15%: indices for training\n",
        "  val_edges = edges[val_edge_ind]\n",
        "  test_edges = edges[test_edge_ind]\n",
        "  train_edges = edges[train_edge_ind]\n",
        "  # the incomplete adjacency matrix for training\n",
        "  arg1 = (np.ones(train_edges.shape[0]), (train_edges[:, 0], train_edges[:, 1]))\n",
        "  A_train_triu = sparse.csr_matrix(arg1, shape=A.shape, dtype='float32')\n",
        "  A_train = A_train_triu + A_train_triu.T\n",
        "  edges = edges.tolist()\n",
        "  str_edges = set(str(edge[0]) + \" \" + str(edge[1]) for edge in edges)\n",
        "  print(\"Selecting the negative test set!\")\n",
        "  str_test_edges_false = set()\n",
        "  while len(str_test_edges_false) < len(test_edges): # picking the same number of negative test edges\n",
        "    ind_i = np.random.randint(0, A.shape[0])\n",
        "    ind_j = np.random.randint(0, A.shape[0])\n",
        "    if ind_i == ind_j: continue\n",
        "    # these ones were selected earlier\n",
        "    if str(ind_i) + \" \" + str(ind_j) in str_edges: continue\n",
        "    if str(ind_j) + \" \" + str(ind_i) in str_edges: continue\n",
        "    if str(ind_j) + \" \" + str(ind_i) in str_test_edges_false: continue\n",
        "    if str(ind_i) + \" \" + str(ind_j) in str_test_edges_false: continue\n",
        "    # these ones were not\n",
        "    str_test_edges_false.add(str(ind_i) + \" \" + str(ind_j))\n",
        "  test_edges_false = []\n",
        "  for str_edge_false in str_test_edges_false:\n",
        "    edge_false = str_edge_false.split(\" \")\n",
        "    test_edges_false.append([int(edge_false[0]), int(edge_false[1])])\n",
        "  print(\"Test set is ready!\")\n",
        "  print(\"Selecting the negative validation set!\")\n",
        "  str_val_edges_false = set()\n",
        "  while len(str_val_edges_false) < len(val_edges): # annyi negatív validációs példát választunk, amennyi pozitív van\n",
        "    ind_i = np.random.randint(0, A.shape[0])\n",
        "    ind_j = np.random.randint(0, A.shape[0])\n",
        "    if ind_i == ind_j: continue\n",
        "    # these ones were selected earlier\n",
        "    if str(ind_i) + \" \" + str(ind_j) in str_edges: continue\n",
        "    if str(ind_j) + \" \" + str(ind_i) in str_edges: continue\n",
        "    if str(ind_j) + \" \" + str(ind_i) in str_val_edges_false: continue\n",
        "    if str(ind_i) + \" \" + str(ind_j) in str_val_edges_false: continue\n",
        "    if str(ind_j) + \" \" + str(ind_i) in str_test_edges_false: continue\n",
        "    if str(ind_i) + \" \" + str(ind_j) in str_test_edges_false: continue\n",
        "    # these ones were not\n",
        "    str_val_edges_false.add(str(ind_i) + \" \" + str(ind_j))\n",
        "  val_edges_false = []\n",
        "  for str_edge_false in str_val_edges_false:\n",
        "    edge_false = str_edge_false.split(\" \")\n",
        "    val_edges_false.append([int(edge_false[0]), int(edge_false[1])])\n",
        "  print(\"Validation set is ready!\")\n",
        "  # we are ready\n",
        "  test_edges_false = np.array(test_edges_false)\n",
        "  val_edges_false = np.array(val_edges_false)\n",
        "  return A_train, val_edges, val_edges_false, test_edges, test_edges_false"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p-5rUhczBJdV"
      },
      "source": [
        "# hyperparameters\n",
        "hidden = 512 # number of hidden units in the encoder layer\n",
        "latent = 256 # dimension of the latent variables\n",
        "learning_rate = 0.001\n",
        "epochs = 200\n",
        "nparts = 50 # number of partitions\n",
        "batch_size = 1 # number of clusters per batch\n",
        "K = 3 # number of Lanczos iterations"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RflJi-R3BOkG"
      },
      "source": [
        "filename = '/content/drive/MyDrive/GRAPH DATA/ppi.mat' # dataset"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ABjlQkpsDFgx",
        "outputId": "1792be5d-d512-4f1f-a829-3ec7357e8594"
      },
      "source": [
        "mat_dict = sio.loadmat(filename)\n",
        "A = mat_dict['A'].ceil()\n",
        "X = mat_dict['X']\n",
        "Y = mat_dict['Y']\n",
        "train_mask = mat_dict['train_mask'].squeeze().astype(bool)\n",
        "val_mask = mat_dict['val_mask'].squeeze().astype(bool)\n",
        "test_mask = mat_dict['test_mask'].squeeze().astype(bool)\n",
        "\n",
        "# selecting the validation and test edges, and the incomplete adjacency matrix for training\n",
        "A_train, val_edges, val_edges_false, test_edges, test_edges_false = mask_test_edges(A)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Selecting the negative test set!\n",
            "Test set is ready!\n",
            "Selecting the negative validation set!\n",
            "Validation set is ready!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a2GhIMjvDNzG",
        "outputId": "302dc55a-c92b-450c-f8e7-092275e8e9c5"
      },
      "source": [
        "def cluster_graph(A, nparts):\n",
        "  if nparts == 1:\n",
        "    edge_cuts, parts = 0, [0, ] * A.shape[0]\n",
        "  else:\n",
        "    edge_cuts, parts = metis.part_graph([neighbors for neighbors in A.tolil().rows], nparts=nparts)\n",
        "  print('Number of edge cuts: %d.' % edge_cuts)\n",
        "  cluster_dict = {}\n",
        "  for index, part in enumerate(parts):\n",
        "    if part not in cluster_dict:\n",
        "      cluster_dict[part] = []\n",
        "    cluster_dict[part].append(index)\n",
        "  return cluster_dict\n",
        "\n",
        "# the clustering algorithm (METIS)\n",
        "cluster_dict = cluster_graph(A_train, nparts)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of edge cuts: 252793.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FstzOA7oDgoL"
      },
      "source": [
        "def preprocess_support(A):\n",
        "  N = A.shape[1]\n",
        "  D = sparse.csr_matrix(A.sum(axis=1))\n",
        "  norm = D.power(-0.5)\n",
        "  L = sparse.eye(N, dtype='float32') - A.multiply(norm).T.multiply(norm)\n",
        "  max_eigval = sparse.linalg.eigsh(L, k=1, return_eigenvectors=False)[0]\n",
        "  L_ = 2.0 / max_eigval * L - sparse.eye(N, dtype='float32')\n",
        "  return L_\n",
        "\n",
        "def toTensorSparse(S):\n",
        "  return tf.constant(S.todense())\n",
        "\n",
        "def toTensor(T):\n",
        "  return tf.constant(T)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OVOs3s5TDrHJ"
      },
      "source": [
        "# layer classes\n",
        "\n",
        "class bilinear_layer:\n",
        "\n",
        "  def __init__(self, indim, outdim):\n",
        "    pass\n",
        "\n",
        "  def __call__(self, tensor):\n",
        "    return tf.linalg.matmul(tensor, tf.transpose(tensor))\n",
        "\n",
        "# unused\n",
        "class FC_layer:\n",
        "\n",
        "  def __init__(self, indim, outdim):\n",
        "    initial_value = tf.initializers.he_normal()((indim, outdim,))\n",
        "    self.weight = tf.Variable(initial_value=initial_value, trainable=True)\n",
        "\n",
        "  def __call__(self, tensor):\n",
        "    return tf.linalg.matmul(tensor, self.weight)\n",
        "\n",
        "class GC_layer:\n",
        "\n",
        "  def __init__(self, indim, outdim):\n",
        "    global K\n",
        "    self.K = K + 1\n",
        "    delta = np.zeros((outdim, K + 1, K + 1), dtype='float32')\n",
        "    for o in range(outdim):\n",
        "      delta[o, 0, 0] = 1.0\n",
        "    self.filter = tf.Variable(initial_value=delta, trainable=True)\n",
        "    initial_value = tf.initializers.he_normal()((indim, outdim,))\n",
        "    self.weight = tf.Variable(initial_value=initial_value, trainable=True)\n",
        "\n",
        "  # Lanczos algorithm implemented for multiple vectors\n",
        "  def Lanczos_algorithm(self, tensor, support, K, embed=False):\n",
        "    Q = [tf.zeros(tensor.shape), tensor / tf.linalg.norm(tensor, axis=0)]\n",
        "    C = [tf.zeros(tensor.shape[1])]\n",
        "    B = [tf.zeros(tensor.shape[1])]\n",
        "    for k in range(1, K + 1):\n",
        "      if embed: # numpy pipeline\n",
        "        z = tf.constant(support.dot(Q[k].numpy()))\n",
        "      else: # tensorflow pipeline\n",
        "        z = tf.linalg.matmul(support, Q[k])\n",
        "      C.append(tf.einsum('ab, ab -> b', Q[k], z))\n",
        "      z = z - Q[k] * C[k] - Q[k-1] * B[k-1]\n",
        "      B.append(tf.linalg.norm(z, axis=0))\n",
        "      Q.append(z / B[k])\n",
        "    Vs = tf.transpose(tf.stack(Q[1:-1])) # Lanczos vectors\n",
        "    Cs = tf.transpose(tf.stack(C[1:])) # diagonal Lanczos scalars\n",
        "    Bs = tf.transpose(tf.stack(B[1:-1])) # off-diagonal Lanczos scalars\n",
        "    Hs = tf.linalg.diag(Bs, k=-1) + tf.linalg.diag(Cs) + tf.linalg.diag(Bs, k=1) # tridiagonal matrix\n",
        "    return Vs, Hs\n",
        "\n",
        "  def __call__(self, tensor, support, embed=False):\n",
        "    # see the following reference:\n",
        "    # \"Susnjara, A., Perraudin, N., Kressner, D., & Vandergheynst, P. (2015).\n",
        "    # Accelerated filtering on graphs using lanczos method. arXiv preprint arXiv:1509.04537.\"\n",
        "    tensor = tf.linalg.matmul(tensor, self.weight)\n",
        "    V, H = self.Lanczos_algorithm(tensor, support, self.K, embed)\n",
        "    delta = tf.one_hot(tf.zeros(tensor.shape[1], dtype=tf.uint8), self.K)\n",
        "    norm = tf.linalg.norm(tensor, axis=0)\n",
        "    eigvals, eigvecs = tf.linalg.eigh(H)\n",
        "    T = tf.einsum('abc, acd, ade -> abe', eigvecs, self.filter, eigvecs)\n",
        "    result = tf.einsum('abc, acd, ad, a -> ba', V, T, delta, norm)\n",
        "    return result\n"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qwEu7WB9EWAI"
      },
      "source": [
        "# our model class (for the paper \"Scalable Graph Variational Autoencoders\")\n",
        "\n",
        "class Model:\n",
        "\n",
        "  def __init__(self, size_tuple, optimizer, nonlinear):\n",
        "    self.sources = [] # variables to optimize\n",
        "    self.build(size_tuple) # builds the model by stacking layers on each other\n",
        "    self.optimizer = optimizer\n",
        "    self.nonlinear = nonlinear\n",
        "    self.Z_mean = None # mean embedding layer\n",
        "    self.Z_var = None # variance embedding layer\n",
        "    self.noise = None # the noise sample\n",
        "    self.sample = None # self.Z_mean + self.Z_var * self.noise\n",
        "    self.A_gamma = None # the reconstructions\n",
        "  \n",
        "  def build(self, size_tuple):\n",
        "    X_dim, hidden, latent = size_tuple\n",
        "    self.enc_layer = GC_layer(X_dim, hidden)\n",
        "    self.enc_mean_layer = GC_layer(hidden, latent)\n",
        "    self.enc_var_layer = GC_layer(hidden, latent)\n",
        "    self.A_dec_gamma_layer = bilinear_layer(latent, latent)\n",
        "    # filling the source array with weights\n",
        "    layers = [self.enc_layer, self.enc_mean_layer, self.enc_var_layer]\n",
        "    for layer in layers:\n",
        "      self.sources.append(layer.weight)\n",
        "      self.sources.append(layer.filter)\n",
        "  \n",
        "  # forward propagation in the encoder\n",
        "  def encode(self, X, S):\n",
        "    enc = self.nonlinear(self.enc_layer(X, S))\n",
        "    enc_mean = self.enc_mean_layer(enc, S)\n",
        "    enc_var = tf.math.exp(self.enc_var_layer(enc, S))\n",
        "    return enc_mean, enc_var\n",
        "\n",
        "  # returns only the node embeddings\n",
        "  def embed(self, X, S):\n",
        "    enc = self.nonlinear(self.enc_layer(X, S, embed=True))\n",
        "    enc_mean = self.enc_mean_layer(enc, S, embed=True)\n",
        "    return enc_mean\n",
        "\n",
        "  # forward propagation in the decoder\n",
        "  def decode(self, sample):\n",
        "    A_dec_gamma = self.A_dec_gamma_layer(sample)\n",
        "    return A_dec_gamma\n",
        "\n",
        "  def predict(self, X, S):\n",
        "    self.Z_mean, self.Z_var = self.encode(X, S)\n",
        "    self.noise = tf.random.normal(self.Z_var.shape)\n",
        "    self.sample = self.Z_mean + self.Z_var * self.noise # reparameterization trick\n",
        "    self.A_gamma = self.decode(self.sample)\n",
        "\n",
        "  def train(self, X, A, val_edges, val_edges_false, cluster_dict, batch_size, epochs):\n",
        "    for epoch in range(epochs):\n",
        "      # only a subgraph is used in the training process\n",
        "      samples = random.sample(cluster_dict.keys(), batch_size)\n",
        "      nodes = sum([cluster_dict[sample] for sample in samples], [])\n",
        "      S_batch = toTensorSparse(preprocess_support(A[nodes].T[nodes]))\n",
        "      A_batch = toTensor(A.T[nodes].T[nodes].todense())\n",
        "      X_batch = tf.math.l2_normalize(toTensor(X[nodes]), axis=1)\n",
        "      # optimization\n",
        "      with tf.GradientTape() as tape:\n",
        "        self.predict(X_batch, S_batch)\n",
        "        losses = self.loss(A_batch, X_batch)\n",
        "        loss_ = tf.reduce_sum(losses)\n",
        "      print(epoch, [loss.numpy() for loss in losses], loss_.numpy())\n",
        "      grads = tape.gradient(loss_, self.sources)\n",
        "      self.optimizer.apply_gradients(zip(grads, self.sources))\n",
        "\n",
        "  def test(self, X, A, test_edges, test_edges_false):\n",
        "    S_test = preprocess_support(A)\n",
        "    X_test = tf.math.l2_normalize(toTensor(X), axis=1)\n",
        "    self.Z_mean = self.embed(X_test, S_test)\n",
        "    roc_auc, pr_auc = self.accuracy(test_edges, test_edges_false)\n",
        "    print(roc_auc, pr_auc)\n",
        "\n",
        "  # Kullback–Leibler divergence\n",
        "  def KL_Divergence(self):\n",
        "    loss = 0.5 * tf.reduce_mean(self.Z_mean**2.0 + self.Z_var**2.0 - 2.0 * tf.math.log(self.Z_var) - 1.0)\n",
        "    return loss\n",
        "\n",
        "  # reconstruction loss\n",
        "  def re_A_loss(self, A):\n",
        "    density = tf.reduce_sum(A) / tf.size(A, out_type=tf.float32)\n",
        "    pos_weight = (1.0 - density) / density\n",
        "    loss = -0.5 * tf.reduce_mean(1.0 / (1.0 - density) * tf.nn.weighted_cross_entropy_with_logits(labels=A, logits=self.A_gamma, pos_weight=pos_weight))\n",
        "    return -loss\n",
        "\n",
        "  # list of all loss functions\n",
        "  def loss(self, A, X):\n",
        "    return self.KL_Divergence(), self.re_A_loss(A)\n",
        "  \n",
        "  # through the ratio parameter, the number of edges used for validation/testing can be adjusted\n",
        "  def accuracy(self, edges_pos, edges_neg, ratio=1.0):\n",
        "    A_dec = self.Z_mean\n",
        "    #print(\"positive samples\")\n",
        "    p = np.random.permutation(len(edges_pos))\n",
        "    limit = round(ratio * len(edges_pos))\n",
        "    left_pos = []\n",
        "    right_pos = []\n",
        "    for edge in edges_pos[p][:limit]:\n",
        "      left_pos.append(A_dec[edge[0], :])\n",
        "      right_pos.append(A_dec[edge[1], :])\n",
        "    re_pos = tf.nn.sigmoid(tf.einsum('ij, ij -> i', tf.stack(left_pos), tf.stack(right_pos)))\n",
        "    #print(\"negative samples\")\n",
        "    p = np.random.permutation(len(edges_neg))\n",
        "    limit = round(ratio * len(edges_neg))\n",
        "    left_neg = []\n",
        "    right_neg = []\n",
        "    for edge in edges_neg[p][:limit]:\n",
        "      left_neg.append(A_dec[edge[0], :])\n",
        "      right_neg.append(A_dec[edge[1], :])\n",
        "    re_neg = tf.nn.sigmoid(tf.einsum('ij, ij -> i', tf.stack(left_neg), tf.stack(right_neg)))\n",
        "    #print(\"stacking all\")\n",
        "    re_all = tf.stack([re_pos, re_neg])\n",
        "    all = tf.stack([tf.ones(len(re_pos)), tf.zeros(len(re_neg))])\n",
        "    from sklearn.metrics import roc_auc_score, average_precision_score\n",
        "    #print(\"metrics evaluation\")\n",
        "    return roc_auc_score(all, re_all), average_precision_score(all, re_all)\n"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0zuy3DUXLK4J",
        "outputId": "b68f5d29-ceac-415d-cf8f-45136154808e"
      },
      "source": [
        "size_tuple = (X.shape[1], hidden, latent)\n",
        "optimizer = tf.optimizers.Adam(learning_rate=learning_rate)\n",
        "nonlinear = tf.nn.relu\n",
        "\n",
        "model = Model(size_tuple, optimizer, nonlinear)\n",
        "\n",
        "print('Training...')\n",
        "model.train(X, A_train, val_edges, val_edges_false, cluster_dict, batch_size, epochs)\n",
        "print('Testing...')\n",
        "model.test(X, A_train, test_edges, test_edges_false)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training...\n",
            "0 [0.008317752, 6.246009] 6.254327\n",
            "1 [0.008069412, 5.9270678] 5.9351373\n",
            "2 [0.014910133, 5.648883] 5.663793\n",
            "3 [0.011988765, 5.3217406] 5.3337293\n",
            "4 [0.011688336, 5.334153] 5.3458414\n",
            "5 [0.017081242, 5.09459] 5.1116714\n",
            "6 [0.02987887, 4.9757056] 5.0055847\n",
            "7 [0.026919054, 4.6635675] 4.6904864\n",
            "8 [0.030616263, 4.5900393] 4.6206555\n",
            "9 [0.032911334, 4.4368744] 4.4697857\n",
            "10 [0.038473215, 4.222215] 4.2606883\n",
            "11 [0.03077813, 4.19717] 4.2279477\n",
            "12 [0.043574877, 3.894365] 3.93794\n",
            "13 [0.04895062, 3.9082577] 3.9572084\n",
            "14 [0.051079202, 3.667451] 3.7185302\n",
            "15 [0.0784937, 3.369637] 3.4481306\n",
            "16 [0.058125816, 3.4906619] 3.5487876\n",
            "17 [0.08811037, 3.10078] 3.1888905\n",
            "18 [0.11530216, 2.941951] 3.0572531\n",
            "19 [0.1110618, 2.8980174] 3.0090792\n",
            "20 [0.121197455, 2.6761198] 2.7973173\n",
            "21 [0.15065007, 2.4647005] 2.6153505\n",
            "22 [0.16292194, 2.318737] 2.481659\n",
            "23 [0.26418662, 2.0345817] 2.2987683\n",
            "24 [0.20465362, 2.187996] 2.3926497\n",
            "25 [0.24887456, 1.9449385] 2.193813\n",
            "26 [0.2603096, 2.002591] 2.2629006\n",
            "27 [0.21150362, 2.0916758] 2.3031793\n",
            "28 [0.29212883, 1.7028086] 1.9949374\n",
            "29 [0.33538136, 1.5647414] 1.9001228\n",
            "30 [0.3471962, 1.5421422] 1.8893384\n",
            "31 [0.3747748, 1.4698231] 1.8445979\n",
            "32 [0.34679857, 1.544196] 1.8909945\n",
            "33 [0.35565436, 1.5024745] 1.8581289\n",
            "34 [0.38359535, 1.4495088] 1.8331041\n",
            "35 [0.5448857, 1.1890092] 1.7338948\n",
            "36 [0.4174296, 1.3857312] 1.8031608\n",
            "37 [0.566002, 1.1270792] 1.6930813\n",
            "38 [0.56927305, 1.1214384] 1.6907115\n",
            "39 [0.6845998, 1.1568916] 1.8414915\n",
            "40 [0.67361486, 0.9999668] 1.6735816\n",
            "41 [0.5916961, 1.1179549] 1.709651\n",
            "42 [0.5406798, 1.1750219] 1.7157017\n",
            "43 [0.6980011, 0.9778809] 1.675882\n",
            "44 [0.5466523, 1.1237969] 1.6704493\n",
            "45 [0.7039901, 0.99106604] 1.6950562\n",
            "46 [0.57069343, 1.1088895] 1.6795828\n",
            "47 [0.7299722, 0.9546149] 1.684587\n",
            "48 [0.66658527, 1.0233465] 1.6899319\n",
            "49 [0.96579856, 0.8864834] 1.8522819\n",
            "50 [0.87717474, 0.8421308] 1.7193055\n",
            "51 [0.6605239, 1.0089413] 1.6694652\n",
            "52 [0.6626669, 0.99596834] 1.6586353\n",
            "53 [0.74549913, 0.94693416] 1.6924334\n",
            "54 [0.79018474, 0.9164541] 1.7066388\n",
            "55 [0.7292047, 0.97519165] 1.7043964\n",
            "56 [0.6630519, 0.9977689] 1.6608207\n",
            "57 [0.62772876, 1.0397586] 1.6674874\n",
            "58 [0.6440769, 1.0139023] 1.6579792\n",
            "59 [0.9509682, 0.8923189] 1.8432871\n",
            "60 [0.6605336, 1.0021378] 1.6626713\n",
            "61 [0.7451306, 0.9179567] 1.6630874\n",
            "62 [0.62403303, 1.0450543] 1.6690874\n",
            "63 [0.7673237, 0.9057404] 1.673064\n",
            "64 [0.6787566, 0.9700093] 1.6487659\n",
            "65 [0.7274323, 0.9260987] 1.6535311\n",
            "66 [0.75169903, 0.92966783] 1.6813669\n",
            "67 [0.7801799, 0.91941464] 1.6995945\n",
            "68 [0.7153126, 0.9575915] 1.672904\n",
            "69 [0.61372876, 1.0433925] 1.6571213\n",
            "70 [0.5494767, 1.1153786] 1.6648552\n",
            "71 [0.5841099, 1.0877494] 1.6718593\n",
            "72 [0.59627295, 1.0539447] 1.6502177\n",
            "73 [0.5784448, 1.0753925] 1.6538372\n",
            "74 [0.57217515, 1.0912706] 1.6634457\n",
            "75 [0.6880935, 0.99891925] 1.6870127\n",
            "76 [0.70333916, 1.0364873] 1.7398264\n",
            "77 [0.592881, 1.0611856] 1.6540666\n",
            "78 [0.6595871, 1.0127411] 1.6723282\n",
            "79 [0.69700533, 0.9948226] 1.691828\n",
            "80 [0.6071795, 1.0633684] 1.670548\n",
            "81 [0.565971, 1.1127435] 1.6787145\n",
            "82 [0.59954643, 1.0875213] 1.6870677\n",
            "83 [0.6359163, 1.0160834] 1.6519997\n",
            "84 [0.58496714, 1.0775107] 1.6624779\n",
            "85 [0.7642465, 1.034221] 1.7984676\n",
            "86 [0.8016319, 0.9489039] 1.7505358\n",
            "87 [0.58345455, 1.0818037] 1.6652582\n",
            "88 [0.59612304, 1.0889145] 1.6850376\n",
            "89 [0.64581305, 1.0156496] 1.6614625\n",
            "90 [0.6642001, 1.0056207] 1.6698208\n",
            "91 [0.5558794, 1.1191188] 1.6749983\n",
            "92 [0.8474635, 0.9247155] 1.772179\n",
            "93 [0.64158195, 1.010113] 1.651695\n",
            "94 [0.5917005, 1.0686418] 1.6603422\n",
            "95 [0.5509908, 1.1238585] 1.6748493\n",
            "96 [0.53226435, 1.1598969] 1.6921612\n",
            "97 [0.6071955, 1.0323236] 1.6395191\n",
            "98 [0.5897246, 1.1068109] 1.6965356\n",
            "99 [0.60988384, 1.0295503] 1.6394341\n",
            "100 [0.77837354, 0.9365011] 1.7148746\n",
            "101 [0.5206507, 1.1555505] 1.6762011\n",
            "102 [0.59601206, 1.0669605] 1.6629725\n",
            "103 [0.5581804, 1.112569] 1.6707494\n",
            "104 [0.66199636, 0.9836586] 1.6456549\n",
            "105 [0.543912, 1.1298052] 1.6737173\n",
            "106 [0.5652289, 1.11724] 1.6824689\n",
            "107 [0.6715849, 0.9841481] 1.655733\n",
            "108 [0.680032, 1.0077649] 1.687797\n",
            "109 [0.79927903, 0.9408244] 1.7401035\n",
            "110 [0.798664, 0.93096685] 1.7296308\n",
            "111 [0.69104844, 0.97146237] 1.6625109\n",
            "112 [0.78089446, 0.8931967] 1.6740911\n",
            "113 [0.5978413, 1.1014303] 1.6992717\n",
            "114 [0.7105382, 0.9480557] 1.6585939\n",
            "115 [0.54577076, 1.1365827] 1.6823535\n",
            "116 [0.8374755, 0.9561706] 1.7936461\n",
            "117 [0.77574867, 0.9590252] 1.7347739\n",
            "118 [0.6751143, 1.0223894] 1.6975037\n",
            "119 [0.6004879, 1.0633004] 1.6637883\n",
            "120 [0.69110626, 0.93997633] 1.6310825\n",
            "121 [0.63485605, 0.9939707] 1.6288267\n",
            "122 [0.7490316, 0.9352499] 1.6842816\n",
            "123 [0.5161096, 1.1910481] 1.7071577\n",
            "124 [0.5931688, 1.074989] 1.6681578\n",
            "125 [0.579861, 1.0941459] 1.6740069\n",
            "126 [0.5864713, 1.1975156] 1.7839869\n",
            "127 [0.61013895, 1.05626] 1.666399\n",
            "128 [0.58054274, 1.0651305] 1.6456733\n",
            "129 [0.54584026, 1.1218767] 1.667717\n",
            "130 [0.71900636, 1.0241699] 1.7431762\n",
            "131 [0.7986161, 0.930151] 1.7287672\n",
            "132 [0.5976414, 1.0584409] 1.6560824\n",
            "133 [0.560495, 1.1326866] 1.6931816\n",
            "134 [0.69478285, 0.93716043] 1.6319432\n",
            "135 [0.62576705, 1.0259305] 1.6516976\n",
            "136 [0.67173326, 0.9878253] 1.6595585\n",
            "137 [0.6527939, 1.0409623] 1.6937562\n",
            "138 [0.65542424, 0.9873616] 1.6427858\n",
            "139 [0.68451226, 0.97247124] 1.6569835\n",
            "140 [0.62250745, 1.0263133] 1.6488208\n",
            "141 [0.5328276, 1.1311969] 1.6640245\n",
            "142 [0.79344136, 0.9408202] 1.7342615\n",
            "143 [0.5470262, 1.1080803] 1.6551065\n",
            "144 [0.5470909, 1.1089978] 1.6560887\n",
            "145 [0.6901625, 0.9496738] 1.6398363\n",
            "146 [0.6502541, 0.9970417] 1.6472957\n",
            "147 [0.54228455, 1.108769] 1.6510537\n",
            "148 [0.6244422, 1.0188606] 1.6433028\n",
            "149 [0.6480348, 1.0036862] 1.651721\n",
            "150 [0.5911006, 1.1823711] 1.7734717\n",
            "151 [0.54962194, 1.1232494] 1.6728714\n",
            "152 [0.60489696, 1.0488217] 1.6537187\n",
            "153 [0.6534397, 0.9912907] 1.6447303\n",
            "154 [0.81076765, 0.9134864] 1.7242541\n",
            "155 [0.6609183, 0.980588] 1.6415063\n",
            "156 [0.74353135, 0.9017771] 1.6453085\n",
            "157 [0.638195, 1.0008651] 1.63906\n",
            "158 [0.6420217, 1.0044043] 1.646426\n",
            "159 [0.69320804, 0.965632] 1.6588401\n",
            "160 [0.62020993, 1.0392101] 1.65942\n",
            "161 [0.55032545, 1.1180694] 1.6683948\n",
            "162 [0.5318572, 1.1361902] 1.6680474\n",
            "163 [0.7345914, 0.9084475] 1.643039\n",
            "164 [0.549647, 1.1018753] 1.6515223\n",
            "165 [0.5646436, 1.1162741] 1.6809177\n",
            "166 [0.6544818, 0.984277] 1.6387589\n",
            "167 [0.57213175, 1.0891978] 1.6613295\n",
            "168 [0.5556265, 1.0805085] 1.636135\n",
            "169 [0.5765596, 1.0907257] 1.6672852\n",
            "170 [0.566941, 1.0899371] 1.6568781\n",
            "171 [0.6644941, 0.97984695] 1.644341\n",
            "172 [0.7684322, 0.94215137] 1.7105836\n",
            "173 [0.67891115, 0.9715133] 1.6504245\n",
            "174 [0.6541648, 1.0156789] 1.6698437\n",
            "175 [0.67221355, 0.9800669] 1.6522804\n",
            "176 [0.56921077, 1.1020072] 1.6712179\n",
            "177 [0.6172996, 1.0294604] 1.64676\n",
            "178 [0.58210063, 1.0987759] 1.6808765\n",
            "179 [0.7785804, 0.9379828] 1.7165632\n",
            "180 [0.6487799, 1.0101745] 1.6589544\n",
            "181 [0.6813827, 0.95395154] 1.6353343\n",
            "182 [0.66559803, 0.9791505] 1.6447484\n",
            "183 [0.6085375, 1.1539049] 1.7624424\n",
            "184 [0.7396285, 0.983771] 1.7233995\n",
            "185 [0.6412628, 1.0098878] 1.6511507\n",
            "186 [0.5533583, 1.0936118] 1.6469702\n",
            "187 [0.6087536, 1.0372097] 1.6459634\n",
            "188 [0.63083214, 1.0206697] 1.6515019\n",
            "189 [0.529652, 1.1696845] 1.6993365\n",
            "190 [0.7020721, 0.9562159] 1.658288\n",
            "191 [0.55574864, 1.0920519] 1.6478004\n",
            "192 [0.81704223, 0.90771586] 1.7247581\n",
            "193 [0.5314687, 1.1545712] 1.6860399\n",
            "194 [0.7052862, 0.9484998] 1.653786\n",
            "195 [0.61979914, 1.0219766] 1.6417757\n",
            "196 [0.570955, 1.0736785] 1.6446335\n",
            "197 [0.567928, 1.0850991] 1.653027\n",
            "198 [0.5693476, 1.0777957] 1.6471434\n",
            "199 [0.65048885, 0.99685323] 1.6473421\n",
            "Testing...\n",
            "0.8508995859339693 0.9254497929669846\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}