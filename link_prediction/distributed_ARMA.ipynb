{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "distributed_ARMA.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "zzzB2uJwBC-m",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ac4517cb-912d-4453-f6b7-6babeb0e2739"
      },
      "source": [
        "!sudo apt-get install libmetis-dev\n",
        "!pip install metis\n",
        "import metis\n",
        "import random\n",
        "from itertools import chain\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from scipy import sparse\n",
        "import scipy.io as sio"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "libmetis-dev is already the newest version (5.1.0.dfsg-5).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 30 not upgraded.\n",
            "Requirement already satisfied: metis in /usr/local/lib/python3.7/dist-packages (0.2a5)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j2yJoBMsBFa9"
      },
      "source": [
        "def mask_test_edges(A):\n",
        "  A_triu = sparse.triu(A)\n",
        "  edges = np.stack(A_triu.nonzero()).T # all edges of the graph\n",
        "  num_val = int(0.05 * edges.shape[0]) # 5% of the edges for validation\n",
        "  num_test = int(0.1 * edges.shape[0]) # 10% of the edges for testing\n",
        "  edge_ind = np.arange(edges.shape[0]) # indices of the edges\n",
        "  np.random.shuffle(edge_ind) # shuffling the indices\n",
        "  val_edge_ind = edge_ind[:num_val] # under 5%: indices for validation\n",
        "  test_edge_ind = edge_ind[num_val:(num_val + num_test)] # 5-15%: indices for testing\n",
        "  train_edge_ind = edge_ind[(num_val + num_test):] # over 15%: indices for training\n",
        "  val_edges = edges[val_edge_ind]\n",
        "  test_edges = edges[test_edge_ind]\n",
        "  train_edges = edges[train_edge_ind]\n",
        "  # the incomplete adjacency matrix for training\n",
        "  arg1 = (np.ones(train_edges.shape[0]), (train_edges[:, 0], train_edges[:, 1]))\n",
        "  A_train_triu = sparse.csr_matrix(arg1, shape=A.shape, dtype='float32')\n",
        "  A_train = A_train_triu + A_train_triu.T\n",
        "  edges = edges.tolist()\n",
        "  str_edges = set(str(edge[0]) + \" \" + str(edge[1]) for edge in edges)\n",
        "  print(\"Selecting the negative test set!\")\n",
        "  str_test_edges_false = set()\n",
        "  while len(str_test_edges_false) < len(test_edges): # picking the same number of negative test edges\n",
        "    ind_i = np.random.randint(0, A.shape[0])\n",
        "    ind_j = np.random.randint(0, A.shape[0])\n",
        "    if ind_i == ind_j: continue\n",
        "    # these ones were selected earlier\n",
        "    if str(ind_i) + \" \" + str(ind_j) in str_edges: continue\n",
        "    if str(ind_j) + \" \" + str(ind_i) in str_edges: continue\n",
        "    if str(ind_j) + \" \" + str(ind_i) in str_test_edges_false: continue\n",
        "    if str(ind_i) + \" \" + str(ind_j) in str_test_edges_false: continue\n",
        "    # these ones were not\n",
        "    str_test_edges_false.add(str(ind_i) + \" \" + str(ind_j))\n",
        "  test_edges_false = []\n",
        "  for str_edge_false in str_test_edges_false:\n",
        "    edge_false = str_edge_false.split(\" \")\n",
        "    test_edges_false.append([int(edge_false[0]), int(edge_false[1])])\n",
        "  print(\"Test set is ready!\")\n",
        "  print(\"Selecting the negative validation set!\")\n",
        "  str_val_edges_false = set()\n",
        "  while len(str_val_edges_false) < len(val_edges): # annyi negatív validációs példát választunk, amennyi pozitív van\n",
        "    ind_i = np.random.randint(0, A.shape[0])\n",
        "    ind_j = np.random.randint(0, A.shape[0])\n",
        "    if ind_i == ind_j: continue\n",
        "    # these ones were selected earlier\n",
        "    if str(ind_i) + \" \" + str(ind_j) in str_edges: continue\n",
        "    if str(ind_j) + \" \" + str(ind_i) in str_edges: continue\n",
        "    if str(ind_j) + \" \" + str(ind_i) in str_val_edges_false: continue\n",
        "    if str(ind_i) + \" \" + str(ind_j) in str_val_edges_false: continue\n",
        "    if str(ind_j) + \" \" + str(ind_i) in str_test_edges_false: continue\n",
        "    if str(ind_i) + \" \" + str(ind_j) in str_test_edges_false: continue\n",
        "    # these ones were not\n",
        "    str_val_edges_false.add(str(ind_i) + \" \" + str(ind_j))\n",
        "  val_edges_false = []\n",
        "  for str_edge_false in str_val_edges_false:\n",
        "    edge_false = str_edge_false.split(\" \")\n",
        "    val_edges_false.append([int(edge_false[0]), int(edge_false[1])])\n",
        "  print(\"Validation set is ready!\")\n",
        "  # we are ready\n",
        "  test_edges_false = np.array(test_edges_false)\n",
        "  val_edges_false = np.array(val_edges_false)\n",
        "  return A_train, val_edges, val_edges_false, test_edges, test_edges_false"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p-5rUhczBJdV"
      },
      "source": [
        "# hyperparameters\n",
        "hidden = 512 # number of hidden units in the encoder layer\n",
        "latent = 256 # dimension of the latent variables\n",
        "learning_rate = 0.001\n",
        "epochs = 200\n",
        "nparts = 50 # number of partitions\n",
        "batch_size = 1 # number of clusters per batch\n",
        "K = 3 # number of iterations\n",
        "T = 2 # number of threads"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RflJi-R3BOkG"
      },
      "source": [
        "filename = '/content/drive/MyDrive/GRAPH DATA/ppi.mat' # dataset"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ABjlQkpsDFgx",
        "outputId": "d563e006-c6be-403e-f637-e92a9e091209"
      },
      "source": [
        "mat_dict = sio.loadmat(filename)\n",
        "A = mat_dict['A'].ceil()\n",
        "X = mat_dict['X']\n",
        "Y = mat_dict['Y']\n",
        "train_mask = mat_dict['train_mask'].squeeze().astype(bool)\n",
        "val_mask = mat_dict['val_mask'].squeeze().astype(bool)\n",
        "test_mask = mat_dict['test_mask'].squeeze().astype(bool)\n",
        "\n",
        "# selecting the validation and test edges, and the incomplete adjacency matrix for training\n",
        "A_train, val_edges, val_edges_false, test_edges, test_edges_false = mask_test_edges(A)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Selecting the negative test set!\n",
            "Test set is ready!\n",
            "Selecting the negative validation set!\n",
            "Validation set is ready!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a2GhIMjvDNzG",
        "outputId": "01c92d1f-afd3-41b7-a650-b645fb691e33"
      },
      "source": [
        "def cluster_graph(A, nparts):\n",
        "  if nparts == 1:\n",
        "    edge_cuts, parts = 0, [0, ] * A.shape[0]\n",
        "  else:\n",
        "    edge_cuts, parts = metis.part_graph([neighbors for neighbors in A.tolil().rows], nparts=nparts)\n",
        "  print('Number of edge cuts: %d.' % edge_cuts)\n",
        "  cluster_dict = {}\n",
        "  for index, part in enumerate(parts):\n",
        "    if part not in cluster_dict:\n",
        "      cluster_dict[part] = []\n",
        "    cluster_dict[part].append(index)\n",
        "  return cluster_dict\n",
        "\n",
        "# the clustering algorithm (METIS)\n",
        "cluster_dict = cluster_graph(A_train, nparts)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of edge cuts: 255632.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FstzOA7oDgoL"
      },
      "source": [
        "def preprocess_support(A):\n",
        "  N = A.shape[1]\n",
        "  D = sparse.csr_matrix(A.sum(axis=1))\n",
        "  norm = D.power(-0.5)\n",
        "  P = A.multiply(norm).T.multiply(norm)\n",
        "  return P\n",
        "\n",
        "def toTensorSparse(S):\n",
        "  return tf.constant(S.todense())\n",
        "\n",
        "def toTensor(T):\n",
        "  return tf.constant(T)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OVOs3s5TDrHJ"
      },
      "source": [
        "# layer classes\n",
        "\n",
        "class bilinear_layer:\n",
        "\n",
        "  def __init__(self, indim, outdim):\n",
        "    pass\n",
        "\n",
        "  def __call__(self, tensor):\n",
        "    return tf.linalg.matmul(tensor, tf.transpose(tensor))\n",
        "\n",
        "# unused\n",
        "class FC_layer:\n",
        "\n",
        "  def __init__(self, indim, outdim):\n",
        "    initial_value = tf.initializers.he_normal()((indim, outdim,))\n",
        "    self.weight = tf.Variable(initial_value=initial_value, trainable=True)\n",
        "\n",
        "  def __call__(self, tensor):\n",
        "    return tf.linalg.matmul(tensor, self.weight)\n",
        "\n",
        "class GC_layer:\n",
        "\n",
        "  def __init__(self, indim, outdim):\n",
        "    global K\n",
        "    global T\n",
        "    self.K = K\n",
        "    self.T = T\n",
        "    self.ws = []\n",
        "    self.vs = []\n",
        "    for t in range(T):\n",
        "      w_threads = []\n",
        "      v_threads = []\n",
        "      for k in range(self.K):\n",
        "        initial_value = tf.initializers.ones()((1,1))\n",
        "        w_threads.append(tf.Variable(initial_value=(1.0,) * outdim, trainable=True))\n",
        "        initial_value = tf.initializers.ones()((1,1))\n",
        "        v_threads.append(tf.Variable(initial_value=(0.0,) * outdim, trainable=True))\n",
        "      self.ws.append(w_threads)\n",
        "      self.vs.append(v_threads)\n",
        "    initial_value = tf.initializers.he_normal()((indim, outdim,))\n",
        "    self.weight = tf.Variable(initial_value=initial_value, trainable=True)\n",
        "\n",
        "  def __call__(self, tensor, support, embed=False):\n",
        "    if embed: # numpy pipeline\n",
        "      tensor = tensor.numpy().dot(self.weight.numpy())\n",
        "      results = []\n",
        "      for t in range(self.T):\n",
        "         # Personalized PageRank\n",
        "        tensor_ = tensor\n",
        "        for k in range(self.K):\n",
        "          tensor_ = self.ws[t][k].numpy() * support.dot(tensor_) + self.vs[t][k].numpy() * tensor\n",
        "        results.append(tensor_)\n",
        "      return sum(results)\n",
        "    else: # tensorflow pipeline\n",
        "      tensor = tf.linalg.matmul(tensor, self.weight)\n",
        "      results = []\n",
        "      for t in range(self.T):\n",
        "         # Personalized PageRank\n",
        "        tensor_ = tensor\n",
        "        for k in range(self.K):\n",
        "          tensor_ = self.ws[t][k] * tf.linalg.matmul(support, tensor_) + self.vs[t][k] * tensor\n",
        "        results.append(tensor_)\n",
        "      return sum(results)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qwEu7WB9EWAI"
      },
      "source": [
        "# our model class (for the paper \"Scalable Graph Variational Autoencoders\")\n",
        "\n",
        "class Model:\n",
        "\n",
        "  def __init__(self, size_tuple, optimizer, nonlinear):\n",
        "    self.sources = [] # variables to optimize\n",
        "    self.build(size_tuple) # builds the model by stacking layers on each other\n",
        "    self.optimizer = optimizer\n",
        "    self.nonlinear = nonlinear\n",
        "    self.Z_mean = None # mean embedding layer\n",
        "    self.Z_var = None # variance embedding layer\n",
        "    self.noise = None # the noise sample\n",
        "    self.sample = None # self.Z_mean + self.Z_var * self.noise\n",
        "    self.A_gamma = None # the reconstructions\n",
        "  \n",
        "  def build(self, size_tuple):\n",
        "    X_dim, hidden, latent = size_tuple\n",
        "    self.enc_layer = GC_layer(X_dim, hidden)\n",
        "    self.enc_mean_layer = GC_layer(hidden, latent)\n",
        "    self.enc_var_layer = GC_layer(hidden, latent)\n",
        "    self.A_dec_gamma_layer = bilinear_layer(latent, latent)\n",
        "    # filling the source array with weights\n",
        "    layers = [self.enc_layer, self.enc_mean_layer, self.enc_var_layer]\n",
        "    for layer in layers:\n",
        "      self.sources.append(layer.weight)\n",
        "      self.sources += list(chain.from_iterable(layer.ws)) + list(chain.from_iterable(layer.vs))\n",
        "  \n",
        "  # forward propagation in the encoder\n",
        "  def encode(self, X, S):\n",
        "    enc = self.nonlinear(self.enc_layer(X, S))\n",
        "    enc_mean = self.enc_mean_layer(enc, S)\n",
        "    enc_var = tf.math.exp(self.enc_var_layer(enc, S))\n",
        "    return enc_mean, enc_var\n",
        "\n",
        "  # returns only the node embeddings\n",
        "  def embed(self, X, S):\n",
        "    enc = self.nonlinear(self.enc_layer(X, S, embed=True))\n",
        "    enc_mean = self.enc_mean_layer(enc, S, embed=True)\n",
        "    return enc_mean\n",
        "\n",
        "  # forward propagation in the decoder\n",
        "  def decode(self, sample):\n",
        "    A_dec_gamma = self.A_dec_gamma_layer(sample)\n",
        "    return A_dec_gamma\n",
        "\n",
        "  def predict(self, X, S):\n",
        "    self.Z_mean, self.Z_var = self.encode(X, S)\n",
        "    self.noise = tf.random.normal(self.Z_var.shape)\n",
        "    self.sample = self.Z_mean + self.Z_var * self.noise # reparameterization trick\n",
        "    self.A_gamma = self.decode(self.sample)\n",
        "\n",
        "  def train(self, X, A, val_edges, val_edges_false, cluster_dict, batch_size, epochs):\n",
        "    for epoch in range(epochs):\n",
        "      # only a subgraph is used in the training process\n",
        "      samples = random.sample(cluster_dict.keys(), batch_size)\n",
        "      nodes = sum([cluster_dict[sample] for sample in samples], [])\n",
        "      S_batch = toTensorSparse(preprocess_support(A[nodes].T[nodes]))\n",
        "      A_batch = toTensor(A.T[nodes].T[nodes].todense())\n",
        "      X_batch = tf.math.l2_normalize(toTensor(X[nodes]), axis=1)\n",
        "      # optimization\n",
        "      with tf.GradientTape() as tape:\n",
        "        self.predict(X_batch, S_batch)\n",
        "        losses = self.loss(A_batch, X_batch)\n",
        "        loss_ = tf.reduce_sum(losses)\n",
        "      print(epoch, [loss.numpy() for loss in losses], loss_.numpy())\n",
        "      grads = tape.gradient(loss_, self.sources)\n",
        "      self.optimizer.apply_gradients(zip(grads, self.sources))\n",
        "\n",
        "  def test(self, X, A, test_edges, test_edges_false):\n",
        "    S_test = preprocess_support(A)\n",
        "    X_test = tf.math.l2_normalize(toTensor(X), axis=1)\n",
        "    self.Z_mean = self.embed(X_test, S_test)\n",
        "    roc_auc, pr_auc = self.accuracy(test_edges, test_edges_false)\n",
        "    print(roc_auc, pr_auc)\n",
        "\n",
        "  # Kullback–Leibler divergence\n",
        "  def KL_Divergence(self):\n",
        "    loss = 0.5 * tf.reduce_mean(self.Z_mean**2.0 + self.Z_var**2.0 - 2.0 * tf.math.log(self.Z_var) - 1.0)\n",
        "    return loss\n",
        "\n",
        "  # reconstruction loss\n",
        "  def re_A_loss(self, A):\n",
        "    density = tf.reduce_sum(A) / tf.size(A, out_type=tf.float32)\n",
        "    pos_weight = (1.0 - density) / density\n",
        "    loss = -0.5 * tf.reduce_mean(1.0 / (1.0 - density) * tf.nn.weighted_cross_entropy_with_logits(labels=A, logits=self.A_gamma, pos_weight=pos_weight))\n",
        "    return -loss\n",
        "\n",
        "  # list of all loss functions\n",
        "  def loss(self, A, X):\n",
        "    return self.KL_Divergence(), self.re_A_loss(A)\n",
        "  \n",
        "  # through the ratio parameter, the number of edges used for validation/testing can be adjusted\n",
        "  def accuracy(self, edges_pos, edges_neg, ratio=1.0):\n",
        "    A_dec = self.Z_mean\n",
        "    #print(\"positive samples\")\n",
        "    p = np.random.permutation(len(edges_pos))\n",
        "    limit = round(ratio * len(edges_pos))\n",
        "    left_pos = []\n",
        "    right_pos = []\n",
        "    for edge in edges_pos[p][:limit]:\n",
        "      left_pos.append(A_dec[edge[0], :])\n",
        "      right_pos.append(A_dec[edge[1], :])\n",
        "    re_pos = tf.nn.sigmoid(tf.einsum('ij, ij -> i', tf.stack(left_pos), tf.stack(right_pos)))\n",
        "    #print(\"negative samples\")\n",
        "    p = np.random.permutation(len(edges_neg))\n",
        "    limit = round(ratio * len(edges_neg))\n",
        "    left_neg = []\n",
        "    right_neg = []\n",
        "    for edge in edges_neg[p][:limit]:\n",
        "      left_neg.append(A_dec[edge[0], :])\n",
        "      right_neg.append(A_dec[edge[1], :])\n",
        "    re_neg = tf.nn.sigmoid(tf.einsum('ij, ij -> i', tf.stack(left_neg), tf.stack(right_neg)))\n",
        "    #print(\"stacking all\")\n",
        "    re_all = tf.stack([re_pos, re_neg])\n",
        "    all = tf.stack([tf.ones(len(re_pos)), tf.zeros(len(re_neg))])\n",
        "    from sklearn.metrics import roc_auc_score, average_precision_score\n",
        "    #print(\"metrics evaluation\")\n",
        "    return roc_auc_score(all, re_all), average_precision_score(all, re_all)\n"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0zuy3DUXLK4J",
        "outputId": "01c8d5c9-5498-4c15-bbec-b1412dc3e2d1"
      },
      "source": [
        "size_tuple = (X.shape[1], hidden, latent)\n",
        "optimizer = tf.optimizers.Adam(learning_rate=learning_rate)\n",
        "nonlinear = tf.nn.relu\n",
        "\n",
        "model = Model(size_tuple, optimizer, nonlinear)\n",
        "\n",
        "print('Training...')\n",
        "model.train(X, A_train, val_edges, val_edges_false, cluster_dict, batch_size, epochs)\n",
        "print('Testing...')\n",
        "model.test(X, A_train, test_edges, test_edges_false)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training...\n",
            "0 [0.10812142, 9.2097225] 9.317844\n",
            "1 [0.113936685, 7.759367] 7.873304\n",
            "2 [0.051459022, 5.934581] 5.9860396\n",
            "3 [0.097845286, 5.7771173] 5.8749623\n",
            "4 [0.1309065, 5.068611] 5.1995177\n",
            "5 [0.1086506, 4.1462846] 4.2549353\n",
            "6 [0.09446749, 3.9110644] 4.005532\n",
            "7 [0.109861344, 3.4455912] 3.5554526\n",
            "8 [0.18110959, 2.9110293] 3.092139\n",
            "9 [0.1742205, 2.8042567] 2.9784772\n",
            "10 [0.20383692, 2.5866132] 2.79045\n",
            "11 [0.16890427, 2.6120567] 2.780961\n",
            "12 [0.23277374, 2.2260056] 2.4587793\n",
            "13 [0.31785366, 2.1238678] 2.4417214\n",
            "14 [0.29335132, 2.1162674] 2.4096189\n",
            "15 [0.37016726, 1.6594888] 2.029656\n",
            "16 [0.45313275, 1.5301051] 1.9832379\n",
            "17 [0.47626808, 1.4368687] 1.9131367\n",
            "18 [0.5623209, 1.3486826] 1.9110036\n",
            "19 [0.6568086, 1.304159] 1.9609677\n",
            "20 [0.5849573, 1.1729037] 1.7578609\n",
            "21 [0.6508984, 1.1215695] 1.7724679\n",
            "22 [0.4297771, 1.4048761] 1.8346531\n",
            "23 [0.6284183, 1.0887277] 1.717146\n",
            "24 [0.63371426, 1.1888914] 1.8226056\n",
            "25 [0.7578155, 1.0137247] 1.7715402\n",
            "26 [0.560195, 1.140828] 1.7010231\n",
            "27 [0.50846773, 1.2205317] 1.7289994\n",
            "28 [0.7498447, 0.96496797] 1.7148126\n",
            "29 [0.75242823, 0.9332775] 1.6857057\n",
            "30 [0.636802, 1.040863] 1.677665\n",
            "31 [0.7347287, 0.94079626] 1.675525\n",
            "32 [0.5456985, 1.1561259] 1.7018244\n",
            "33 [0.7909785, 1.1841747] 1.9751532\n",
            "34 [0.7116166, 0.9559944] 1.667611\n",
            "35 [0.78539836, 0.9002398] 1.6856382\n",
            "36 [0.8583789, 0.88884276] 1.7472217\n",
            "37 [0.64617336, 1.02181] 1.6679834\n",
            "38 [0.77981305, 0.8911822] 1.6709952\n",
            "39 [0.7054621, 0.9460793] 1.6515415\n",
            "40 [0.61569065, 1.0463473] 1.6620378\n",
            "41 [0.5331618, 1.1646231] 1.6977849\n",
            "42 [0.70400345, 0.9832387] 1.6872422\n",
            "43 [0.8336266, 0.8507176] 1.6843443\n",
            "44 [0.58220106, 1.0790659] 1.661267\n",
            "45 [0.746464, 0.9167872] 1.6632512\n",
            "46 [0.7460116, 0.94777125] 1.6937828\n",
            "47 [0.69724107, 1.0507187] 1.7479597\n",
            "48 [0.60681736, 1.0433046] 1.6501219\n",
            "49 [0.9020091, 0.8224586] 1.7244678\n",
            "50 [0.71664435, 0.94699705] 1.6636415\n",
            "51 [0.7729853, 0.8824183] 1.6554036\n",
            "52 [0.7068842, 0.9575135] 1.6643977\n",
            "53 [0.55754584, 1.1323946] 1.6899405\n",
            "54 [0.6337257, 1.0130961] 1.6468217\n",
            "55 [0.70392174, 1.0391204] 1.7430422\n",
            "56 [0.7404857, 0.910572] 1.6510577\n",
            "57 [0.65686494, 1.0181981] 1.6750631\n",
            "58 [0.46163586, 1.2579539] 1.7195897\n",
            "59 [0.508528, 1.1614692] 1.6699972\n",
            "60 [0.55185986, 1.0963678] 1.6482277\n",
            "61 [0.55510014, 1.095915] 1.651015\n",
            "62 [0.46949565, 1.2390529] 1.7085485\n",
            "63 [0.6921204, 0.96118355] 1.6533039\n",
            "64 [0.72004, 0.94316787] 1.6632079\n",
            "65 [0.66319555, 0.9959846] 1.6591802\n",
            "66 [0.66567045, 0.9915372] 1.6572077\n",
            "67 [0.6823455, 0.9667146] 1.6490601\n",
            "68 [0.74305147, 0.9438325] 1.6868839\n",
            "69 [0.6315403, 1.0137439] 1.6452842\n",
            "70 [0.62396437, 1.0185677] 1.6425321\n",
            "71 [0.71714646, 1.0192149] 1.7363613\n",
            "72 [0.6878751, 0.9529325] 1.6408076\n",
            "73 [0.6970715, 0.96220714] 1.6592786\n",
            "74 [0.6756364, 0.966062] 1.6416984\n",
            "75 [0.64785385, 1.0130353] 1.6608891\n",
            "76 [0.65812606, 0.9784427] 1.6365688\n",
            "77 [0.618041, 1.0050161] 1.6230571\n",
            "78 [0.6703736, 0.97072685] 1.6411004\n",
            "79 [0.7142907, 0.9214736] 1.6357644\n",
            "80 [0.5274503, 1.1373065] 1.6647568\n",
            "81 [0.8078282, 0.84628683] 1.654115\n",
            "82 [0.54412234, 1.1287785] 1.6729008\n",
            "83 [0.6776481, 0.9551684] 1.6328166\n",
            "84 [0.6299696, 1.0161588] 1.6461284\n",
            "85 [0.63643235, 1.0273283] 1.6637607\n",
            "86 [0.5800575, 1.0573277] 1.6373852\n",
            "87 [0.52697086, 1.1406945] 1.6676654\n",
            "88 [0.6277916, 1.0224849] 1.6502764\n",
            "89 [0.62825024, 1.0453515] 1.6736017\n",
            "90 [0.5791998, 1.0637475] 1.6429473\n",
            "91 [0.6463914, 1.0238185] 1.6702099\n",
            "92 [0.5377184, 1.1474816] 1.6852\n",
            "93 [0.65319675, 0.99340856] 1.6466053\n",
            "94 [0.77558583, 0.9926618] 1.7682476\n",
            "95 [0.6516058, 1.0043429] 1.6559486\n",
            "96 [0.54905736, 1.1277732] 1.6768305\n",
            "97 [0.6315428, 1.0230072] 1.65455\n",
            "98 [0.64762664, 1.000938] 1.6485647\n",
            "99 [0.6952934, 0.9437205] 1.639014\n",
            "100 [0.5327229, 1.140717] 1.67344\n",
            "101 [0.6597577, 0.98938316] 1.6491408\n",
            "102 [0.7193722, 0.918198] 1.6375701\n",
            "103 [0.5110843, 1.1484001] 1.6594844\n",
            "104 [0.69664234, 0.9372884] 1.6339307\n",
            "105 [0.47089514, 1.233598] 1.7044932\n",
            "106 [0.58210266, 1.0704864] 1.6525891\n",
            "107 [0.9860233, 1.0517317] 2.037755\n",
            "108 [0.98036265, 0.89051336] 1.8708761\n",
            "109 [0.96675116, 0.8341757] 1.8009269\n",
            "110 [0.71053255, 0.9953698] 1.7059023\n",
            "111 [0.7404377, 1.0371163] 1.777554\n",
            "112 [0.60255927, 1.0853724] 1.6879318\n",
            "113 [0.5079104, 1.1778855] 1.6857959\n",
            "114 [0.6299462, 1.027317] 1.6572633\n",
            "115 [0.47918984, 1.1953261] 1.674516\n",
            "116 [0.5875744, 1.0791016] 1.666676\n",
            "117 [0.63059616, 1.0824192] 1.7130153\n",
            "118 [0.4906816, 1.2115626] 1.7022443\n",
            "119 [0.47170654, 1.3324908] 1.8041973\n",
            "120 [0.65645415, 1.0061513] 1.6626055\n",
            "121 [0.49366575, 1.1995068] 1.6931725\n",
            "122 [0.49239948, 1.287269] 1.7796685\n",
            "123 [0.6340479, 1.0649874] 1.6990354\n",
            "124 [0.5421792, 1.1219797] 1.6641589\n",
            "125 [0.65318495, 0.99936795] 1.6525528\n",
            "126 [0.95017475, 0.9243886] 1.8745633\n",
            "127 [0.5235864, 1.1618882] 1.6854746\n",
            "128 [0.8142195, 0.95544535] 1.7696648\n",
            "129 [0.5713095, 1.073306] 1.6446154\n",
            "130 [0.57345784, 1.0775284] 1.6509862\n",
            "131 [0.5733511, 1.0662849] 1.639636\n",
            "132 [0.6724699, 0.98237514] 1.654845\n",
            "133 [0.6871531, 0.9822825] 1.6694356\n",
            "134 [0.67844313, 0.9843665] 1.6628096\n",
            "135 [0.57310283, 1.0733308] 1.6464336\n",
            "136 [0.5775782, 1.0871279] 1.6647061\n",
            "137 [0.6684347, 0.98257744] 1.6510122\n",
            "138 [0.9554545, 0.9395896] 1.8950441\n",
            "139 [0.64396703, 1.0045699] 1.6485369\n",
            "140 [0.6595543, 1.0103852] 1.6699395\n",
            "141 [0.80401003, 0.8612088] 1.6652188\n",
            "142 [0.59477186, 1.0489463] 1.6437181\n",
            "143 [0.5990875, 1.0673782] 1.6664656\n",
            "144 [0.60111254, 1.1430204] 1.744133\n",
            "145 [0.4588246, 1.2551905] 1.7140151\n",
            "146 [0.6882415, 0.99008346] 1.6783249\n",
            "147 [0.629026, 1.0384148] 1.6674409\n",
            "148 [0.6024655, 1.1371659] 1.7396314\n",
            "149 [0.55940336, 1.1031804] 1.6625838\n",
            "150 [0.6032874, 1.0400875] 1.6433749\n",
            "151 [0.6231935, 1.0418167] 1.6650102\n",
            "152 [0.6328266, 1.0100404] 1.6428671\n",
            "153 [0.6598066, 0.9789765] 1.6387831\n",
            "154 [0.66007316, 1.0033739] 1.663447\n",
            "155 [0.56216604, 1.1099479] 1.6721139\n",
            "156 [0.6952948, 0.9665401] 1.661835\n",
            "157 [0.56575733, 1.0833901] 1.6491475\n",
            "158 [0.6499163, 0.99117595] 1.6410923\n",
            "159 [0.7367234, 0.9133603] 1.6500838\n",
            "160 [0.6666031, 1.0055892] 1.6721923\n",
            "161 [0.5383558, 1.1071244] 1.6454803\n",
            "162 [0.56919324, 1.0925117] 1.6617049\n",
            "163 [0.5410423, 1.1228974] 1.6639397\n",
            "164 [0.7098149, 1.039451] 1.7492659\n",
            "165 [0.83006006, 0.8439098] 1.6739699\n",
            "166 [0.5965017, 1.0627226] 1.6592243\n",
            "167 [0.5796008, 1.0802411] 1.6598419\n",
            "168 [0.57401794, 1.0761869] 1.6502049\n",
            "169 [0.5013985, 1.1814657] 1.6828642\n",
            "170 [0.5541456, 1.1578572] 1.7120028\n",
            "171 [0.8010748, 0.8959824] 1.6970572\n",
            "172 [0.5637557, 1.0852643] 1.64902\n",
            "173 [0.6217379, 1.0381566] 1.6598945\n",
            "174 [0.6559501, 0.98191744] 1.6378676\n",
            "175 [0.7003969, 0.95343316] 1.65383\n",
            "176 [0.52991074, 1.1427525] 1.6726632\n",
            "177 [0.7345446, 0.9094054] 1.64395\n",
            "178 [0.635101, 1.0293137] 1.6644146\n",
            "179 [0.6949493, 0.95347565] 1.648425\n",
            "180 [0.64168304, 1.0095199] 1.6512029\n",
            "181 [0.5816866, 1.1151711] 1.6968577\n",
            "182 [0.7484171, 0.9220915] 1.6705086\n",
            "183 [0.750278, 1.0038221] 1.7541001\n",
            "184 [0.640272, 0.9995031] 1.639775\n",
            "185 [0.5257983, 1.1589818] 1.6847801\n",
            "186 [0.6958989, 0.959955] 1.6558539\n",
            "187 [0.6963501, 0.9510807] 1.6474308\n",
            "188 [0.5221043, 1.1603266] 1.682431\n",
            "189 [0.9729351, 0.85691214] 1.8298472\n",
            "190 [0.84238476, 0.8191772] 1.661562\n",
            "191 [0.5858369, 1.0639038] 1.6497407\n",
            "192 [0.6620123, 1.004956] 1.6669683\n",
            "193 [0.61267656, 1.0685748] 1.6812513\n",
            "194 [0.56577533, 1.0947213] 1.6604967\n",
            "195 [0.48834077, 1.2045028] 1.6928436\n",
            "196 [0.6499426, 0.9883086] 1.6382512\n",
            "197 [0.7806036, 0.9850238] 1.7656274\n",
            "198 [0.48633957, 1.2189295] 1.7052691\n",
            "199 [0.5752825, 1.0883607] 1.6636431\n",
            "Testing...\n",
            "0.85125380171245 0.925626900856225\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}