{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "polynomial.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "zzzB2uJwBC-m",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "db65136e-775c-4128-bbfb-12e7afdcc9e3"
      },
      "source": [
        "!sudo apt-get install libmetis-dev\n",
        "!pip install metis\n",
        "import metis\n",
        "import random\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from scipy import sparse\n",
        "import scipy.io as sio"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "libmetis-dev is already the newest version (5.1.0.dfsg-5).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 30 not upgraded.\n",
            "Requirement already satisfied: metis in /usr/local/lib/python3.7/dist-packages (0.2a5)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j2yJoBMsBFa9"
      },
      "source": [
        "def mask_test_edges(A):\n",
        "  A_triu = sparse.triu(A)\n",
        "  edges = np.stack(A_triu.nonzero()).T # all edges of the graph\n",
        "  num_val = int(0.05 * edges.shape[0]) # 5% of the edges for validation\n",
        "  num_test = int(0.1 * edges.shape[0]) # 10% of the edges for testing\n",
        "  edge_ind = np.arange(edges.shape[0]) # indices of the edges\n",
        "  np.random.shuffle(edge_ind) # shuffling the indices\n",
        "  val_edge_ind = edge_ind[:num_val] # under 5%: indices for validation\n",
        "  test_edge_ind = edge_ind[num_val:(num_val + num_test)] # 5-15%: indices for testing\n",
        "  train_edge_ind = edge_ind[(num_val + num_test):] # over 15%: indices for training\n",
        "  val_edges = edges[val_edge_ind]\n",
        "  test_edges = edges[test_edge_ind]\n",
        "  train_edges = edges[train_edge_ind]\n",
        "  # the incomplete adjacency matrix for training\n",
        "  arg1 = (np.ones(train_edges.shape[0]), (train_edges[:, 0], train_edges[:, 1]))\n",
        "  A_train_triu = sparse.csr_matrix(arg1, shape=A.shape, dtype='float32')\n",
        "  A_train = A_train_triu + A_train_triu.T\n",
        "  edges = edges.tolist()\n",
        "  str_edges = set(str(edge[0]) + \" \" + str(edge[1]) for edge in edges)\n",
        "  print(\"Selecting the negative test set!\")\n",
        "  str_test_edges_false = set()\n",
        "  while len(str_test_edges_false) < len(test_edges): # picking the same number of negative test edges\n",
        "    ind_i = np.random.randint(0, A.shape[0])\n",
        "    ind_j = np.random.randint(0, A.shape[0])\n",
        "    if ind_i == ind_j: continue\n",
        "    # these ones were selected earlier\n",
        "    if str(ind_i) + \" \" + str(ind_j) in str_edges: continue\n",
        "    if str(ind_j) + \" \" + str(ind_i) in str_edges: continue\n",
        "    if str(ind_j) + \" \" + str(ind_i) in str_test_edges_false: continue\n",
        "    if str(ind_i) + \" \" + str(ind_j) in str_test_edges_false: continue\n",
        "    # these ones were not\n",
        "    str_test_edges_false.add(str(ind_i) + \" \" + str(ind_j))\n",
        "  test_edges_false = []\n",
        "  for str_edge_false in str_test_edges_false:\n",
        "    edge_false = str_edge_false.split(\" \")\n",
        "    test_edges_false.append([int(edge_false[0]), int(edge_false[1])])\n",
        "  print(\"Test set is ready!\")\n",
        "  print(\"Selecting the negative validation set!\")\n",
        "  str_val_edges_false = set()\n",
        "  while len(str_val_edges_false) < len(val_edges): # annyi negatív validációs példát választunk, amennyi pozitív van\n",
        "    ind_i = np.random.randint(0, A.shape[0])\n",
        "    ind_j = np.random.randint(0, A.shape[0])\n",
        "    if ind_i == ind_j: continue\n",
        "    # these ones were selected earlier\n",
        "    if str(ind_i) + \" \" + str(ind_j) in str_edges: continue\n",
        "    if str(ind_j) + \" \" + str(ind_i) in str_edges: continue\n",
        "    if str(ind_j) + \" \" + str(ind_i) in str_val_edges_false: continue\n",
        "    if str(ind_i) + \" \" + str(ind_j) in str_val_edges_false: continue\n",
        "    if str(ind_j) + \" \" + str(ind_i) in str_test_edges_false: continue\n",
        "    if str(ind_i) + \" \" + str(ind_j) in str_test_edges_false: continue\n",
        "    # these ones were not\n",
        "    str_val_edges_false.add(str(ind_i) + \" \" + str(ind_j))\n",
        "  val_edges_false = []\n",
        "  for str_edge_false in str_val_edges_false:\n",
        "    edge_false = str_edge_false.split(\" \")\n",
        "    val_edges_false.append([int(edge_false[0]), int(edge_false[1])])\n",
        "  print(\"Validation set is ready!\")\n",
        "  # we are ready\n",
        "  test_edges_false = np.array(test_edges_false)\n",
        "  val_edges_false = np.array(val_edges_false)\n",
        "  return A_train, val_edges, val_edges_false, test_edges, test_edges_false"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p-5rUhczBJdV"
      },
      "source": [
        "# hyperparameters\n",
        "hidden = 512 # number of hidden units in the encoder layer\n",
        "latent = 256 # dimension of the latent variables\n",
        "learning_rate = 0.001\n",
        "epochs = 200\n",
        "nparts = 50 # number of partitions\n",
        "batch_size = 1 # number of clusters per batch\n",
        "K = 3 # degree of polynomial filter"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RflJi-R3BOkG"
      },
      "source": [
        "filename = '/content/drive/MyDrive/GRAPH DATA/ppi.mat' # dataset"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ABjlQkpsDFgx",
        "outputId": "c0f3ec59-cb30-458d-c0b8-44c2565ea041"
      },
      "source": [
        "mat_dict = sio.loadmat(filename)\n",
        "A = mat_dict['A'].ceil()\n",
        "X = mat_dict['X']\n",
        "Y = mat_dict['Y']\n",
        "train_mask = mat_dict['train_mask'].squeeze().astype(bool)\n",
        "val_mask = mat_dict['val_mask'].squeeze().astype(bool)\n",
        "test_mask = mat_dict['test_mask'].squeeze().astype(bool)\n",
        "\n",
        "# selecting the validation and test edges, and the incomplete adjacency matrix for training\n",
        "A_train, val_edges, val_edges_false, test_edges, test_edges_false = mask_test_edges(A)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Selecting the negative test set!\n",
            "Test set is ready!\n",
            "Selecting the negative validation set!\n",
            "Validation set is ready!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a2GhIMjvDNzG",
        "outputId": "c895da22-29ab-4385-d9e1-d0a4122eae84"
      },
      "source": [
        "def cluster_graph(A, nparts):\n",
        "  if nparts == 1:\n",
        "    edge_cuts, parts = 0, [0, ] * A.shape[0]\n",
        "  else:\n",
        "    edge_cuts, parts = metis.part_graph([neighbors for neighbors in A.tolil().rows], nparts=nparts)\n",
        "  print('Number of edge cuts: %d.' % edge_cuts)\n",
        "  cluster_dict = {}\n",
        "  for index, part in enumerate(parts):\n",
        "    if part not in cluster_dict:\n",
        "      cluster_dict[part] = []\n",
        "    cluster_dict[part].append(index)\n",
        "  return cluster_dict\n",
        "\n",
        "# the clustering algorithm (METIS)\n",
        "cluster_dict = cluster_graph(A_train, nparts)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of edge cuts: 264262.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FstzOA7oDgoL"
      },
      "source": [
        "def preprocess_support(A):\n",
        "  N = A.shape[1]\n",
        "  D = sparse.csr_matrix(A.sum(axis=1))\n",
        "  norm = D.power(-0.5)\n",
        "  L = sparse.eye(N, dtype='float32') - A.multiply(norm).T.multiply(norm)\n",
        "  max_eigval = sparse.linalg.eigsh(L, k=1, return_eigenvectors=False)[0]\n",
        "  L_ = 2.0 / max_eigval * L - sparse.eye(N, dtype='float32')\n",
        "  return L_\n",
        "\n",
        "def toTensorSparse(S):\n",
        "  return tf.constant(S.todense())\n",
        "\n",
        "def toTensor(T):\n",
        "  return tf.constant(T)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OVOs3s5TDrHJ"
      },
      "source": [
        "# layer classes\n",
        "\n",
        "class bilinear_layer:\n",
        "\n",
        "  def __init__(self, indim, outdim):\n",
        "    pass\n",
        "\n",
        "  def __call__(self, tensor):\n",
        "    return tf.linalg.matmul(tensor, tf.transpose(tensor))\n",
        "\n",
        "# unused\n",
        "class FC_layer:\n",
        "\n",
        "  def __init__(self, indim, outdim):\n",
        "    initial_value = tf.initializers.he_normal()((indim, outdim,))\n",
        "    self.weight = tf.Variable(initial_value=initial_value, trainable=True)\n",
        "\n",
        "  def __call__(self, tensor):\n",
        "    return tf.linalg.matmul(tensor, self.weight)\n",
        "\n",
        "class GC_layer:\n",
        "\n",
        "  def __init__(self, indim, outdim):\n",
        "    global K\n",
        "    initial_value = tf.initializers.he_normal()((indim, outdim,))\n",
        "    self.weight = tf.Variable(initial_value=initial_value, trainable=True)\n",
        "    delta = np.zeros((K + 1, outdim), dtype='float32')\n",
        "    for o in range(outdim):\n",
        "      delta[0, o] = 1.0\n",
        "    self.coeffs = tf.Variable(initial_value=delta, trainable=True)\n",
        "\n",
        "  def __call__(self, tensor, support, embed=False):\n",
        "    global K\n",
        "    if embed: # numpy pipeline\n",
        "      transform = tensor.numpy().dot(self.weight.numpy())\n",
        "      # Legendre polynomials\n",
        "      basis = [transform]\n",
        "      if (K > 0):\n",
        "        basis.append(support.dot(transform))\n",
        "      if (K > 1):\n",
        "        for k in range(2, K + 1):\n",
        "          basis.append((2.0 * k - 1.0) / k * support.dot(basis[k-1]) - (k - 1.0) / k * basis[k-2])\n",
        "      # linear combination\n",
        "      result = np.zeros(transform.shape)\n",
        "      for coeff, base in zip(self.coeffs.numpy(), basis):\n",
        "        result += base * coeff\n",
        "      return result\n",
        "    else: # tensorflow pipeline\n",
        "      transform = tf.linalg.matmul(tensor, self.weight)\n",
        "      # Legendre polynomials\n",
        "      basis = [transform]\n",
        "      if (K > 0):\n",
        "        basis.append(tf.linalg.matmul(support, transform))\n",
        "      if (K > 1):\n",
        "        for k in range(2, K + 1):\n",
        "          basis.append((2.0 * k - 1.0) / k * tf.linalg.matmul(support, basis[k-1]) - (k - 1.0) / k * basis[k-2])\n",
        "      # linear combination\n",
        "      result = tf.zeros(transform.shape)\n",
        "      for k in range(K + 1):\n",
        "        result += self.coeffs[k] * basis[k]\n",
        "      return result"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qwEu7WB9EWAI"
      },
      "source": [
        "# our model class (for the paper \"Scalable Graph Variational Autoencoders\")\n",
        "\n",
        "class Model:\n",
        "\n",
        "  def __init__(self, size_tuple, optimizer, nonlinear):\n",
        "    self.sources = [] # variables to optimize\n",
        "    self.build(size_tuple) # builds the model by stacking layers on each other\n",
        "    self.optimizer = optimizer\n",
        "    self.nonlinear = nonlinear\n",
        "    self.Z_mean = None # mean embedding layer\n",
        "    self.Z_var = None # variance embedding layer\n",
        "    self.noise = None # the noise sample\n",
        "    self.sample = None # self.Z_mean + self.Z_var * self.noise\n",
        "    self.A_gamma = None # the reconstructions\n",
        "  \n",
        "  def build(self, size_tuple):\n",
        "    X_dim, hidden, latent = size_tuple\n",
        "    self.enc_layer = GC_layer(X_dim, hidden)\n",
        "    self.enc_mean_layer = GC_layer(hidden, latent)\n",
        "    self.enc_var_layer = GC_layer(hidden, latent)\n",
        "    self.A_dec_gamma_layer = bilinear_layer(latent, latent)\n",
        "    # filling the source array with weights\n",
        "    layers = [self.enc_layer, self.enc_mean_layer, self.enc_var_layer]\n",
        "    for layer in layers:\n",
        "      self.sources.append(layer.weight)\n",
        "      self.sources.append(layer.coeffs)\n",
        "  \n",
        "  # forward propagation in the encoder\n",
        "  def encode(self, X, S):\n",
        "    enc = self.nonlinear(self.enc_layer(X, S))\n",
        "    enc_mean = self.enc_mean_layer(enc, S)\n",
        "    enc_var = tf.math.exp(self.enc_var_layer(enc, S))\n",
        "    return enc_mean, enc_var\n",
        "\n",
        "  # returns only the node embeddings\n",
        "  def embed(self, X, S):\n",
        "    enc = self.nonlinear(self.enc_layer(X, S, embed=True))\n",
        "    enc_mean = self.enc_mean_layer(enc, S, embed=True)\n",
        "    return enc_mean\n",
        "\n",
        "  # forward propagation in the decoder\n",
        "  def decode(self, sample):\n",
        "    A_dec_gamma = self.A_dec_gamma_layer(sample)\n",
        "    return A_dec_gamma\n",
        "\n",
        "  def predict(self, X, S):\n",
        "    self.Z_mean, self.Z_var = self.encode(X, S)\n",
        "    self.noise = tf.random.normal(self.Z_var.shape)\n",
        "    self.sample = self.Z_mean + self.Z_var * self.noise # reparameterization trick\n",
        "    self.A_gamma = self.decode(self.sample)\n",
        "\n",
        "  def train(self, X, A, val_edges, val_edges_false, cluster_dict, batch_size, epochs):\n",
        "    for epoch in range(epochs):\n",
        "      # only a subgraph is used in the training process\n",
        "      samples = random.sample(cluster_dict.keys(), batch_size)\n",
        "      nodes = sum([cluster_dict[sample] for sample in samples], [])\n",
        "      S_batch = toTensorSparse(preprocess_support(A[nodes].T[nodes]))\n",
        "      A_batch = toTensor(A.T[nodes].T[nodes].todense())\n",
        "      X_batch = tf.math.l2_normalize(toTensor(X[nodes]), axis=1)\n",
        "      # optimization\n",
        "      with tf.GradientTape() as tape:\n",
        "        self.predict(X_batch, S_batch)\n",
        "        losses = self.loss(A_batch, X_batch)\n",
        "        loss_ = tf.reduce_sum(losses)\n",
        "      print(epoch, [loss.numpy() for loss in losses], loss_.numpy())\n",
        "      grads = tape.gradient(loss_, self.sources)\n",
        "      self.optimizer.apply_gradients(zip(grads, self.sources))\n",
        "\n",
        "  def test(self, X, A, test_edges, test_edges_false):\n",
        "    S_test = preprocess_support(A)\n",
        "    X_test = tf.math.l2_normalize(toTensor(X), axis=1)\n",
        "    self.Z_mean = self.embed(X_test, S_test)\n",
        "    roc_auc, pr_auc = self.accuracy(test_edges, test_edges_false)\n",
        "    print(roc_auc, pr_auc)\n",
        "\n",
        "  # Kullback–Leibler divergence\n",
        "  def KL_Divergence(self):\n",
        "    loss = 0.5 * tf.reduce_mean(self.Z_mean**2.0 + self.Z_var**2.0 - 2.0 * tf.math.log(self.Z_var) - 1.0)\n",
        "    return loss\n",
        "\n",
        "  # reconstruction loss\n",
        "  def re_A_loss(self, A):\n",
        "    density = tf.reduce_sum(A) / tf.size(A, out_type=tf.float32)\n",
        "    pos_weight = (1.0 - density) / density\n",
        "    loss = -0.5 * tf.reduce_mean(1.0 / (1.0 - density) * tf.nn.weighted_cross_entropy_with_logits(labels=A, logits=self.A_gamma, pos_weight=pos_weight))\n",
        "    return -loss\n",
        "\n",
        "  # list of all loss functions\n",
        "  def loss(self, A, X):\n",
        "    return self.KL_Divergence(), self.re_A_loss(A)\n",
        "  \n",
        "  # through the ratio parameter, the number of edges used for validation/testing can be adjusted\n",
        "  def accuracy(self, edges_pos, edges_neg, ratio=1.0):\n",
        "    A_dec = self.Z_mean\n",
        "    #print(\"positive samples\")\n",
        "    p = np.random.permutation(len(edges_pos))\n",
        "    limit = round(ratio * len(edges_pos))\n",
        "    left_pos = []\n",
        "    right_pos = []\n",
        "    for edge in edges_pos[p][:limit]:\n",
        "      left_pos.append(A_dec[edge[0], :])\n",
        "      right_pos.append(A_dec[edge[1], :])\n",
        "    re_pos = tf.nn.sigmoid(tf.einsum('ij, ij -> i', tf.stack(left_pos), tf.stack(right_pos)))\n",
        "    #print(\"negative samples\")\n",
        "    p = np.random.permutation(len(edges_neg))\n",
        "    limit = round(ratio * len(edges_neg))\n",
        "    left_neg = []\n",
        "    right_neg = []\n",
        "    for edge in edges_neg[p][:limit]:\n",
        "      left_neg.append(A_dec[edge[0], :])\n",
        "      right_neg.append(A_dec[edge[1], :])\n",
        "    re_neg = tf.nn.sigmoid(tf.einsum('ij, ij -> i', tf.stack(left_neg), tf.stack(right_neg)))\n",
        "    #print(\"stacking all\")\n",
        "    re_all = tf.stack([re_pos, re_neg])\n",
        "    all = tf.stack([tf.ones(len(re_pos)), tf.zeros(len(re_neg))])\n",
        "    from sklearn.metrics import roc_auc_score, average_precision_score\n",
        "    #print(\"metrics evaluation\")\n",
        "    return roc_auc_score(all, re_all), average_precision_score(all, re_all)\n"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0zuy3DUXLK4J",
        "outputId": "e006f114-980f-42eb-bd8b-45cd0e837ce8"
      },
      "source": [
        "size_tuple = (X.shape[1], hidden, latent)\n",
        "optimizer = tf.optimizers.Adam(learning_rate=learning_rate)\n",
        "nonlinear = tf.nn.relu\n",
        "\n",
        "model = Model(size_tuple, optimizer, nonlinear)\n",
        "\n",
        "print('Training...')\n",
        "model.train(X, A_train, val_edges, val_edges_false, cluster_dict, batch_size, epochs)\n",
        "print('Testing...')\n",
        "model.test(X, A_train, test_edges, test_edges_false)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training...\n",
            "0 [0.06464395, 7.707883] 7.7725267\n",
            "1 [0.058087215, 6.772413] 6.8305\n",
            "2 [0.057125974, 5.877184] 5.93431\n",
            "3 [0.06047299, 5.449503] 5.509976\n",
            "4 [0.0675751, 5.207366] 5.274941\n",
            "5 [0.07742687, 4.5501184] 4.6275454\n",
            "6 [0.089785315, 4.436356] 4.526141\n",
            "7 [0.10496212, 3.9020765] 4.0070386\n",
            "8 [0.12179014, 3.7572207] 3.879011\n",
            "9 [0.14117445, 3.3899853] 3.5311599\n",
            "10 [0.16182041, 3.2826173] 3.4444377\n",
            "11 [0.18252866, 3.0011227] 3.1836514\n",
            "12 [0.20642927, 2.8541708] 3.0606\n",
            "13 [0.23213482, 2.5913901] 2.823525\n",
            "14 [0.25327024, 2.4665341] 2.7198043\n",
            "15 [0.28471896, 2.1717014] 2.4564204\n",
            "16 [0.3092684, 2.1525495] 2.461818\n",
            "17 [0.3397213, 1.9127606] 2.252482\n",
            "18 [0.36694536, 1.889] 2.2559454\n",
            "19 [0.39211866, 1.767612] 2.1597307\n",
            "20 [0.42995787, 1.6458273] 2.0757852\n",
            "21 [0.45844454, 1.5241292] 1.9825737\n",
            "22 [0.47774345, 1.4870636] 1.964807\n",
            "23 [0.50975895, 1.3781377] 1.8878967\n",
            "24 [0.52813816, 1.3630139] 1.891152\n",
            "25 [0.5665404, 1.2669294] 1.8334699\n",
            "26 [0.59552133, 1.2062231] 1.8017445\n",
            "27 [0.6004503, 1.2162464] 1.8166966\n",
            "28 [0.63611597, 1.1171366] 1.7532525\n",
            "29 [0.64803296, 1.1469512] 1.7949841\n",
            "30 [0.67374855, 1.1017238] 1.7754724\n",
            "31 [0.6856815, 1.0849234] 1.7706048\n",
            "32 [0.70921785, 1.0436014] 1.7528193\n",
            "33 [0.7114564, 1.0470073] 1.7584637\n",
            "34 [0.7245822, 1.0394714] 1.7640536\n",
            "35 [0.7376539, 1.0029742] 1.740628\n",
            "36 [0.75910485, 0.9892285] 1.7483333\n",
            "37 [0.7647654, 0.9720272] 1.7367926\n",
            "38 [0.777803, 0.91946435] 1.6972673\n",
            "39 [0.7672458, 0.9775716] 1.7448175\n",
            "40 [0.7714234, 0.95682746] 1.7282509\n",
            "41 [0.7879379, 0.90813804] 1.6960759\n",
            "42 [0.7885311, 0.9194349] 1.7079661\n",
            "43 [0.77767575, 0.93894374] 1.7166195\n",
            "44 [0.7869416, 0.9216911] 1.7086327\n",
            "45 [0.7987835, 0.9130965] 1.71188\n",
            "46 [0.7734374, 0.9310002] 1.7044375\n",
            "47 [0.77135694, 0.9231316] 1.6944885\n",
            "48 [0.76873994, 0.9335126] 1.7022526\n",
            "49 [0.7763416, 0.9232008] 1.6995424\n",
            "50 [0.74867713, 0.94721866] 1.6958958\n",
            "51 [0.75593984, 0.9373853] 1.6933252\n",
            "52 [0.76548266, 0.9025592] 1.668042\n",
            "53 [0.7437049, 0.9360794] 1.6797843\n",
            "54 [0.74898154, 0.9050488] 1.6540303\n",
            "55 [0.73620415, 0.9438891] 1.6800933\n",
            "56 [0.7254747, 0.9488168] 1.6742915\n",
            "57 [0.7256516, 0.9482511] 1.6739028\n",
            "58 [0.71677434, 0.9539221] 1.6706965\n",
            "59 [0.71399057, 0.95017177] 1.6641624\n",
            "60 [0.7119357, 0.9238028] 1.6357385\n",
            "61 [0.7082402, 0.94726294] 1.6555032\n",
            "62 [0.706281, 0.9726326] 1.6789136\n",
            "63 [0.6991782, 0.9760016] 1.6751798\n",
            "64 [0.7016038, 0.97230726] 1.6739111\n",
            "65 [0.68891686, 0.98620903] 1.6751258\n",
            "66 [0.6955055, 0.9401806] 1.6356862\n",
            "67 [0.69929177, 0.9356425] 1.6349342\n",
            "68 [0.68867815, 0.9642897] 1.6529679\n",
            "69 [0.686534, 0.9824766] 1.6690106\n",
            "70 [0.68053675, 0.977444] 1.6579807\n",
            "71 [0.68258697, 0.97049326] 1.6530802\n",
            "72 [0.67916423, 0.98314154] 1.6623058\n",
            "73 [0.6805265, 0.96809876] 1.6486253\n",
            "74 [0.6907518, 0.9425905] 1.6333423\n",
            "75 [0.67876345, 0.9743182] 1.6530817\n",
            "76 [0.6801074, 0.9654486] 1.645556\n",
            "77 [0.6800125, 0.96659565] 1.6466081\n",
            "78 [0.68063045, 0.96566164] 1.6462921\n",
            "79 [0.6828991, 0.9677835] 1.6506827\n",
            "80 [0.692654, 0.9450308] 1.6376848\n",
            "81 [0.68277395, 0.95401484] 1.6367888\n",
            "82 [0.6823767, 0.9145109] 1.5968876\n",
            "83 [0.68431234, 0.9583673] 1.6426797\n",
            "84 [0.6778985, 0.95657223] 1.6344707\n",
            "85 [0.68175226, 0.96221024] 1.6439625\n",
            "86 [0.68465245, 0.95794815] 1.6426005\n",
            "87 [0.6908794, 0.95240986] 1.6432893\n",
            "88 [0.6913018, 0.95009434] 1.6413962\n",
            "89 [0.68906915, 0.9532446] 1.6423137\n",
            "90 [0.686321, 0.95088565] 1.6372067\n",
            "91 [0.6884542, 0.9510756] 1.6395298\n",
            "92 [0.6863173, 0.95005524] 1.6363726\n",
            "93 [0.6978542, 0.9210932] 1.6189475\n",
            "94 [0.68876165, 0.9432864] 1.6320481\n",
            "95 [0.6892475, 0.9334796] 1.6227272\n",
            "96 [0.6899713, 0.9443626] 1.6343338\n",
            "97 [0.6892642, 0.9466132] 1.6358774\n",
            "98 [0.69372755, 0.9411751] 1.6349027\n",
            "99 [0.6879251, 0.9005959] 1.588521\n",
            "100 [0.6919824, 0.929673] 1.6216555\n",
            "101 [0.6893095, 0.93036366] 1.6196731\n",
            "102 [0.68812776, 0.9283921] 1.6165199\n",
            "103 [0.691694, 0.942274] 1.633968\n",
            "104 [0.68960917, 0.9394125] 1.6290216\n",
            "105 [0.68744797, 0.93850297] 1.6259509\n",
            "106 [0.6928998, 0.93455213] 1.6274519\n",
            "107 [0.69347936, 0.9383889] 1.6318682\n",
            "108 [0.6874072, 0.9267537] 1.6141609\n",
            "109 [0.68733335, 0.9299061] 1.6172395\n",
            "110 [0.6861034, 0.8958299] 1.5819333\n",
            "111 [0.6904814, 0.92578685] 1.6162683\n",
            "112 [0.68865347, 0.93717873] 1.6258322\n",
            "113 [0.68755144, 0.92019546] 1.6077468\n",
            "114 [0.69138336, 0.9056944] 1.5970778\n",
            "115 [0.68958217, 0.9219535] 1.6115357\n",
            "116 [0.6873167, 0.9263896] 1.6137064\n",
            "117 [0.6967055, 0.90017176] 1.5968773\n",
            "118 [0.69109917, 0.92147875] 1.6125779\n",
            "119 [0.6853012, 0.91241443] 1.5977156\n",
            "120 [0.6867919, 0.9369441] 1.623736\n",
            "121 [0.6905885, 0.92513925] 1.6157277\n",
            "122 [0.683114, 0.9314148] 1.6145288\n",
            "123 [0.6790512, 0.89184767] 1.5708989\n",
            "124 [0.6857627, 0.9188598] 1.6046225\n",
            "125 [0.69307154, 0.9091554] 1.602227\n",
            "126 [0.6923154, 0.9121746] 1.60449\n",
            "127 [0.67974645, 0.9185477] 1.5982941\n",
            "128 [0.68718904, 0.9005998] 1.5877888\n",
            "129 [0.6789348, 0.9234381] 1.6023729\n",
            "130 [0.67486066, 0.88424575] 1.5591063\n",
            "131 [0.678405, 0.91094023] 1.5893452\n",
            "132 [0.6892482, 0.91048855] 1.5997367\n",
            "133 [0.68089795, 0.91045225] 1.5913502\n",
            "134 [0.6895938, 0.9202196] 1.6098135\n",
            "135 [0.6802218, 0.91019464] 1.5904164\n",
            "136 [0.674164, 0.91763276] 1.5917968\n",
            "137 [0.67549473, 0.9174959] 1.5929906\n",
            "138 [0.68533516, 0.8991078] 1.584443\n",
            "139 [0.6834002, 0.91772246] 1.6011226\n",
            "140 [0.68344504, 0.9225217] 1.6059668\n",
            "141 [0.67535186, 0.9073842] 1.582736\n",
            "142 [0.68061167, 0.91380143] 1.594413\n",
            "143 [0.6780752, 0.9054025] 1.5834777\n",
            "144 [0.67326033, 0.8759756] 1.5492359\n",
            "145 [0.6814567, 0.8917314] 1.5731881\n",
            "146 [0.6804951, 0.9238809] 1.604376\n",
            "147 [0.6903747, 0.88532] 1.5756947\n",
            "148 [0.68598557, 0.9089113] 1.5948968\n",
            "149 [0.6809421, 0.90228754] 1.5832297\n",
            "150 [0.6790153, 0.91019756] 1.5892129\n",
            "151 [0.6777885, 0.8878356] 1.5656241\n",
            "152 [0.6793589, 0.9046096] 1.5839685\n",
            "153 [0.6784152, 0.8777867] 1.5562019\n",
            "154 [0.680889, 0.9227607] 1.6036497\n",
            "155 [0.67668945, 0.90137655] 1.578066\n",
            "156 [0.67873716, 0.89768666] 1.5764239\n",
            "157 [0.6914295, 0.8736938] 1.5651233\n",
            "158 [0.6813435, 0.9022336] 1.5835772\n",
            "159 [0.69118387, 0.8833864] 1.5745702\n",
            "160 [0.69129837, 0.8835005] 1.5747988\n",
            "161 [0.6824544, 0.90947545] 1.5919299\n",
            "162 [0.67059636, 0.8751068] 1.5457032\n",
            "163 [0.6755188, 0.8767497] 1.5522685\n",
            "164 [0.67365766, 0.8953685] 1.5690262\n",
            "165 [0.6878175, 0.8779458] 1.5657632\n",
            "166 [0.67472607, 0.89047414] 1.5652002\n",
            "167 [0.6759334, 0.8840279] 1.5599613\n",
            "168 [0.679399, 0.8919572] 1.5713563\n",
            "169 [0.66480607, 0.8747442] 1.5395503\n",
            "170 [0.67448527, 0.8910862] 1.5655715\n",
            "171 [0.6722617, 0.89459693] 1.5668586\n",
            "172 [0.67804676, 0.89218056] 1.5702274\n",
            "173 [0.6745898, 0.89401454] 1.5686044\n",
            "174 [0.66684127, 0.88775325] 1.5545945\n",
            "175 [0.6699052, 0.86654526] 1.5364504\n",
            "176 [0.6712553, 0.87258375] 1.543839\n",
            "177 [0.6784852, 0.8804554] 1.5589406\n",
            "178 [0.6829061, 0.86851096] 1.5514171\n",
            "179 [0.67190915, 0.9061014] 1.5780106\n",
            "180 [0.66449565, 0.8995646] 1.5640602\n",
            "181 [0.668443, 0.86190754] 1.5303506\n",
            "182 [0.6709451, 0.8628661] 1.5338112\n",
            "183 [0.67594564, 0.87717265] 1.5531182\n",
            "184 [0.6810077, 0.88579106] 1.5667987\n",
            "185 [0.66369504, 0.88217336] 1.5458684\n",
            "186 [0.6667714, 0.86869144] 1.5354629\n",
            "187 [0.66383976, 0.8739346] 1.5377743\n",
            "188 [0.6744768, 0.8722212] 1.546698\n",
            "189 [0.66248083, 0.86987096] 1.5323517\n",
            "190 [0.66965437, 0.88949984] 1.5591543\n",
            "191 [0.6676075, 0.87118906] 1.5387965\n",
            "192 [0.66773176, 0.845815] 1.5135467\n",
            "193 [0.67283785, 0.89080495] 1.5636427\n",
            "194 [0.6630274, 0.88939375] 1.5524211\n",
            "195 [0.6611405, 0.87766457] 1.538805\n",
            "196 [0.66693026, 0.87713975] 1.54407\n",
            "197 [0.6671142, 0.87350285] 1.540617\n",
            "198 [0.6689102, 0.84738255] 1.5162928\n",
            "199 [0.66813743, 0.8840017] 1.552139\n",
            "Testing...\n",
            "0.8027873117465281 0.901393655873264\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}