{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "smoothing.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "zzzB2uJwBC-m"
      },
      "source": [
        "!sudo apt-get install libmetis-dev\n",
        "!pip install metis\n",
        "import metis\n",
        "import random\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from scipy import sparse\n",
        "import scipy.io as sio"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j2yJoBMsBFa9"
      },
      "source": [
        "def mask_test_edges(A):\n",
        "  A_triu = sparse.triu(A)\n",
        "  edges = np.stack(A_triu.nonzero()).T # all edges of the graph\n",
        "  num_val = int(0.05 * edges.shape[0]) # 5% of the edges for validation\n",
        "  num_test = int(0.1 * edges.shape[0]) # 10% of the edges for testing\n",
        "  edge_ind = np.arange(edges.shape[0]) # indices of the edges\n",
        "  np.random.shuffle(edge_ind) # shuffling the indices\n",
        "  val_edge_ind = edge_ind[:num_val] # under 5%: indices for validation\n",
        "  test_edge_ind = edge_ind[num_val:(num_val + num_test)] # 5-15%: indices for testing\n",
        "  train_edge_ind = edge_ind[(num_val + num_test):] # over 15%: indices for training\n",
        "  val_edges = edges[val_edge_ind]\n",
        "  test_edges = edges[test_edge_ind]\n",
        "  train_edges = edges[train_edge_ind]\n",
        "  # the incomplete adjacency matrix for training\n",
        "  arg1 = (np.ones(train_edges.shape[0]), (train_edges[:, 0], train_edges[:, 1]))\n",
        "  A_train_triu = sparse.csr_matrix(arg1, shape=A.shape, dtype='float32')\n",
        "  A_train = A_train_triu + A_train_triu.T\n",
        "  edges = edges.tolist()\n",
        "  str_edges = set(str(edge[0]) + \" \" + str(edge[1]) for edge in edges)\n",
        "  print(\"Selecting the negative test set!\")\n",
        "  str_test_edges_false = set()\n",
        "  while len(str_test_edges_false) < len(test_edges): # picking the same number of negative test edges\n",
        "    ind_i = np.random.randint(0, A.shape[0])\n",
        "    ind_j = np.random.randint(0, A.shape[0])\n",
        "    if ind_i == ind_j: continue\n",
        "    # these ones were selected earlier\n",
        "    if str(ind_i) + \" \" + str(ind_j) in str_edges: continue\n",
        "    if str(ind_j) + \" \" + str(ind_i) in str_edges: continue\n",
        "    if str(ind_j) + \" \" + str(ind_i) in str_test_edges_false: continue\n",
        "    if str(ind_i) + \" \" + str(ind_j) in str_test_edges_false: continue\n",
        "    # these ones were not\n",
        "    str_test_edges_false.add(str(ind_i) + \" \" + str(ind_j))\n",
        "  test_edges_false = []\n",
        "  for str_edge_false in str_test_edges_false:\n",
        "    edge_false = str_edge_false.split(\" \")\n",
        "    test_edges_false.append([int(edge_false[0]), int(edge_false[1])])\n",
        "  print(\"Test set is ready!\")\n",
        "  print(\"Selecting the negative validation set!\")\n",
        "  str_val_edges_false = set()\n",
        "  while len(str_val_edges_false) < len(val_edges): # annyi negatív validációs példát választunk, amennyi pozitív van\n",
        "    ind_i = np.random.randint(0, A.shape[0])\n",
        "    ind_j = np.random.randint(0, A.shape[0])\n",
        "    if ind_i == ind_j: continue\n",
        "    # these ones were selected earlier\n",
        "    if str(ind_i) + \" \" + str(ind_j) in str_edges: continue\n",
        "    if str(ind_j) + \" \" + str(ind_i) in str_edges: continue\n",
        "    if str(ind_j) + \" \" + str(ind_i) in str_val_edges_false: continue\n",
        "    if str(ind_i) + \" \" + str(ind_j) in str_val_edges_false: continue\n",
        "    if str(ind_j) + \" \" + str(ind_i) in str_test_edges_false: continue\n",
        "    if str(ind_i) + \" \" + str(ind_j) in str_test_edges_false: continue\n",
        "    # these ones were not\n",
        "    str_val_edges_false.add(str(ind_i) + \" \" + str(ind_j))\n",
        "  val_edges_false = []\n",
        "  for str_edge_false in str_val_edges_false:\n",
        "    edge_false = str_edge_false.split(\" \")\n",
        "    val_edges_false.append([int(edge_false[0]), int(edge_false[1])])\n",
        "  print(\"Validation set is ready!\")\n",
        "  # we are ready\n",
        "  test_edges_false = np.array(test_edges_false)\n",
        "  val_edges_false = np.array(val_edges_false)\n",
        "  return A_train, val_edges, val_edges_false, test_edges, test_edges_false"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p-5rUhczBJdV"
      },
      "source": [
        "# hyperparameters\n",
        "hidden = 512 # number of hidden units in the encoder layer\n",
        "latent = 256 # dimension of the latent variables\n",
        "learning_rate = 0.001\n",
        "epochs = 200\n",
        "nparts = 50 # number of partitions\n",
        "batch_size = 1 # number of clusters per batch"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RflJi-R3BOkG"
      },
      "source": [
        "filename = '/content/drive/MyDrive/GRAPH DATA/ppi.mat' # dataset"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ABjlQkpsDFgx",
        "outputId": "88336315-721d-4dcd-c43f-6da43eece43f"
      },
      "source": [
        "mat_dict = sio.loadmat(filename)\n",
        "A = mat_dict['A'].ceil()\n",
        "X = mat_dict['X']\n",
        "Y = mat_dict['Y']\n",
        "train_mask = mat_dict['train_mask'].squeeze().astype(bool)\n",
        "val_mask = mat_dict['val_mask'].squeeze().astype(bool)\n",
        "test_mask = mat_dict['test_mask'].squeeze().astype(bool)\n",
        "\n",
        "# selecting the validation and test edges, and the incomplete adjacency matrix for training\n",
        "A_train, val_edges, val_edges_false, test_edges, test_edges_false = mask_test_edges(A)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Selecting the negative test set!\n",
            "Test set is ready!\n",
            "Selecting the negative validation set!\n",
            "Validation set is ready!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a2GhIMjvDNzG",
        "outputId": "87be7685-57ef-4b2b-9262-04009c09b8e1"
      },
      "source": [
        "def cluster_graph(A, nparts):\n",
        "  if nparts == 1:\n",
        "    edge_cuts, parts = 0, [0, ] * A.shape[0]\n",
        "  else:\n",
        "    edge_cuts, parts = metis.part_graph([neighbors for neighbors in A.tolil().rows], nparts=nparts)\n",
        "  print('Number of edge cuts: %d.' % edge_cuts)\n",
        "  cluster_dict = {}\n",
        "  for index, part in enumerate(parts):\n",
        "    if part not in cluster_dict:\n",
        "      cluster_dict[part] = []\n",
        "    cluster_dict[part].append(index)\n",
        "  return cluster_dict\n",
        "\n",
        "# the clustering algorithm (METIS)\n",
        "cluster_dict = cluster_graph(A_train, nparts)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of edge cuts: 256396.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FstzOA7oDgoL"
      },
      "source": [
        "def preprocess_support(A):\n",
        "  N = A.shape[1]\n",
        "  A_I = A + sparse.eye(N, dtype='float32')\n",
        "  D_I = sparse.csr_matrix(A_I.sum(axis=1))\n",
        "  norm = D_I.power(-0.5)\n",
        "  return A_I.multiply(norm).T.multiply(norm)\n",
        "\n",
        "def toTensorSparse(S):\n",
        "  return tf.constant(S.todense())\n",
        "\n",
        "def toTensor(T):\n",
        "  return tf.constant(T)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OVOs3s5TDrHJ"
      },
      "source": [
        "# layer classes\n",
        "\n",
        "class bilinear_layer:\n",
        "\n",
        "  def __init__(self, indim, outdim):\n",
        "    pass\n",
        "\n",
        "  def __call__(self, tensor):\n",
        "    return tf.linalg.matmul(tensor, tf.transpose(tensor))\n",
        "\n",
        "# unused\n",
        "class FC_layer:\n",
        "\n",
        "  def __init__(self, indim, outdim):\n",
        "    initial_value = tf.initializers.he_normal()((indim, outdim,))\n",
        "    self.weight = tf.Variable(initial_value=initial_value, trainable=True)\n",
        "\n",
        "  def __call__(self, tensor):\n",
        "    return tf.linalg.matmul(tensor, self.weight)\n",
        "\n",
        "class GC_layer:\n",
        "\n",
        "  def __init__(self, indim, outdim):\n",
        "    initial_value = tf.initializers.he_normal()((indim, outdim,))\n",
        "    self.weight = tf.Variable(initial_value=initial_value, trainable=True)\n",
        "\n",
        "  def __call__(self, tensor, support, embed=False):\n",
        "    if embed: # numpy pipeline\n",
        "      return support.dot(tensor.numpy().dot(self.weight.numpy()))\n",
        "    else: # tensorflow pipeline\n",
        "      return tf.linalg.matmul(support, tf.linalg.matmul(tensor, self.weight))\n"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qwEu7WB9EWAI"
      },
      "source": [
        "# our model class (for the paper \"Scalable Graph Variational Autoencoders\")\n",
        "\n",
        "class Model:\n",
        "\n",
        "  def __init__(self, size_tuple, optimizer, nonlinear):\n",
        "    self.sources = [] # variables to optimize\n",
        "    self.build(size_tuple) # builds the model by stacking layers on each other\n",
        "    self.optimizer = optimizer\n",
        "    self.nonlinear = nonlinear\n",
        "    self.Z_mean = None # mean embedding layer\n",
        "    self.Z_var = None # variance embedding layer\n",
        "    self.noise = None # the noise sample\n",
        "    self.sample = None # self.Z_mean + self.Z_var * self.noise\n",
        "    self.A_gamma = None # the reconstructions\n",
        "  \n",
        "  def build(self, size_tuple):\n",
        "    X_dim, hidden, latent = size_tuple\n",
        "    self.enc_layer = GC_layer(X_dim, hidden)\n",
        "    self.enc_mean_layer = GC_layer(hidden, latent)\n",
        "    self.enc_var_layer = GC_layer(hidden, latent)\n",
        "    self.A_dec_gamma_layer = bilinear_layer(latent, latent)\n",
        "    # filling the source array with weights\n",
        "    layers = [self.enc_layer, self.enc_mean_layer, self.enc_var_layer]\n",
        "    for layer in layers:\n",
        "      self.sources.append(layer.weight)\n",
        "  \n",
        "  # forward propagation in the encoder\n",
        "  def encode(self, X, S):\n",
        "    enc = self.nonlinear(self.enc_layer(X, S))\n",
        "    enc_mean = self.enc_mean_layer(enc, S)\n",
        "    enc_var = tf.math.exp(self.enc_var_layer(enc, S))\n",
        "    return enc_mean, enc_var\n",
        "\n",
        "  # returns only the node embeddings\n",
        "  def embed(self, X, S):\n",
        "    enc = self.nonlinear(self.enc_layer(X, S, embed=True))\n",
        "    enc_mean = self.enc_mean_layer(enc, S, embed=True)\n",
        "    return enc_mean\n",
        "\n",
        "  # forward propagation in the decoder\n",
        "  def decode(self, sample):\n",
        "    A_dec_gamma = self.A_dec_gamma_layer(sample)\n",
        "    return A_dec_gamma\n",
        "\n",
        "  def predict(self, X, S):\n",
        "    self.Z_mean, self.Z_var = self.encode(X, S)\n",
        "    self.noise = tf.random.normal(self.Z_var.shape)\n",
        "    self.sample = self.Z_mean + self.Z_var * self.noise # reparameterization trick\n",
        "    self.A_gamma = self.decode(self.sample)\n",
        "\n",
        "  def train(self, X, A, val_edges, val_edges_false, cluster_dict, batch_size, epochs):\n",
        "    for epoch in range(epochs):\n",
        "      # only a subgraph is used in the training process\n",
        "      samples = random.sample(cluster_dict.keys(), batch_size)\n",
        "      nodes = sum([cluster_dict[sample] for sample in samples], [])\n",
        "      S_batch = toTensorSparse(preprocess_support(A[nodes].T[nodes]))\n",
        "      A_batch = toTensor(A.T[nodes].T[nodes].todense())\n",
        "      X_batch = tf.math.l2_normalize(toTensor(X[nodes]), axis=1)\n",
        "      # optimization\n",
        "      with tf.GradientTape() as tape:\n",
        "        self.predict(X_batch, S_batch)\n",
        "        losses = self.loss(A_batch, X_batch)\n",
        "        loss_ = tf.reduce_sum(losses)\n",
        "      print(epoch, [loss.numpy() for loss in losses], loss_.numpy())\n",
        "      grads = tape.gradient(loss_, self.sources)\n",
        "      self.optimizer.apply_gradients(zip(grads, self.sources))\n",
        "\n",
        "  def test(self, X, A, test_edges, test_edges_false):\n",
        "    S_test = preprocess_support(A)\n",
        "    X_test = tf.math.l2_normalize(toTensor(X), axis=1)\n",
        "    self.Z_mean = self.embed(X_test, S_test)\n",
        "    roc_auc, pr_auc = self.accuracy(test_edges, test_edges_false)\n",
        "    print(roc_auc, pr_auc)\n",
        "\n",
        "  # Kullback–Leibler divergence\n",
        "  def KL_Divergence(self):\n",
        "    loss = 0.5 * tf.reduce_mean(self.Z_mean**2.0 + self.Z_var**2.0 - 2.0 * tf.math.log(self.Z_var) - 1.0)\n",
        "    return loss\n",
        "\n",
        "  # reconstruction loss\n",
        "  def re_A_loss(self, A):\n",
        "    density = tf.reduce_sum(A) / tf.size(A, out_type=tf.float32)\n",
        "    pos_weight = (1.0 - density) / density\n",
        "    loss = -0.5 * tf.reduce_mean(1.0 / (1.0 - density) * tf.nn.weighted_cross_entropy_with_logits(labels=A, logits=self.A_gamma, pos_weight=pos_weight))\n",
        "    return -loss\n",
        "\n",
        "  # list of all loss functions\n",
        "  def loss(self, A, X):\n",
        "    return self.KL_Divergence(), self.re_A_loss(A)\n",
        "  \n",
        "  # through the ratio parameter, the number of edges used for validation/testing can be adjusted\n",
        "  def accuracy(self, edges_pos, edges_neg, ratio=1.0):\n",
        "    A_dec = self.Z_mean\n",
        "    #print(\"positive samples\")\n",
        "    p = np.random.permutation(len(edges_pos))\n",
        "    limit = round(ratio * len(edges_pos))\n",
        "    left_pos = []\n",
        "    right_pos = []\n",
        "    for edge in edges_pos[p][:limit]:\n",
        "      left_pos.append(A_dec[edge[0], :])\n",
        "      right_pos.append(A_dec[edge[1], :])\n",
        "    re_pos = tf.nn.sigmoid(tf.einsum('ij, ij -> i', tf.stack(left_pos), tf.stack(right_pos)))\n",
        "    #print(\"negative samples\")\n",
        "    p = np.random.permutation(len(edges_neg))\n",
        "    limit = round(ratio * len(edges_neg))\n",
        "    left_neg = []\n",
        "    right_neg = []\n",
        "    for edge in edges_neg[p][:limit]:\n",
        "      left_neg.append(A_dec[edge[0], :])\n",
        "      right_neg.append(A_dec[edge[1], :])\n",
        "    re_neg = tf.nn.sigmoid(tf.einsum('ij, ij -> i', tf.stack(left_neg), tf.stack(right_neg)))\n",
        "    #print(\"stacking all\")\n",
        "    re_all = tf.stack([re_pos, re_neg])\n",
        "    all = tf.stack([tf.ones(len(re_pos)), tf.zeros(len(re_neg))])\n",
        "    from sklearn.metrics import roc_auc_score, average_precision_score\n",
        "    #print(\"metrics evaluation\")\n",
        "    return roc_auc_score(all, re_all), average_precision_score(all, re_all)\n"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0zuy3DUXLK4J",
        "outputId": "c331b28f-deb5-4ca1-9eef-f057078ce2bb"
      },
      "source": [
        "size_tuple = (X.shape[1], hidden, latent)\n",
        "optimizer = tf.optimizers.Adam(learning_rate=learning_rate)\n",
        "nonlinear = tf.nn.relu\n",
        "\n",
        "model = Model(size_tuple, optimizer, nonlinear)\n",
        "\n",
        "print('Training...')\n",
        "model.train(X, A_train, val_edges, val_edges_false, cluster_dict, batch_size, epochs)\n",
        "print('Testing...')\n",
        "model.test(X, A_train, test_edges, test_edges_false)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training...\n",
            "0 [0.008090394, 6.362612] 6.3707023\n",
            "1 [0.010542677, 6.1958475] 6.2063904\n",
            "2 [0.017628027, 5.676636] 5.6942644\n",
            "3 [0.009597128, 5.7348504] 5.7444477\n",
            "4 [0.011113051, 5.5302577] 5.541371\n",
            "5 [0.019743498, 5.1415095] 5.161253\n",
            "6 [0.016184924, 5.0888844] 5.105069\n",
            "7 [0.020871136, 4.8912997] 4.912171\n",
            "8 [0.022928244, 4.7580094] 4.7809377\n",
            "9 [0.060446724, 4.4491463] 4.509593\n",
            "10 [0.059518743, 4.1890326] 4.2485514\n",
            "11 [0.036158126, 4.2500343] 4.2861924\n",
            "12 [0.045208987, 4.0896463] 4.1348553\n",
            "13 [0.054629136, 3.8185406] 3.8731697\n",
            "14 [0.07471843, 3.4037118] 3.4784303\n",
            "15 [0.06539471, 3.5889428] 3.6543374\n",
            "16 [0.17974919, 2.7527184] 2.9324677\n",
            "17 [0.11195927, 2.884241] 2.9962003\n",
            "18 [0.25881562, 2.18259] 2.4414055\n",
            "19 [0.1491782, 2.520105] 2.6692832\n",
            "20 [0.15856859, 2.421049] 2.5796177\n",
            "21 [0.34171987, 1.7235627] 2.0652826\n",
            "22 [0.20031853, 2.2010155] 2.401334\n",
            "23 [0.28215313, 1.8929169] 2.17507\n",
            "24 [0.22011301, 2.0662692] 2.2863822\n",
            "25 [0.25627804, 1.964869] 2.221147\n",
            "26 [0.36459044, 1.6267744] 1.9913648\n",
            "27 [0.5417957, 1.4164642] 1.9582598\n",
            "28 [0.5965339, 1.2358629] 1.8323967\n",
            "29 [0.4475817, 1.3511554] 1.798737\n",
            "30 [0.40575495, 1.3880857] 1.7938406\n",
            "31 [0.40170458, 1.4596364] 1.861341\n",
            "32 [0.4307657, 1.3300018] 1.7607675\n",
            "33 [0.783385, 0.9812537] 1.7646387\n",
            "34 [0.4483954, 1.3118466] 1.760242\n",
            "35 [0.4081242, 1.386617] 1.7947412\n",
            "36 [0.5836866, 1.0849408] 1.6686274\n",
            "37 [0.5096538, 1.1978076] 1.7074614\n",
            "38 [0.67695975, 1.0766721] 1.7536318\n",
            "39 [0.89577484, 0.8713985] 1.7671733\n",
            "40 [0.49099678, 1.1842352] 1.6752319\n",
            "41 [0.73151255, 0.95341355] 1.684926\n",
            "42 [1.038666, 0.8030325] 1.8416985\n",
            "43 [0.6649076, 0.9514444] 1.616352\n",
            "44 [0.77148616, 0.9096376] 1.6811237\n",
            "45 [0.59832853, 1.0663435] 1.6646721\n",
            "46 [0.5721193, 1.0973897] 1.6695089\n",
            "47 [0.64968675, 1.0098906] 1.6595774\n",
            "48 [0.55688846, 1.1012836] 1.658172\n",
            "49 [0.85654855, 0.8845237] 1.7410722\n",
            "50 [0.8222613, 0.94905424] 1.7713156\n",
            "51 [0.6000803, 1.0608463] 1.6609266\n",
            "52 [0.69904006, 0.93485075] 1.6338909\n",
            "53 [0.5870107, 1.0668598] 1.6538706\n",
            "54 [1.0686903, 0.7407287] 1.8094189\n",
            "55 [0.80628085, 0.9213993] 1.7276802\n",
            "56 [1.1515225, 0.7132412] 1.8647637\n",
            "57 [0.71404296, 0.9080333] 1.6220763\n",
            "58 [1.1383615, 0.7117154] 1.8500769\n",
            "59 [0.57339257, 1.0902122] 1.6636047\n",
            "60 [0.86609495, 0.8698127] 1.7359077\n",
            "61 [0.5912419, 1.0526323] 1.6438742\n",
            "62 [0.7485483, 0.90728045] 1.6558287\n",
            "63 [0.60478157, 1.0505699] 1.6553514\n",
            "64 [0.5516537, 1.0882459] 1.6398995\n",
            "65 [1.0818262, 0.7893377] 1.8711638\n",
            "66 [0.50582457, 1.1484413] 1.6542659\n",
            "67 [0.51355, 1.1879588] 1.7015088\n",
            "68 [0.9841836, 0.7613062] 1.7454898\n",
            "69 [0.7421377, 0.89441264] 1.6365503\n",
            "70 [0.68117315, 0.9405676] 1.6217408\n",
            "71 [0.7405249, 0.9188703] 1.6593952\n",
            "72 [1.0369403, 0.7564498] 1.7933902\n",
            "73 [0.502775, 1.1792842] 1.6820593\n",
            "74 [0.53621227, 1.1214247] 1.6576369\n",
            "75 [0.56909853, 1.0527704] 1.6218688\n",
            "76 [0.49597362, 1.1869984] 1.682972\n",
            "77 [0.70259994, 1.007722] 1.7103219\n",
            "78 [0.48815116, 1.1904548] 1.678606\n",
            "79 [0.5961828, 1.0794888] 1.6756716\n",
            "80 [0.6383689, 0.98067486] 1.6190438\n",
            "81 [0.5735241, 1.0394816] 1.6130058\n",
            "82 [0.56544226, 1.0883522] 1.6537945\n",
            "83 [0.5208314, 1.1306591] 1.6514904\n",
            "84 [0.64585984, 0.9716849] 1.6175447\n",
            "85 [0.4930881, 1.1747379] 1.667826\n",
            "86 [0.5282812, 1.135683] 1.6639642\n",
            "87 [0.53973573, 1.1016808] 1.6414165\n",
            "88 [0.76914394, 0.8797966] 1.6489406\n",
            "89 [0.5431215, 1.0947433] 1.6378648\n",
            "90 [0.799196, 0.858717] 1.657913\n",
            "91 [0.58564854, 1.0428914] 1.6285399\n",
            "92 [0.70234793, 0.95398664] 1.6563346\n",
            "93 [0.7757242, 0.9229753] 1.6986995\n",
            "94 [0.6753612, 0.9476706] 1.6230319\n",
            "95 [0.6705202, 0.93583196] 1.6063521\n",
            "96 [0.6913416, 0.9162782] 1.6076198\n",
            "97 [0.6228668, 1.0190313] 1.6418982\n",
            "98 [0.57246786, 1.056247] 1.6287148\n",
            "99 [0.6735262, 0.93404484] 1.6075711\n",
            "100 [0.87083596, 0.8819504] 1.7527864\n",
            "101 [0.8041554, 0.84764725] 1.6518027\n",
            "102 [0.70591575, 0.91292983] 1.6188456\n",
            "103 [0.6549915, 0.9927965] 1.647788\n",
            "104 [0.55034506, 1.0963444] 1.6466894\n",
            "105 [0.5898144, 1.0352523] 1.6250668\n",
            "106 [0.6815636, 0.9253538] 1.6069174\n",
            "107 [0.7028621, 0.92569286] 1.6285549\n",
            "108 [0.5261507, 1.1228667] 1.6490175\n",
            "109 [0.5263616, 1.1222438] 1.6486053\n",
            "110 [0.809351, 0.8720409] 1.681392\n",
            "111 [0.61478114, 1.0257477] 1.6405288\n",
            "112 [1.0230887, 0.766385] 1.7894738\n",
            "113 [0.8529805, 0.8766695] 1.72965\n",
            "114 [0.53979075, 1.1098301] 1.6496209\n",
            "115 [0.79527545, 0.85244447] 1.6477199\n",
            "116 [0.5737401, 1.0590891] 1.6328292\n",
            "117 [1.0609848, 0.7241976] 1.7851825\n",
            "118 [0.5976013, 1.007535] 1.6051363\n",
            "119 [0.6691859, 0.96339244] 1.6325784\n",
            "120 [0.5233236, 1.1115091] 1.6348326\n",
            "121 [0.5840612, 1.0277086] 1.6117699\n",
            "122 [0.6369712, 0.96423143] 1.6012026\n",
            "123 [0.69949335, 0.93999594] 1.6394893\n",
            "124 [0.75766873, 0.8819121] 1.6395808\n",
            "125 [0.5837201, 1.0570264] 1.6407465\n",
            "126 [1.0186021, 0.7256801] 1.7442822\n",
            "127 [0.7134449, 0.97646445] 1.6899093\n",
            "128 [0.5272185, 1.141695] 1.6689136\n",
            "129 [0.5011332, 1.1811901] 1.6823233\n",
            "130 [0.5012648, 1.1569004] 1.6581652\n",
            "131 [0.7549298, 0.87825984] 1.6331897\n",
            "132 [0.72338, 0.90485024] 1.6282303\n",
            "133 [0.48162597, 1.1851742] 1.6668003\n",
            "134 [0.4991666, 1.1723524] 1.671519\n",
            "135 [0.5022256, 1.1658295] 1.668055\n",
            "136 [0.51523197, 1.1331216] 1.6483536\n",
            "137 [0.5302693, 1.1171743] 1.6474435\n",
            "138 [0.5297407, 1.1253664] 1.6551071\n",
            "139 [0.5314566, 1.1229258] 1.6543823\n",
            "140 [0.5364634, 1.1173279] 1.6537913\n",
            "141 [0.6190591, 1.0084058] 1.6274649\n",
            "142 [0.79098624, 0.8488598] 1.6398461\n",
            "143 [0.5829998, 1.041807] 1.6248069\n",
            "144 [0.5663742, 1.0630517] 1.6294259\n",
            "145 [0.6974042, 0.94780165] 1.6452059\n",
            "146 [0.6592966, 0.96478254] 1.6240791\n",
            "147 [1.1302806, 0.7579038] 1.8881844\n",
            "148 [0.65234214, 0.95849895] 1.610841\n",
            "149 [0.60359997, 1.0219408] 1.6255407\n",
            "150 [0.8728643, 0.8461762] 1.7190405\n",
            "151 [1.0957493, 0.7098965] 1.8056457\n",
            "152 [0.7482191, 0.8835272] 1.6317463\n",
            "153 [0.66637266, 0.9602405] 1.6266131\n",
            "154 [0.55815774, 1.081838] 1.6399958\n",
            "155 [1.0596393, 0.75080913] 1.8104484\n",
            "156 [0.8084723, 0.8441898] 1.652662\n",
            "157 [0.71614575, 0.8874732] 1.603619\n",
            "158 [1.0853254, 0.6914583] 1.7767837\n",
            "159 [0.6740642, 0.9373429] 1.611407\n",
            "160 [0.52944726, 1.1293303] 1.6587775\n",
            "161 [0.51411605, 1.1347007] 1.6488167\n",
            "162 [0.5162115, 1.1527908] 1.6690023\n",
            "163 [0.9588678, 0.78679514] 1.7456629\n",
            "164 [0.4897306, 1.1857016] 1.6754322\n",
            "165 [0.7131493, 0.9717321] 1.6848814\n",
            "166 [0.9189637, 0.8038723] 1.722836\n",
            "167 [0.49130306, 1.1745236] 1.6658267\n",
            "168 [0.5519084, 1.0618023] 1.6137106\n",
            "169 [0.9941966, 0.721476] 1.7156726\n",
            "170 [0.5181731, 1.129658] 1.6478311\n",
            "171 [0.7733995, 0.9164541] 1.6898535\n",
            "172 [0.7062408, 0.9016484] 1.6078892\n",
            "173 [0.731024, 0.90222704] 1.6332511\n",
            "174 [0.4760699, 1.2094727] 1.6855426\n",
            "175 [0.9103259, 0.78695995] 1.6972859\n",
            "176 [0.48021019, 1.208889] 1.6890992\n",
            "177 [0.6132335, 1.0247453] 1.6379788\n",
            "178 [0.5716536, 1.0252284] 1.596882\n",
            "179 [0.61654305, 1.0057838] 1.6223269\n",
            "180 [0.7300087, 0.92410904] 1.6541178\n",
            "181 [0.65727365, 0.9548123] 1.6120859\n",
            "182 [0.91838473, 0.7963379] 1.7147226\n",
            "183 [0.5139743, 1.1514663] 1.6654406\n",
            "184 [0.72064775, 0.89143384] 1.6120815\n",
            "185 [0.50921243, 1.1308812] 1.6400936\n",
            "186 [0.475184, 1.2046827] 1.6798667\n",
            "187 [0.9020226, 0.80227226] 1.7042949\n",
            "188 [0.6270379, 0.96794486] 1.5949827\n",
            "189 [0.4879075, 1.1798766] 1.6677841\n",
            "190 [0.598937, 0.99333686] 1.5922738\n",
            "191 [0.53118926, 1.1200583] 1.6512475\n",
            "192 [0.57003516, 1.0482941] 1.6183293\n",
            "193 [0.6066056, 0.9843176] 1.5909232\n",
            "194 [0.6544056, 0.9503903] 1.6047959\n",
            "195 [0.6442975, 0.933326] 1.5776235\n",
            "196 [0.54803705, 1.096058] 1.6440951\n",
            "197 [0.5989805, 1.0316105] 1.6305909\n",
            "198 [0.840883, 0.8632047] 1.7040877\n",
            "199 [1.004775, 0.7372697] 1.7420447\n",
            "Testing...\n",
            "0.8249441194073603 0.9124720597036802\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}