!sudo apt-get install libmetis-dev
!pip install metis
import metis
import random
import tensorflow as tf
from scipy import sparse
import scipy.io as sio

# hyperparameters
hidden = 512
latent = 256
learning_rate = 0.01
epochs = 1000
nparts = 50
batch_size = 1
K = 1

filename = '/content/drive/My Drive/TDK/ppi.mat'

mat_dict = sio.loadmat(filename)
A = mat_dict['A']
X = mat_dict['X']
Y = mat_dict['Y']
train_mask = mat_dict['train_mask'][0].astype(bool)
val_mask = mat_dict['val_mask'][0].astype(bool)
test_mask = mat_dict['test_mask'][0].astype(bool)

A_train = A
X_train = X

def cluster_graph(A, nparts):
  edge_cuts, parts = metis.part_graph([neighbors for neighbors in A.tolil().rows], nparts=nparts)
  print('Number of edge cuts: %d.' % edge_cuts)
  slices = {}
  for index, part in enumerate(parts):
    if part not in slices:
      slices[part] = []
    slices[part].append(index)
  return slices

slices = cluster_graph(A_train, nparts)

def preprocess_support(A):
  N = A.shape[1]
  D = sparse.csr_matrix(A.sum(axis=1))
  norm = D.power(-0.5)
  L = sparse.eye(N, dtype='float32') - A.multiply(norm).T.multiply(norm)
  max_eigval = sparse.linalg.eigsh(L, k=1, return_eigenvectors=False)[0]
  L_ = 2.0 / max_eigval * L - sparse.eye(N, dtype='float32')
  supports = [sparse.csr_matrix(sparse.eye(N, dtype='float32'))]
  if (K > 0):
    supports.append(L_)
  if (K > 1):
    for k in range(2, K + 1):
      supports.append((((2.0 * k - 1.0) * L_.dot(supports[k-1]) - (k - 1.0) * supports[k-2]) / k).astype('float32'))
  return supports

def toTensorSparse(supports):
  return tf.stack([support.todense() for support in supports], axis=-1)

def toTensor(T):
  return tf.constant(T)

class bilinear_layer:

  def __init__(self, indim, outdim):
    initial_value = tf.initializers.he_normal()((indim, outdim,))
    #self.weight = tf.Variable(initial_value=initial_value, trainable=True)

  def __call__(self, tensor):
    return tf.linalg.matmul(tensor, tf.transpose(tensor))

class FC_layer:

  def __init__(self, indim, outdim):
    initial_value = tf.initializers.he_normal()((indim, outdim,))
    self.weight = tf.Variable(initial_value=initial_value, trainable=True)

  def __call__(self, tensor):
    return tf.linalg.matmul(tensor, self.weight)

class GC_layer:

  def __init__(self, indim, outdim):
    global K
    initial_value = tf.initializers.he_normal()((indim, outdim,))
    self.weight = tf.Variable(initial_value=initial_value, trainable=True)
    initial_value = tf.squeeze(tf.eye(K + 1, num_columns=1))
    self.coeffs = tf.Variable(initial_value, trainable=True)

  def __call__(self, tensor, supports):
    return tf.einsum('nmk, mi, io, k -> no', supports, tensor, self.weight, self.coeffs)

class Model:

  def __init__(self, size_tuple, optimizer):
    self.sources = []
    self.build(size_tuple)
    self.optimizer = optimizer
    self.μ = None
    self.logσ = None

  def build(self, size_tuple):
    X_dim, hidden, latent = size_tuple
    self.A_enc_layer = GC_layer(X_dim, hidden)
    self.A_enc_layer0 = GC_layer(hidden, hidden)
    self.A_μ_layer = GC_layer(hidden, latent)
    self.A_logσ_layer = GC_layer(hidden, latent)
    self.A_result_layer = bilinear_layer(latent, latent)
    self.X_enc_layer = GC_layer(X_dim, hidden)
    self.X_μ_layer = GC_layer(hidden, latent)
    self.X_logσ_layer = GC_layer(hidden, latent)
    self.X_result_layer = FC_layer(latent, X_dim)
    layers = [self.A_enc_layer, self.A_enc_layer0, self.X_enc_layer, self.A_μ_layer, self.X_μ_layer, self.A_logσ_layer, self.X_logσ_layer]
    for layer in layers:
      self.sources += [layer.weight, layer.coeffs]
    self.sources += [self.X_result_layer.weight]
  
  def encode(self, X, S):
    A_enc = tf.nn.relu(self.A_enc_layer(X, S))
    A_enc0 = tf.nn.relu(self.A_enc_layer0(A_enc, S))
    A_μ = self.A_μ_layer(A_enc0, S)
    A_logσ = self.A_logσ_layer(A_enc0, S)
    X_enc = tf.nn.relu(self.X_enc_layer(X, S))
    X_μ = self.X_μ_layer(X_enc, S)
    X_logσ = self.X_logσ_layer(X_enc, S)
    return (A_μ + X_μ) * 0.5, tf.math.log((tf.math.exp(A_logσ)**2.0 * 0.25 + tf.math.exp(X_logσ)**2.0 * 0.25)**0.5)

  def decode(self, sample):
    A_result = self.A_result_layer(sample)
    X_result = self.X_result_layer(sample)
    return A_result, X_result

  def predict(self, X, S):
    self.μ, self.logσ = self.encode(X, S)
    sample = self.μ + tf.math.exp(self.logσ) * tf.random.normal(self.logσ.shape)
    re_A, re_X = self.decode(sample)
    return re_A, re_X

  def train(self, X, A, slices, batch_size, epochs):
    for epoch in range(epochs):
      samples = random.sample(slices.keys(), batch_size)
      nodes = sum([slices[sample] for sample in samples], [])
      # import numpy as np
      # a = np.arange(A.shape[1])
      # p = np.squeeze(np.asarray(A.sum(axis=1))) / np.sum(np.squeeze(np.asarray(A.sum(axis=1))))
      # nodes = np.random.choice(a, size=2500, replace=False, p=p).tolist()
      S_batch = toTensorSparse(preprocess_support(A[nodes].T[nodes]))
      A_batch = toTensor(A.T[nodes].T[nodes].todense())
      X_batch = tf.math.l2_normalize(toTensor(X[nodes]), axis=1)
      with tf.GradientTape() as tape:
        re_A, re_X = self.predict(X_batch, S_batch)
        losses = self.loss(A_batch, re_A, X_batch, re_X)
        loss_ = tf.reduce_sum(losses)
        print(epoch, [loss.numpy() for loss in losses], loss_.numpy())
      if tf.math.is_inf(loss_).numpy():
        continue
      grads = tape.gradient(loss_, self.sources)
      self.optimizer.apply_gradients(zip(grads, self.sources))

  def re_A_loss(self, A, re_A):
    density = tf.reduce_sum(A) / tf.size(A, out_type=tf.float32)
    pos_weight = (1.0 - density) / density
    H_A = tf.nn.weighted_cross_entropy_with_logits(labels=A, logits=re_A, pos_weight=pos_weight) / (1.0 - density) / 2.0
    loss = tf.reduce_mean(H_A)
    return loss
  
  def re_X_loss(self, X, re_X):
    H_X = tf.losses.mse(X, re_X)
    loss = tf.reduce_mean(H_X)
    return loss

  def relative_entropy(self):
    H_S = (self.μ**2.0 + tf.math.exp(self.logσ)**2.0 - 2.0 * self.logσ - 1.0) / 2.0
    loss = tf.reduce_mean(H_S)
    return loss / 1000.0

  def weight_regularizer(self):
    decoder_weights = tf.concat([tf.reshape(source, -1) for source in self.sources[-2:]], axis=0)
    loss = tf.reduce_sum(decoder_weights**2.0)
    return loss / 1000.0

  def loss(self, A, re_A, X, re_X):
    return self.re_A_loss(A, re_A), self.re_X_loss(X, re_X), self.relative_entropy()#, self.weight_regularizer()

size_tuple = (X.shape[1], hidden, latent)
optimizer = tf.optimizers.Adam(learning_rate=learning_rate)

model = Model(size_tuple, optimizer)

model.train(X_train, A_train, slices, batch_size, epochs)

import numpy as np

def GC_function(tensor, supports, layer):
  transform = np.dot(tensor, layer.weight.numpy())
  result = np.zeros(transform.shape)
  for support, coeff in zip(supports, layer.coeffs.numpy()):
    result += support.dot(transform * coeff)
  return result

S = preprocess_support(A)
X = X / np.linalg.norm(X, axis=1)[:, None]

A_enc = np.maximum(GC_function(X, S, model.A_enc_layer), 0.0)
A_enc0 = np.maximum(GC_function(A_enc, S, model.A_enc_layer0), 0.0)
A_μ = GC_function(A_enc0, S, model.A_μ_layer)

X_enc = np.maximum(GC_function(X, S, model.X_enc_layer), 0.0)
X_μ = GC_function(X_enc, S, model.X_μ_layer)

X = (A_μ + X_μ) / 2.0

from sklearn.ensemble import RandomForestClassifier
model = RandomForestClassifier(verbose=2, n_jobs=-1, n_estimators=100)
model.fit(X[train_mask], Y[train_mask])
from sklearn.metrics import accuracy_score, f1_score, average_precision_score
accuracy = accuracy_score(Y[test_mask], model.predict(X[test_mask]))
microf1 = f1_score(Y[test_mask], model.predict(X[test_mask]), average='micro')
average_precision = average_precision_score(Y[test_mask], model.predict(X[test_mask]), average='micro')
print(accuracy, microf1, average_precision)

# cluster: 0.8597031136857349 0.9658465340972925 0.9497043881429764

# fastgae:
