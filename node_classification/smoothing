
!sudo apt-get install libmetis-dev
!pip install metis
import metis
import random
import tensorflow as tf
from scipy import sparse
import scipy.io as sio

# hyperparameters
hidden = 512
latent = 256
learning_rate = 0.01
epochs = 1000
nparts = 50
batch_size = 1

filename = '/content/drive/My Drive/TDK/ppi.mat'

mat_dict = sio.loadmat(filename)
A = mat_dict['A']
X = mat_dict['X']
Y = mat_dict['Y']
train_mask = mat_dict['train_mask'][0].astype(bool)
val_mask = mat_dict['val_mask'][0].astype(bool)
test_mask = mat_dict['test_mask'][0].astype(bool)

A_train = A
X_train = X

def cluster_graph(A, nparts):
  edge_cuts, parts = metis.part_graph([neighbors for neighbors in A.tolil().rows], nparts=nparts)
  print('Number of edge cuts: %d.' % edge_cuts)
  slices = {}
  for index, part in enumerate(parts):
    if part not in slices:
      slices[part] = []
    slices[part].append(index)
  return slices

slices = cluster_graph(A_train, nparts)

def preprocess_support(A):
  N = A.shape[1]
  A_ = A + sparse.eye(N, dtype='float32')
  D = sparse.csr_matrix(A_.sum(axis=1))
  norm = D.power(-0.5)
  return A_.multiply(norm).T.multiply(norm)

def toTensorSparse(S):
  return tf.constant(S.todense())

def toTensor(T):
  return tf.constant(T)

class bilinear_layer:

  def __init__(self, indim, outdim):
    initial_value = tf.initializers.he_normal()((indim, outdim,))
    self.weight = tf.Variable(initial_value=initial_value, trainable=True)

  def __call__(self, tensor):
    #return tf.linalg.matmul(tensor, tf.linalg.matmul(self.weight, tf.transpose(tensor)))
    return tf.linalg.matmul(tensor, tf.transpose(tensor))

class FC_layer:

  def __init__(self, indim, outdim):
    initial_value = tf.initializers.he_normal()((indim, outdim,))
    self.weight = tf.Variable(initial_value=initial_value, trainable=True)

  def __call__(self, tensor):
    return tf.linalg.matmul(tensor, self.weight)

class GC_layer:

  def __init__(self, indim, outdim):
    initial_value = tf.initializers.he_normal()((indim, outdim,))
    self.weight = tf.Variable(initial_value=initial_value, trainable=True)

  def __call__(self, tensor, support):
    return tf.linalg.matmul(support, tf.linalg.matmul(tensor, self.weight))


class Model:

  def __init__(self, size_tuple, optimizer):
    self.sources = []
    self.build(size_tuple)
    self.optimizer = optimizer
    self.μ = None
    self.logσ = None

  def build(self, size_tuple):
    X_dim, hidden, latent = size_tuple
    self.A_enc_layer = GC_layer(X_dim, hidden)
    self.A_μ_layer = GC_layer(hidden, latent)
    self.A_logσ_layer = GC_layer(hidden, latent)
    self.A_result_layer = bilinear_layer(latent, latent)
    self.X_enc_layer = GC_layer(X_dim, hidden)
    self.X_μ_layer = GC_layer(hidden, latent)
    self.X_logσ_layer = GC_layer(hidden, latent)
    self.X_result_layer = FC_layer(latent, X_dim)
    layers = [self.A_enc_layer, self.X_enc_layer, self.A_μ_layer, self.X_μ_layer, self.A_logσ_layer, self.X_logσ_layer] \
              + [self.X_result_layer]# + [self.A_result_layer]
    for layer in layers:
      self.sources.append(layer.weight)
  
  def encode(self, X, S):
    A_enc = tf.nn.tanh(self.A_enc_layer(X, S))
    A_μ = self.A_μ_layer(A_enc, S)
    A_logσ = self.A_logσ_layer(A_enc, S)
    X_enc = tf.nn.tanh(self.X_enc_layer(X, S))
    X_μ = self.X_μ_layer(X_enc, S)
    X_logσ = self.X_logσ_layer(X_enc, S)
    return (A_μ + X_μ) * 0.5, (A_logσ + X_logσ) * 0.5

  def decode(self, sample):
    A_result = self.A_result_layer(sample)
    X_result = self.X_result_layer(sample)
    return A_result, X_result

  def predict(self, X, S):
    self.μ, self.logσ = self.encode(X, S)
    sample = self.μ + tf.math.exp(self.logσ) * tf.random.normal(self.logσ.shape)
    re_A, re_X = self.decode(sample)
    return re_A, re_X

  def train(self, X, A, slices, batch_size, epochs):
    for epoch in range(epochs):
      samples = random.sample(slices.keys(), batch_size)
      nodes = sum([slices[sample] for sample in samples], [])
      # import numpy as np
      # a = np.arange(A.shape[1])
      # p = np.squeeze(np.asarray(A.sum(axis=1))) / np.sum(np.squeeze(np.asarray(A.sum(axis=1))))
      # nodes = np.random.choice(a, size=5000, replace=False, p=p).tolist()
      S_batch = toTensorSparse(preprocess_support(A[nodes].T[nodes]))
      A_batch = toTensor(A.T[nodes].T[nodes].todense())
      X_batch = tf.math.l2_normalize(toTensor(X[nodes]), axis=1)
      with tf.GradientTape() as tape:
        re_A, re_X = self.predict(X_batch, S_batch)
        losses = self.loss(A_batch, re_A, X_batch, re_X)
        loss_ = tf.reduce_sum(losses)
        print(epoch, [loss.numpy() for loss in losses], loss_.numpy())
      grads = tape.gradient(loss_, self.sources)
      self.optimizer.apply_gradients(zip(grads, self.sources))

  def re_A_loss(self, A, re_A):
    density = tf.reduce_sum(A) / tf.size(A, out_type=tf.float32)
    pos_weight = (1.0 - density) / density
    H_A = tf.nn.weighted_cross_entropy_with_logits(labels=A, logits=re_A, pos_weight=pos_weight) / (1.0 - density) / 2.0
    loss = tf.reduce_mean(H_A)
    return loss
  
  def re_X_loss(self, X, re_X):
    H_X = tf.losses.mse(X, re_X)
    loss = tf.reduce_mean(H_X)
    return loss

  def relative_entropy(self):
    H_S = (self.μ**2.0 + tf.math.exp(self.logσ)**2.0 - 2.0 * self.logσ - 1.0) / 2.0
    loss = tf.reduce_mean(H_S)
    return loss / 1000.0

  def weight_regularizer(self):
    decoder_weights = tf.concat([tf.reshape(source, -1) for source in self.sources[-2:]], axis=0)
    loss = tf.reduce_sum(decoder_weights**2.0)
    return loss / 1000.0

  def loss(self, A, re_A, X, re_X):
    return self.re_A_loss(A, re_A), self.re_X_loss(X, re_X), self.relative_entropy()#, self.weight_regularizer()

size_tuple = (X.shape[1], hidden, latent)
optimizer = tf.optimizers.Adam(learning_rate=learning_rate)

model = Model(size_tuple, optimizer)

model.train(X_train, A_train, slices, batch_size, epochs)


import numpy as np

S = preprocess_support(A)
X = X / np.linalg.norm(X, axis=1)[:, None]

A_enc = np.tanh(S.dot(np.dot(X, model.A_enc_layer.weight.numpy())))
A_μ = S.dot(np.dot(A_enc, model.A_μ_layer.weight.numpy()))

X_enc = np.tanh(S.dot(np.dot(X, model.X_enc_layer.weight.numpy())))
X_μ = S.dot(np.dot(X_enc, model.X_μ_layer.weight.numpy()))

X = (A_μ + X_μ) / 2.0

from sklearn.ensemble import RandomForestClassifier
model = RandomForestClassifier(verbose=2, n_jobs=-1, n_estimators=100)
model.fit(X[train_mask], Y[train_mask])
from sklearn.metrics import accuracy_score, f1_score, average_precision_score
accuracy = accuracy_score(Y[test_mask], model.predict(X[test_mask]))
microf1 = f1_score(Y[test_mask], model.predict(X[test_mask]), average='micro')
average_precision = average_precision_score(Y[test_mask], model.predict(X[test_mask]), average='micro')
print(accuracy, microf1, average_precision)
