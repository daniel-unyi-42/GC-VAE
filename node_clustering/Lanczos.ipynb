{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Lanczos.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "zzzB2uJwBC-m",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "af47792d-cdb7-4b8d-88e1-4c7e152cb2f5"
      },
      "source": [
        "!sudo apt-get install libmetis-dev\n",
        "!pip install metis\n",
        "import metis\n",
        "import random\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from scipy import sparse\n",
        "import scipy.io as sio"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "libmetis-dev is already the newest version (5.1.0.dfsg-5).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 30 not upgraded.\n",
            "Requirement already satisfied: metis in /usr/local/lib/python3.7/dist-packages (0.2a5)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p-5rUhczBJdV"
      },
      "source": [
        "# hyperparameters\n",
        "hidden = 512 # number of hidden units in the encoder layer\n",
        "latent = 256 # dimension of the latent variables\n",
        "learning_rate = 0.01\n",
        "epochs = 200\n",
        "nparts = 1500 # number of partitions\n",
        "batch_size = 20 # number of clusters per batch\n",
        "K = 3 # number of Lanczos iterations"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RflJi-R3BOkG"
      },
      "source": [
        "filename = '/content/drive/MyDrive/GRAPH DATA/reddit.mat' # dataset"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ABjlQkpsDFgx"
      },
      "source": [
        "mat_dict = sio.loadmat(filename)\n",
        "A = mat_dict['A'].ceil()\n",
        "X = mat_dict['X']\n",
        "Y = mat_dict['Y']\n",
        "train_mask = mat_dict['train_mask'].squeeze().astype(bool)\n",
        "val_mask = mat_dict['val_mask'].squeeze().astype(bool)\n",
        "test_mask = mat_dict['test_mask'].squeeze().astype(bool)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a2GhIMjvDNzG",
        "outputId": "7dbbb48b-fdf7-4635-a233-8ba34725fdc6"
      },
      "source": [
        "def cluster_graph(A, nparts):\n",
        "  if nparts == 1:\n",
        "    edge_cuts, parts = 0, [0, ] * A.shape[0]\n",
        "  else:\n",
        "    edge_cuts, parts = metis.part_graph([neighbors for neighbors in A.tolil().rows], nparts=nparts)\n",
        "  print('Number of edge cuts: %d.' % edge_cuts)\n",
        "  cluster_dict = {}\n",
        "  for index, part in enumerate(parts):\n",
        "    if part not in cluster_dict:\n",
        "      cluster_dict[part] = []\n",
        "    cluster_dict[part].append(index)\n",
        "  return cluster_dict\n",
        "\n",
        "# the clustering algorithm (METIS)\n",
        "cluster_dict = cluster_graph(A, nparts)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of edge cuts: 9609639.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FstzOA7oDgoL"
      },
      "source": [
        "def preprocess_support(A):\n",
        "  N = A.shape[1]\n",
        "  D = sparse.csr_matrix(A.sum(axis=1))\n",
        "  norm = D.power(-0.5)\n",
        "  L = sparse.eye(N, dtype='float32') - A.multiply(norm).T.multiply(norm)\n",
        "  max_eigval = sparse.linalg.eigsh(L, k=1, return_eigenvectors=False)[0]\n",
        "  L_ = 2.0 / max_eigval * L - sparse.eye(N, dtype='float32')\n",
        "  return L_\n",
        "\n",
        "def toTensorSparse(S):\n",
        "  return tf.constant(S.todense())\n",
        "\n",
        "def toTensor(T):\n",
        "  return tf.constant(T)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OVOs3s5TDrHJ"
      },
      "source": [
        "# layer classes\n",
        "\n",
        "class bilinear_layer:\n",
        "\n",
        "  def __init__(self, indim, outdim):\n",
        "    pass\n",
        "\n",
        "  def __call__(self, tensor):\n",
        "    return tf.linalg.matmul(tensor, tf.transpose(tensor))\n",
        "\n",
        "# unused\n",
        "class FC_layer:\n",
        "\n",
        "  def __init__(self, indim, outdim):\n",
        "    initial_value = tf.initializers.he_normal()((indim, outdim,))\n",
        "    self.weight = tf.Variable(initial_value=initial_value, trainable=True)\n",
        "\n",
        "  def __call__(self, tensor):\n",
        "    return tf.linalg.matmul(tensor, self.weight)\n",
        "\n",
        "class GC_layer:\n",
        "\n",
        "  def __init__(self, indim, outdim):\n",
        "    global K\n",
        "    self.K = K + 1\n",
        "    delta = np.zeros((outdim, K + 1, K + 1), dtype='float32')\n",
        "    for o in range(outdim):\n",
        "      delta[o, 0, 0] = 1.0\n",
        "    self.filter = tf.Variable(initial_value=delta, trainable=True)\n",
        "    initial_value = tf.initializers.he_normal()((indim, outdim,))\n",
        "    self.weight = tf.Variable(initial_value=initial_value, trainable=True)\n",
        "\n",
        "  # Lanczos algorithm implemented for multiple vectors\n",
        "  def Lanczos_algorithm(self, tensor, support, K, embed=False):\n",
        "    Q = [tf.zeros(tensor.shape), tensor / tf.linalg.norm(tensor, axis=0)]\n",
        "    C = [tf.zeros(tensor.shape[1])]\n",
        "    B = [tf.zeros(tensor.shape[1])]\n",
        "    for k in range(1, K + 1):\n",
        "      if embed: # numpy pipeline\n",
        "        z = tf.constant(support.dot(Q[k].numpy()))\n",
        "      else: # tensorflow pipeline\n",
        "        z = tf.linalg.matmul(support, Q[k])\n",
        "      C.append(tf.einsum('ab, ab -> b', Q[k], z))\n",
        "      z = z - Q[k] * C[k] - Q[k-1] * B[k-1]\n",
        "      B.append(tf.linalg.norm(z, axis=0))\n",
        "      Q.append(z / B[k])\n",
        "    Vs = tf.transpose(tf.stack(Q[1:-1])) # Lanczos vectors\n",
        "    Cs = tf.transpose(tf.stack(C[1:])) # diagonal Lanczos scalars\n",
        "    Bs = tf.transpose(tf.stack(B[1:-1])) # off-diagonal Lanczos scalars\n",
        "    Hs = tf.linalg.diag(Bs, k=-1) + tf.linalg.diag(Cs) + tf.linalg.diag(Bs, k=1) # tridiagonal matrix\n",
        "    return Vs, Hs\n",
        "\n",
        "  def __call__(self, tensor, support, embed=False):\n",
        "    # see the following reference:\n",
        "    # \"Susnjara, A., Perraudin, N., Kressner, D., & Vandergheynst, P. (2015).\n",
        "    # Accelerated filtering on graphs using lanczos method. arXiv preprint arXiv:1509.04537.\"\n",
        "    tensor = tf.linalg.matmul(tensor, self.weight)\n",
        "    V, H = self.Lanczos_algorithm(tensor, support, self.K, embed)\n",
        "    delta = tf.one_hot(tf.zeros(tensor.shape[1], dtype=tf.uint8), self.K)\n",
        "    norm = tf.linalg.norm(tensor, axis=0)\n",
        "    eigvals, eigvecs = tf.linalg.eigh(H)\n",
        "    T = tf.einsum('abc, acd, ade -> abe', eigvecs, self.filter, eigvecs)\n",
        "    result = tf.einsum('abc, acd, ad, a -> ba', V, T, delta, norm)\n",
        "    return result\n"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qwEu7WB9EWAI"
      },
      "source": [
        "# our model class (for the paper \"Scalable Graph Variational Autoencoders\")\n",
        "\n",
        "class Model:\n",
        "\n",
        "  def __init__(self, size_tuple, optimizer, nonlinear):\n",
        "    self.sources = [] # variables to optimize\n",
        "    self.build(size_tuple) # builds the model by stacking layers on each other\n",
        "    self.optimizer = optimizer\n",
        "    self.nonlinear = nonlinear\n",
        "    self.Z_mean = None # mean embedding layer\n",
        "    self.Z_var = None # variance embedding layer\n",
        "    self.noise = None # the noise sample\n",
        "    self.sample = None # self.Z_mean + self.Z_var * self.noise\n",
        "    self.A_gamma = None # the reconstructions\n",
        "  \n",
        "  def build(self, size_tuple):\n",
        "    X_dim, hidden, latent = size_tuple\n",
        "    self.enc_layer = GC_layer(X_dim, hidden)\n",
        "    self.enc_mean_layer = GC_layer(hidden, latent)\n",
        "    self.enc_var_layer = GC_layer(hidden, latent)\n",
        "    self.A_dec_gamma_layer = bilinear_layer(latent, latent)\n",
        "    # filling the source array with weights\n",
        "    layers = [self.enc_layer, self.enc_mean_layer, self.enc_var_layer]\n",
        "    for layer in layers:\n",
        "      self.sources.append(layer.weight)\n",
        "      self.sources.append(layer.filter)\n",
        "  \n",
        "  # forward propagation in the encoder\n",
        "  def encode(self, X, S):\n",
        "    enc = self.nonlinear(self.enc_layer(X, S))\n",
        "    enc_mean = self.enc_mean_layer(enc, S)\n",
        "    enc_var = tf.math.exp(self.enc_var_layer(enc, S))\n",
        "    return enc_mean, enc_var\n",
        "\n",
        "  # returns only the node embeddings\n",
        "  def embed(self, X, S):\n",
        "    enc = self.nonlinear(self.enc_layer(X, S, embed=True))\n",
        "    enc_mean = self.enc_mean_layer(enc, S, embed=True)\n",
        "    return enc_mean\n",
        "\n",
        "  # forward propagation in the decoder\n",
        "  def decode(self, sample):\n",
        "    A_dec_gamma = self.A_dec_gamma_layer(sample)\n",
        "    return A_dec_gamma\n",
        "\n",
        "  def predict(self, X, S):\n",
        "    self.Z_mean, self.Z_var = self.encode(X, S)\n",
        "    self.noise = tf.random.normal(self.Z_var.shape)\n",
        "    self.sample = self.Z_mean + self.Z_var * self.noise # reparameterization trick\n",
        "    self.A_gamma = self.decode(self.sample)\n",
        "\n",
        "  def train(self, X, A, cluster_dict, batch_size, epochs):\n",
        "    for epoch in range(epochs):\n",
        "      # only a subgraph is used in the training process\n",
        "      samples = random.sample(cluster_dict.keys(), batch_size)\n",
        "      nodes = sum([cluster_dict[sample] for sample in samples], [])\n",
        "      S_batch = toTensorSparse(preprocess_support(A[nodes].T[nodes]))\n",
        "      A_batch = toTensor(A.T[nodes].T[nodes].todense())\n",
        "      X_batch = tf.math.l2_normalize(toTensor(X[nodes]), axis=1)\n",
        "      # optimization\n",
        "      with tf.GradientTape() as tape:\n",
        "        self.predict(X_batch, S_batch)\n",
        "        losses = self.loss(A_batch, X_batch)\n",
        "        loss_ = tf.reduce_sum(losses)\n",
        "      print(epoch, [loss.numpy() for loss in losses], loss_.numpy())\n",
        "      grads = tape.gradient(loss_, self.sources)\n",
        "      self.optimizer.apply_gradients(zip(grads, self.sources))\n",
        "\n",
        "  # Kullback–Leibler divergence\n",
        "  def KL_Divergence(self):\n",
        "    loss = 0.5 * tf.reduce_mean(self.Z_mean**2.0 + self.Z_var**2.0 - 2.0 * tf.math.log(self.Z_var) - 1.0)\n",
        "    return loss\n",
        "\n",
        "  # reconstruction loss\n",
        "  def re_A_loss(self, A):\n",
        "    density = tf.reduce_sum(A) / tf.size(A, out_type=tf.float32)\n",
        "    pos_weight = (1.0 - density) / density\n",
        "    loss = -0.5 * tf.reduce_mean(1.0 / (1.0 - density) * tf.nn.weighted_cross_entropy_with_logits(labels=A, logits=self.A_gamma, pos_weight=pos_weight))\n",
        "    return -loss\n",
        "\n",
        "  # list of all loss functions\n",
        "  def loss(self, A, X):\n",
        "    return self.KL_Divergence(), self.re_A_loss(A)\n"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0zuy3DUXLK4J",
        "outputId": "805ba4fd-67ab-47fe-bd3f-ccefb17f3737"
      },
      "source": [
        "size_tuple = (X.shape[1], hidden, latent)\n",
        "optimizer = tf.optimizers.Adam(learning_rate=learning_rate)\n",
        "nonlinear = tf.nn.relu\n",
        "\n",
        "model = Model(size_tuple, optimizer, nonlinear)\n",
        "\n",
        "print('Training...')\n",
        "model.train(X, A, cluster_dict, batch_size, epochs)\n"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training...\n",
            "0 [0.003802459, 6.483147] 6.4869494\n",
            "1 [0.025074247, 5.100501] 5.1255755\n",
            "2 [0.07272147, 3.6049988] 3.6777203\n",
            "3 [0.20927277, 2.2150311] 2.424304\n",
            "4 [0.45730138, 1.592591] 2.0498924\n",
            "5 [0.7325524, 1.001307] 1.7338594\n",
            "6 [1.0259452, 1.1978085] 2.2237537\n",
            "7 [1.1405663, 0.82289326] 1.9634596\n",
            "8 [1.2961732, 1.0005741] 2.2967472\n",
            "9 [1.1595438, 0.74875605] 1.9082998\n",
            "10 [1.1969844, 0.9330629] 2.1300473\n",
            "11 [0.9596665, 0.7973858] 1.7570523\n",
            "12 [0.8047619, 0.8703625] 1.6751244\n",
            "13 [0.6495329, 0.9633046] 1.6128376\n",
            "14 [0.54674673, 1.1267725] 1.6735193\n",
            "15 [0.46032926, 1.2836887] 1.744018\n",
            "16 [0.4250397, 1.3477314] 1.7727711\n",
            "17 [0.4143241, 1.3658344] 1.7801585\n",
            "18 [0.4214787, 1.3453584] 1.7668371\n",
            "19 [0.46001872, 1.2531964] 1.7132151\n",
            "20 [0.5244157, 1.1124256] 1.6368413\n",
            "21 [0.65035945, 0.9871469] 1.6375064\n",
            "22 [0.7130035, 0.8845272] 1.5975307\n",
            "23 [0.7841942, 0.8319147] 1.6161089\n",
            "24 [0.85805744, 0.75830257] 1.61636\n",
            "25 [0.8110677, 0.8012322] 1.6122999\n",
            "26 [0.7944675, 0.86402565] 1.6584932\n",
            "27 [0.80353653, 0.8119866] 1.6155231\n",
            "28 [0.767954, 0.80661964] 1.5745736\n",
            "29 [0.6999601, 0.8871013] 1.5870614\n",
            "30 [0.66909486, 0.9567688] 1.6258637\n",
            "31 [0.5920094, 1.0213568] 1.6133662\n",
            "32 [0.5772626, 1.0632894] 1.640552\n",
            "33 [0.5514758, 1.0593796] 1.6108553\n",
            "34 [0.55839443, 1.050982] 1.6093764\n",
            "35 [0.5560459, 1.0473151] 1.603361\n",
            "36 [0.58319217, 1.0075191] 1.5907114\n",
            "37 [0.63165134, 0.9523495] 1.5840008\n",
            "38 [0.67761546, 0.8697601] 1.5473756\n",
            "39 [0.66444284, 0.8626304] 1.5270733\n",
            "40 [0.7090503, 0.8254479] 1.5344982\n",
            "41 [0.723181, 0.8800558] 1.6032368\n",
            "42 [0.7068916, 0.8430781] 1.5499697\n",
            "43 [0.683095, 0.87915844] 1.5622535\n",
            "44 [0.6677878, 0.8854424] 1.5532302\n",
            "45 [0.66512686, 0.89739394] 1.5625207\n",
            "46 [0.63451904, 0.90330863] 1.5378277\n",
            "47 [0.6024735, 0.9160597] 1.5185332\n",
            "48 [0.5923754, 0.9132399] 1.5056152\n",
            "49 [0.59485954, 0.92813975] 1.5229993\n",
            "50 [0.59528327, 0.9179743] 1.5132575\n",
            "51 [0.587235, 0.9190107] 1.5062456\n",
            "52 [0.6229051, 0.938785] 1.5616901\n",
            "53 [0.6492807, 0.8499585] 1.4992392\n",
            "54 [0.64685726, 0.89611953] 1.5429769\n",
            "55 [0.65452904, 0.9190677] 1.5735967\n",
            "56 [0.6309335, 0.86101216] 1.4919457\n",
            "57 [0.61642015, 0.92971575] 1.5461359\n",
            "58 [0.6068877, 0.9206182] 1.5275059\n",
            "59 [0.58663714, 0.9811424] 1.5677795\n",
            "60 [0.5713807, 0.99960613] 1.5709867\n",
            "61 [0.58408815, 0.9854914] 1.5695796\n",
            "62 [0.60856676, 0.92619175] 1.5347586\n",
            "63 [0.6485059, 0.87881756] 1.5273235\n",
            "64 [0.68502545, 0.87891585] 1.5639412\n",
            "65 [0.70628524, 0.9183239] 1.6246091\n",
            "66 [0.6644203, 0.88356096] 1.5479813\n",
            "67 [0.62056524, 0.94372964] 1.5642948\n",
            "68 [0.6283373, 0.9347363] 1.5630736\n",
            "69 [0.6204858, 0.96949965] 1.5899854\n",
            "70 [0.643524, 0.8969034] 1.5404274\n",
            "71 [0.6312432, 0.9194835] 1.5507267\n",
            "72 [0.63017863, 0.9163419] 1.5465205\n",
            "73 [0.6173585, 0.89794254] 1.515301\n",
            "74 [0.6155354, 0.8588364] 1.4743718\n",
            "75 [0.5914026, 0.9934289] 1.5848315\n",
            "76 [0.6334102, 0.8782324] 1.5116427\n",
            "77 [0.612142, 0.91793865] 1.5300807\n",
            "78 [0.63420516, 0.85684395] 1.491049\n",
            "79 [0.66556823, 0.8696126] 1.5351808\n",
            "80 [0.6744859, 0.8311572] 1.5056431\n",
            "81 [0.6766381, 0.8821293] 1.5587674\n",
            "82 [0.6346305, 0.8631081] 1.4977386\n",
            "83 [0.623623, 0.8819545] 1.5055776\n",
            "84 [0.62142444, 0.9128586] 1.534283\n",
            "85 [0.60270095, 0.8612739] 1.4639748\n",
            "86 [0.58373123, 0.9354594] 1.5191905\n",
            "87 [0.606637, 0.9254357] 1.5320728\n",
            "88 [0.60538155, 0.8775346] 1.4829161\n",
            "89 [0.5969541, 0.8848692] 1.4818233\n",
            "90 [0.6119163, 0.9071245] 1.5190408\n",
            "91 [0.62194276, 0.9087773] 1.53072\n",
            "92 [0.65077114, 0.8098827] 1.4606538\n",
            "93 [0.6539506, 0.8945766] 1.5485272\n",
            "94 [0.620938, 0.93184733] 1.5527854\n",
            "95 [0.65048283, 0.9338938] 1.5843766\n",
            "96 [0.61794287, 0.8515532] 1.469496\n",
            "97 [0.60832816, 0.8585725] 1.4669006\n",
            "98 [0.6192226, 0.8785919] 1.4978144\n",
            "99 [0.61808234, 0.86588633] 1.4839687\n",
            "100 [0.6542221, 0.88077956] 1.5350016\n",
            "101 [0.6545534, 0.8345415] 1.489095\n",
            "102 [0.68178904, 0.79346186] 1.475251\n",
            "103 [0.6297699, 0.86259955] 1.4923694\n",
            "104 [0.6269497, 0.84967613] 1.4766259\n",
            "105 [0.61535764, 0.93275815] 1.5481157\n",
            "106 [0.60978246, 0.9312042] 1.5409867\n",
            "107 [0.59787947, 0.8895993] 1.4874787\n",
            "108 [0.619549, 0.87443835] 1.4939873\n",
            "109 [0.6184107, 0.84498745] 1.4633982\n",
            "110 [0.63241374, 0.88585913] 1.5182729\n",
            "111 [0.62686116, 0.89095926] 1.5178204\n",
            "112 [0.6452154, 0.8064926] 1.4517081\n",
            "113 [0.6418158, 0.8628044] 1.5046202\n",
            "114 [0.6539657, 0.8551864] 1.5091522\n",
            "115 [0.63691187, 0.84591424] 1.4828261\n",
            "116 [0.62716115, 0.8822619] 1.509423\n",
            "117 [0.56877214, 0.97472095] 1.543493\n",
            "118 [0.5901943, 0.93565106] 1.5258453\n",
            "119 [0.57460934, 0.9221582] 1.4967675\n",
            "120 [0.5902002, 0.87712365] 1.4673238\n",
            "121 [0.64507926, 0.8412441] 1.4863234\n",
            "122 [0.64593536, 0.83209616] 1.4780315\n",
            "123 [0.6537442, 0.84714484] 1.5008891\n",
            "124 [0.64905876, 0.8349322] 1.4839909\n",
            "125 [0.6244073, 0.86199564] 1.486403\n",
            "126 [0.63824815, 0.8230698] 1.461318\n",
            "127 [0.6276803, 0.85849476] 1.4861751\n",
            "128 [0.59491545, 0.8612936] 1.4562091\n",
            "129 [0.6152095, 0.9068387] 1.5220482\n",
            "130 [0.63445055, 0.8276095] 1.46206\n",
            "131 [0.6072907, 0.8612177] 1.4685084\n",
            "132 [0.6333317, 0.8275141] 1.4608458\n",
            "133 [0.6378113, 0.80932426] 1.4471356\n",
            "134 [0.6395648, 0.8850489] 1.5246137\n",
            "135 [0.6295101, 0.8390665] 1.4685767\n",
            "136 [0.63327783, 0.8847405] 1.5180182\n",
            "137 [0.62074476, 0.9222455] 1.5429902\n",
            "138 [0.6385603, 0.8425297] 1.4810901\n",
            "139 [0.61499375, 0.84976715] 1.4647609\n",
            "140 [0.5891938, 1.0502875] 1.6394813\n",
            "141 [0.5952441, 0.87058073] 1.4658248\n",
            "142 [0.62221885, 0.8561167] 1.4783356\n",
            "143 [0.6396509, 0.8030319] 1.4426827\n",
            "144 [0.6551444, 0.8427664] 1.4979107\n",
            "145 [0.62146854, 0.94136596] 1.5628345\n",
            "146 [0.5905583, 0.9789051] 1.5694634\n",
            "147 [0.6054002, 0.89673305] 1.5021333\n",
            "148 [0.58472586, 0.92735326] 1.5120791\n",
            "149 [0.5980642, 0.88966197] 1.4877262\n",
            "150 [0.6030444, 0.8871937] 1.4902381\n",
            "151 [0.66803896, 0.8261242] 1.4941632\n",
            "152 [0.6637086, 0.85356766] 1.5172763\n",
            "153 [0.6678416, 0.8140771] 1.4819187\n",
            "154 [0.6567477, 0.83694714] 1.4936948\n",
            "155 [0.69026667, 0.76842415] 1.4586909\n",
            "156 [0.64185685, 0.84385556] 1.4857124\n",
            "157 [0.6312565, 0.83091587] 1.4621724\n",
            "158 [0.60985845, 0.9236982] 1.5335567\n",
            "159 [0.5470081, 0.9985432] 1.5455513\n",
            "160 [0.6058584, 0.87819326] 1.4840517\n",
            "161 [0.6145031, 0.8391023] 1.4536054\n",
            "162 [0.6220409, 0.9036953] 1.5257362\n",
            "163 [0.6150875, 0.8545911] 1.4696786\n",
            "164 [0.5869807, 0.8761191] 1.4630997\n",
            "165 [0.63766855, 0.83154607] 1.4692147\n",
            "166 [0.62574947, 0.86094224] 1.4866917\n",
            "167 [0.6393376, 0.8379003] 1.4772379\n",
            "168 [0.61721796, 0.86797243] 1.4851904\n",
            "169 [0.6469922, 0.8095914] 1.4565836\n",
            "170 [0.6002488, 0.8685552] 1.468804\n",
            "171 [0.6067503, 0.87278235] 1.4795327\n",
            "172 [0.6307364, 0.8412213] 1.4719577\n",
            "173 [0.5995396, 0.8670283] 1.4665679\n",
            "174 [0.5971446, 0.904312] 1.5014566\n",
            "175 [0.63224006, 0.8522133] 1.4844534\n",
            "176 [0.63798714, 0.83819455] 1.4761817\n",
            "177 [0.6232113, 0.8248338] 1.4480451\n",
            "178 [0.6022378, 0.8312763] 1.4335141\n",
            "179 [0.60510224, 0.8751563] 1.4802585\n",
            "180 [0.5862947, 0.9149787] 1.5012734\n",
            "181 [0.6124197, 0.8409719] 1.4533916\n",
            "182 [0.6238606, 0.90019876] 1.5240593\n",
            "183 [0.65814966, 0.78970027] 1.44785\n",
            "184 [0.65237105, 0.7904932] 1.4428642\n",
            "185 [0.61100286, 0.8580462] 1.469049\n",
            "186 [0.60363704, 0.89238364] 1.4960207\n",
            "187 [0.57853985, 0.92141944] 1.4999592\n",
            "188 [0.5862457, 0.89307094] 1.4793167\n",
            "189 [0.619791, 0.81844735] 1.4382384\n",
            "190 [0.647249, 0.79019654] 1.4374455\n",
            "191 [0.6581988, 0.7839975] 1.4421962\n",
            "192 [0.6715393, 0.7737087] 1.445248\n",
            "193 [0.6620498, 0.8197395] 1.4817894\n",
            "194 [0.62887603, 0.82379544] 1.4526715\n",
            "195 [0.57291687, 0.9228518] 1.4957687\n",
            "196 [0.57114875, 0.890778] 1.4619267\n",
            "197 [0.583826, 0.9193202] 1.5031462\n",
            "198 [0.64482665, 0.90368855] 1.5485152\n",
            "199 [0.692091, 0.78961325] 1.4817042\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8w0etaSQCTfl"
      },
      "source": [
        "S = preprocess_support(A)\n",
        "X = tf.math.l2_normalize(toTensor(X), axis=1)\n",
        "embs = model.embed(X, S) # node embeddings"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AdVqsS7rCW1Q"
      },
      "source": [
        "# node clustering using the KMeans algorithm\n",
        "from sklearn.cluster import KMeans\n",
        "y_pred = KMeans(n_clusters=Y.shape[1]).fit(embs).predict(embs)\n",
        "y_true = np.argmax(Y, axis=1)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ymukgNdsCZLU",
        "outputId": "e3961f31-06cd-4f05-f733-8f192234b62b"
      },
      "source": [
        "# result\n",
        "from sklearn.metrics import adjusted_mutual_info_score\n",
        "print(adjusted_mutual_info_score(y_true, y_pred))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.48847694277422654\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}