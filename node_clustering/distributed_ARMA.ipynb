{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "distributed_ARMA.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "zzzB2uJwBC-m",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "94e03347-0ef0-4552-e058-116833a43175"
      },
      "source": [
        "!sudo apt-get install libmetis-dev\n",
        "!pip install metis\n",
        "import metis\n",
        "import random\n",
        "from itertools import chain\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from scipy import sparse\n",
        "import scipy.io as sio"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "libmetis-dev is already the newest version (5.1.0.dfsg-5).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 30 not upgraded.\n",
            "Requirement already satisfied: metis in /usr/local/lib/python3.7/dist-packages (0.2a5)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p-5rUhczBJdV"
      },
      "source": [
        "# hyperparameters\n",
        "hidden = 512 # number of hidden units in the encoder layer\n",
        "latent = 256 # dimension of the latent variables\n",
        "learning_rate = 0.01\n",
        "epochs = 200\n",
        "nparts = 1500 # number of partitions\n",
        "batch_size = 20 # number of clusters per batch\n",
        "K = 3 # number of iterations\n",
        "T = 2 # number of threads"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RflJi-R3BOkG"
      },
      "source": [
        "filename = '/content/drive/MyDrive/GRAPH DATA/reddit.mat' # dataset"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ABjlQkpsDFgx"
      },
      "source": [
        "mat_dict = sio.loadmat(filename)\n",
        "A = mat_dict['A'].ceil()\n",
        "X = mat_dict['X']\n",
        "Y = mat_dict['Y']\n",
        "train_mask = mat_dict['train_mask'].squeeze().astype(bool)\n",
        "val_mask = mat_dict['val_mask'].squeeze().astype(bool)\n",
        "test_mask = mat_dict['test_mask'].squeeze().astype(bool)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a2GhIMjvDNzG",
        "outputId": "7bbfede4-780b-49ec-a246-e0d8efecf497"
      },
      "source": [
        "def cluster_graph(A, nparts):\n",
        "  if nparts == 1:\n",
        "    edge_cuts, parts = 0, [0, ] * A.shape[0]\n",
        "  else:\n",
        "    edge_cuts, parts = metis.part_graph([neighbors for neighbors in A.tolil().rows], nparts=nparts)\n",
        "  print('Number of edge cuts: %d.' % edge_cuts)\n",
        "  cluster_dict = {}\n",
        "  for index, part in enumerate(parts):\n",
        "    if part not in cluster_dict:\n",
        "      cluster_dict[part] = []\n",
        "    cluster_dict[part].append(index)\n",
        "  return cluster_dict\n",
        "\n",
        "# the clustering algorithm (METIS)\n",
        "cluster_dict = cluster_graph(A, nparts)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of edge cuts: 9609639.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FstzOA7oDgoL"
      },
      "source": [
        "def preprocess_support(A):\n",
        "  N = A.shape[1]\n",
        "  D = sparse.csr_matrix(A.sum(axis=1))\n",
        "  norm = D.power(-0.5)\n",
        "  P = A.multiply(norm).T.multiply(norm)\n",
        "  return P\n",
        "\n",
        "def toTensorSparse(S):\n",
        "  return tf.constant(S.todense())\n",
        "\n",
        "def toTensor(T):\n",
        "  return tf.constant(T)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OVOs3s5TDrHJ"
      },
      "source": [
        "# layer classes\n",
        "\n",
        "class bilinear_layer:\n",
        "\n",
        "  def __init__(self, indim, outdim):\n",
        "    pass\n",
        "\n",
        "  def __call__(self, tensor):\n",
        "    return tf.linalg.matmul(tensor, tf.transpose(tensor))\n",
        "\n",
        "# unused\n",
        "class FC_layer:\n",
        "\n",
        "  def __init__(self, indim, outdim):\n",
        "    initial_value = tf.initializers.he_normal()((indim, outdim,))\n",
        "    self.weight = tf.Variable(initial_value=initial_value, trainable=True)\n",
        "\n",
        "  def __call__(self, tensor):\n",
        "    return tf.linalg.matmul(tensor, self.weight)\n",
        "\n",
        "class GC_layer:\n",
        "\n",
        "  def __init__(self, indim, outdim):\n",
        "    global K\n",
        "    global T\n",
        "    self.K = K\n",
        "    self.T = T\n",
        "    self.ws = []\n",
        "    self.vs = []\n",
        "    for t in range(T):\n",
        "      w_threads = []\n",
        "      v_threads = []\n",
        "      for k in range(self.K):\n",
        "        initial_value = tf.initializers.ones()((1,1))\n",
        "        w_threads.append(tf.Variable(initial_value=(1.0,) * outdim, trainable=True))\n",
        "        initial_value = tf.initializers.ones()((1,1))\n",
        "        v_threads.append(tf.Variable(initial_value=(0.0,) * outdim, trainable=True))\n",
        "      self.ws.append(w_threads)\n",
        "      self.vs.append(v_threads)\n",
        "    initial_value = tf.initializers.he_normal()((indim, outdim,))\n",
        "    self.weight = tf.Variable(initial_value=initial_value, trainable=True)\n",
        "\n",
        "  def __call__(self, tensor, support, embed=False):\n",
        "    if embed: # numpy pipeline\n",
        "      tensor = tensor.numpy().dot(self.weight.numpy())\n",
        "      results = []\n",
        "      for t in range(self.T):\n",
        "         # Personalized PageRank\n",
        "        tensor_ = tensor\n",
        "        for k in range(self.K):\n",
        "          tensor_ = self.ws[t][k].numpy() * support.dot(tensor_) + self.vs[t][k].numpy() * tensor\n",
        "        results.append(tensor_)\n",
        "      return sum(results)\n",
        "    else: # tensorflow pipeline\n",
        "      tensor = tf.linalg.matmul(tensor, self.weight)\n",
        "      results = []\n",
        "      for t in range(self.T):\n",
        "         # Personalized PageRank\n",
        "        tensor_ = tensor\n",
        "        for k in range(self.K):\n",
        "          tensor_ = self.ws[t][k] * tf.linalg.matmul(support, tensor_) + self.vs[t][k] * tensor\n",
        "        results.append(tensor_)\n",
        "      return sum(results)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qwEu7WB9EWAI"
      },
      "source": [
        "# our model class (for the paper \"Scalable Graph Variational Autoencoders\")\n",
        "\n",
        "class Model:\n",
        "\n",
        "  def __init__(self, size_tuple, optimizer, nonlinear):\n",
        "    self.sources = [] # variables to optimize\n",
        "    self.build(size_tuple) # builds the model by stacking layers on each other\n",
        "    self.optimizer = optimizer\n",
        "    self.nonlinear = nonlinear\n",
        "    self.Z_mean = None # mean embedding layer\n",
        "    self.Z_var = None # variance embedding layer\n",
        "    self.noise = None # the noise sample\n",
        "    self.sample = None # self.Z_mean + self.Z_var * self.noise\n",
        "    self.A_gamma = None # the reconstructions\n",
        "  \n",
        "  def build(self, size_tuple):\n",
        "    X_dim, hidden, latent = size_tuple\n",
        "    self.enc_layer = GC_layer(X_dim, hidden)\n",
        "    self.enc_mean_layer = GC_layer(hidden, latent)\n",
        "    self.enc_var_layer = GC_layer(hidden, latent)\n",
        "    self.A_dec_gamma_layer = bilinear_layer(latent, latent)\n",
        "    # filling the source array with weights\n",
        "    layers = [self.enc_layer, self.enc_mean_layer, self.enc_var_layer]\n",
        "    for layer in layers:\n",
        "      self.sources.append(layer.weight)\n",
        "      self.sources += list(chain.from_iterable(layer.ws)) + list(chain.from_iterable(layer.vs))\n",
        "  \n",
        "  # forward propagation in the encoder\n",
        "  def encode(self, X, S):\n",
        "    enc = self.nonlinear(self.enc_layer(X, S))\n",
        "    enc_mean = self.enc_mean_layer(enc, S)\n",
        "    enc_var = tf.math.exp(self.enc_var_layer(enc, S))\n",
        "    return enc_mean, enc_var\n",
        "\n",
        "  # returns only the node embeddings\n",
        "  def embed(self, X, S):\n",
        "    enc = self.nonlinear(self.enc_layer(X, S, embed=True))\n",
        "    enc_mean = self.enc_mean_layer(enc, S, embed=True)\n",
        "    return enc_mean\n",
        "\n",
        "  # forward propagation in the decoder\n",
        "  def decode(self, sample):\n",
        "    A_dec_gamma = self.A_dec_gamma_layer(sample)\n",
        "    return A_dec_gamma\n",
        "\n",
        "  def predict(self, X, S):\n",
        "    self.Z_mean, self.Z_var = self.encode(X, S)\n",
        "    self.noise = tf.random.normal(self.Z_var.shape)\n",
        "    self.sample = self.Z_mean + self.Z_var * self.noise # reparameterization trick\n",
        "    self.A_gamma = self.decode(self.sample)\n",
        "\n",
        "  def train(self, X, A, cluster_dict, batch_size, epochs):\n",
        "    for epoch in range(epochs):\n",
        "      # only a subgraph is used in the training process\n",
        "      samples = random.sample(cluster_dict.keys(), batch_size)\n",
        "      nodes = sum([cluster_dict[sample] for sample in samples], [])\n",
        "      S_batch = toTensorSparse(preprocess_support(A[nodes].T[nodes]))\n",
        "      A_batch = toTensor(A.T[nodes].T[nodes].todense())\n",
        "      X_batch = tf.math.l2_normalize(toTensor(X[nodes]), axis=1)\n",
        "      # optimization\n",
        "      with tf.GradientTape() as tape:\n",
        "        self.predict(X_batch, S_batch)\n",
        "        losses = self.loss(A_batch, X_batch)\n",
        "        loss_ = tf.reduce_sum(losses)\n",
        "      print(epoch, [loss.numpy() for loss in losses], loss_.numpy())\n",
        "      grads = tape.gradient(loss_, self.sources)\n",
        "      self.optimizer.apply_gradients(zip(grads, self.sources))\n",
        "\n",
        "  # Kullback–Leibler divergence\n",
        "  def KL_Divergence(self):\n",
        "    loss = 0.5 * tf.reduce_mean(self.Z_mean**2.0 + self.Z_var**2.0 - 2.0 * tf.math.log(self.Z_var) - 1.0)\n",
        "    return loss\n",
        "\n",
        "  # reconstruction loss\n",
        "  def re_A_loss(self, A):\n",
        "    density = tf.reduce_sum(A) / tf.size(A, out_type=tf.float32)\n",
        "    pos_weight = (1.0 - density) / density\n",
        "    loss = -0.5 * tf.reduce_mean(1.0 / (1.0 - density) * tf.nn.weighted_cross_entropy_with_logits(labels=A, logits=self.A_gamma, pos_weight=pos_weight))\n",
        "    return -loss\n",
        "\n",
        "  # list of all loss functions\n",
        "  def loss(self, A, X):\n",
        "    return self.KL_Divergence(), self.re_A_loss(A)\n"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0zuy3DUXLK4J",
        "outputId": "123fb470-f55e-4dbc-a377-846a2e61b96f"
      },
      "source": [
        "size_tuple = (X.shape[1], hidden, latent)\n",
        "optimizer = tf.optimizers.Adam(learning_rate=learning_rate)\n",
        "nonlinear = tf.nn.relu\n",
        "\n",
        "model = Model(size_tuple, optimizer, nonlinear)\n",
        "\n",
        "print('Training...')\n",
        "model.train(X, A, cluster_dict, batch_size, epochs)\n"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training...\n",
            "0 [0.05253221, 7.752583] 7.805115\n",
            "1 [0.1316293, 6.8970757] 7.028705\n",
            "2 [0.103592224, 3.284567] 3.3881593\n",
            "3 [0.11459511, 3.2251737] 3.339769\n",
            "4 [0.16292262, 2.8630779] 3.0260005\n",
            "5 [0.2922866, 2.5808218] 2.8731084\n",
            "6 [0.40841094, 2.2481785] 2.6565895\n",
            "7 [0.38590783, 1.7438947] 2.1298025\n",
            "8 [0.44854495, 1.4378425] 1.8863875\n",
            "9 [0.57055163, 1.1967516] 1.7673032\n",
            "10 [0.6777655, 1.0805383] 1.7583038\n",
            "11 [0.76226115, 0.96510106] 1.7273622\n",
            "12 [0.7841671, 0.8878813] 1.6720483\n",
            "13 [0.8641766, 0.80950844] 1.6736851\n",
            "14 [0.7478354, 0.8804591] 1.6282945\n",
            "15 [0.7398347, 0.89880925] 1.638644\n",
            "16 [0.644827, 0.98742694] 1.6322539\n",
            "17 [0.6643279, 1.0158188] 1.6801467\n",
            "18 [0.63837653, 1.0234615] 1.661838\n",
            "19 [0.63903457, 1.0091599] 1.6481946\n",
            "20 [0.6166922, 0.9896429] 1.6063352\n",
            "21 [0.63435096, 0.94981396] 1.5841649\n",
            "22 [0.6792267, 0.90958005] 1.5888067\n",
            "23 [0.7050769, 0.8658333] 1.5709102\n",
            "24 [0.76702887, 0.8356003] 1.6026292\n",
            "25 [0.73512214, 0.8259166] 1.5610387\n",
            "26 [0.72359884, 0.83582884] 1.5594277\n",
            "27 [0.6372122, 0.94744396] 1.5846562\n",
            "28 [0.6062358, 0.9439614] 1.5501971\n",
            "29 [0.62063754, 0.9868111] 1.6074486\n",
            "30 [0.64203507, 0.9004648] 1.5424998\n",
            "31 [0.6711251, 0.8588715] 1.5299966\n",
            "32 [0.7068449, 0.8572767] 1.5641216\n",
            "33 [0.6561395, 0.88521093] 1.5413504\n",
            "34 [0.63868374, 0.88895357] 1.5276372\n",
            "35 [0.6319238, 0.8900378] 1.5219616\n",
            "36 [0.6586165, 0.8855384] 1.5441549\n",
            "37 [0.6177169, 0.89269847] 1.5104153\n",
            "38 [0.6457937, 0.97732335] 1.623117\n",
            "39 [0.6295892, 0.9245092] 1.5540984\n",
            "40 [0.6575704, 0.87618744] 1.5337579\n",
            "41 [0.6156513, 0.9200575] 1.5357088\n",
            "42 [0.66801524, 1.0323017] 1.7003169\n",
            "43 [0.6028812, 0.92566] 1.5285412\n",
            "44 [0.60040075, 0.9890641] 1.5894649\n",
            "45 [0.6674485, 0.9186588] 1.5861073\n",
            "46 [0.70857376, 0.91026986] 1.6188436\n",
            "47 [0.7927567, 0.8023177] 1.5950744\n",
            "48 [0.7105669, 0.8516516] 1.5622184\n",
            "49 [0.6712053, 0.86078084] 1.5319861\n",
            "50 [0.60278, 0.9340612] 1.5368412\n",
            "51 [0.6494379, 0.88203824] 1.5314761\n",
            "52 [0.65208524, 0.89209104] 1.5441763\n",
            "53 [0.6486316, 0.88535976] 1.5339913\n",
            "54 [0.73088604, 0.8132918] 1.5441778\n",
            "55 [0.75155884, 0.7747846] 1.5263435\n",
            "56 [0.6650747, 0.83940077] 1.5044755\n",
            "57 [0.627828, 0.8668985] 1.4947264\n",
            "58 [0.5989844, 0.9377411] 1.5367255\n",
            "59 [0.6463678, 0.8479247] 1.4942925\n",
            "60 [0.62187576, 0.8214704] 1.4433461\n",
            "61 [0.63620126, 0.9077682] 1.5439694\n",
            "62 [0.6496485, 0.92684984] 1.5764983\n",
            "63 [0.6024255, 0.95758075] 1.5600063\n",
            "64 [0.6148665, 0.87931997] 1.4941864\n",
            "65 [0.6490773, 0.8573749] 1.5064522\n",
            "66 [0.6871489, 0.84240246] 1.5295514\n",
            "67 [0.70666933, 0.7627429] 1.4694122\n",
            "68 [0.6800875, 0.81516445] 1.4952519\n",
            "69 [0.6310808, 0.83495754] 1.4660383\n",
            "70 [0.55646944, 0.9193223] 1.4757917\n",
            "71 [0.59855014, 0.9014454] 1.4999955\n",
            "72 [0.6381974, 0.8132503] 1.4514477\n",
            "73 [0.6940825, 0.7936247] 1.4877071\n",
            "74 [0.669086, 0.850965] 1.520051\n",
            "75 [0.6695106, 0.7849931] 1.4545038\n",
            "76 [0.6454708, 0.81169516] 1.457166\n",
            "77 [0.6165868, 0.85789454] 1.4744813\n",
            "78 [0.61233324, 0.85324395] 1.4655771\n",
            "79 [0.64089763, 0.84817284] 1.4890704\n",
            "80 [0.66764855, 0.86228764] 1.5299362\n",
            "81 [0.5990993, 0.9084189] 1.5075182\n",
            "82 [0.6127808, 0.86152595] 1.4743068\n",
            "83 [0.579296, 0.89011747] 1.4694135\n",
            "84 [0.6186971, 0.8559615] 1.4746586\n",
            "85 [0.6926234, 0.81245804] 1.5050814\n",
            "86 [0.70873296, 0.76637006] 1.475103\n",
            "87 [0.6291239, 0.84424937] 1.4733733\n",
            "88 [0.6031873, 0.85133266] 1.45452\n",
            "89 [0.59159046, 0.90073943] 1.4923298\n",
            "90 [0.65827966, 0.82128376] 1.4795635\n",
            "91 [0.6359294, 0.83379906] 1.4697285\n",
            "92 [0.6760607, 0.77222896] 1.4482896\n",
            "93 [0.6692667, 0.83611465] 1.5053813\n",
            "94 [0.5617586, 0.9277667] 1.4895253\n",
            "95 [0.5825675, 0.8831184] 1.4656858\n",
            "96 [0.61604595, 0.8569993] 1.4730452\n",
            "97 [0.638392, 0.8364164] 1.4748085\n",
            "98 [0.70233303, 0.74452096] 1.446854\n",
            "99 [0.67118615, 0.7822406] 1.4534268\n",
            "100 [0.6791857, 0.7722008] 1.4513865\n",
            "101 [0.61221945, 0.8614659] 1.4736853\n",
            "102 [0.63184786, 0.8575413] 1.4893892\n",
            "103 [0.57706094, 0.88934845] 1.4664094\n",
            "104 [0.637853, 0.8596337] 1.4974867\n",
            "105 [0.65277463, 0.81510985] 1.4678845\n",
            "106 [0.64582783, 0.8238675] 1.4696953\n",
            "107 [0.64253736, 0.8190558] 1.4615932\n",
            "108 [0.6519871, 0.8817392] 1.5337262\n",
            "109 [0.6041702, 0.88862175] 1.4927919\n",
            "110 [0.5950064, 0.9253839] 1.5203903\n",
            "111 [0.6328197, 0.8277083] 1.460528\n",
            "112 [0.66399044, 0.8260391] 1.4900296\n",
            "113 [0.66113895, 0.770022] 1.4311609\n",
            "114 [0.66989523, 0.79170674] 1.461602\n",
            "115 [0.63487375, 0.86257595] 1.4974496\n",
            "116 [0.6125516, 0.82980573] 1.4423573\n",
            "117 [0.65841687, 0.78512824] 1.4435451\n",
            "118 [0.6624237, 0.82141685] 1.4838405\n",
            "119 [0.661486, 0.7879197] 1.4494057\n",
            "120 [0.6530528, 0.8286383] 1.4816911\n",
            "121 [0.6357194, 0.8430526] 1.478772\n",
            "122 [0.6069085, 0.8439067] 1.4508152\n",
            "123 [0.5408618, 0.8904525] 1.4313142\n",
            "124 [0.64298344, 0.89629835] 1.5392818\n",
            "125 [0.5950488, 0.85672385] 1.4517727\n",
            "126 [0.61717325, 0.8453539] 1.4625272\n",
            "127 [0.652971, 0.8367278] 1.4896989\n",
            "128 [0.6734243, 0.80776757] 1.4811919\n",
            "129 [0.63617647, 0.90442026] 1.5405967\n",
            "130 [0.6208228, 0.875128] 1.4959507\n",
            "131 [0.60753596, 0.8595097] 1.4670457\n",
            "132 [0.6106474, 0.81435776] 1.4250052\n",
            "133 [0.67189294, 0.8897203] 1.5616133\n",
            "134 [0.65437174, 1.0608612] 1.715233\n",
            "135 [0.5066713, 0.9597021] 1.4663734\n",
            "136 [0.5294544, 0.96564955] 1.495104\n",
            "137 [0.6386239, 0.8594192] 1.4980431\n",
            "138 [0.73324513, 0.7679955] 1.5012406\n",
            "139 [0.81482637, 0.7322926] 1.5471189\n",
            "140 [0.7191498, 0.7566434] 1.4757932\n",
            "141 [0.6207875, 0.8576639] 1.4784515\n",
            "142 [0.52259326, 1.0422647] 1.564858\n",
            "143 [0.6068348, 0.90437144] 1.5112063\n",
            "144 [0.6460733, 0.8288751] 1.4749484\n",
            "145 [0.7071092, 0.843032] 1.5501412\n",
            "146 [0.7550432, 0.7159884] 1.4710317\n",
            "147 [0.7191185, 0.7861613] 1.5052798\n",
            "148 [0.6587636, 0.84837496] 1.5071385\n",
            "149 [0.5888863, 0.8834943] 1.4723806\n",
            "150 [0.55107194, 0.9513366] 1.5024085\n",
            "151 [0.50413865, 1.0025843] 1.5067229\n",
            "152 [0.6155087, 0.84734255] 1.4628513\n",
            "153 [0.7024018, 0.72672915] 1.429131\n",
            "154 [0.7549758, 0.7212586] 1.4762344\n",
            "155 [0.7679077, 0.7622391] 1.5301468\n",
            "156 [0.67075735, 0.7839904] 1.4547477\n",
            "157 [0.58115864, 0.83531356] 1.4164722\n",
            "158 [0.5674138, 0.9361129] 1.5035267\n",
            "159 [0.5573422, 0.91581833] 1.4731605\n",
            "160 [0.5827765, 0.85963196] 1.4424084\n",
            "161 [0.6640841, 0.86251146] 1.5265956\n",
            "162 [0.66898286, 0.8492969] 1.5182798\n",
            "163 [0.7064583, 0.86473584] 1.5711942\n",
            "165 [0.62432337, 0.84023565] 1.4645591\n",
            "166 [0.6198167, 0.84024054] 1.4600573\n",
            "167 [0.6542377, 0.7747061] 1.4289439\n",
            "168 [0.6831689, 0.88387305] 1.5670419\n",
            "169 [0.67157197, 0.8200308] 1.4916028\n",
            "170 [0.66432214, 0.7533465] 1.4176686\n",
            "171 [0.6459785, 0.81531096] 1.4612894\n",
            "172 [0.603091, 0.82714915] 1.4302402\n",
            "173 [0.6106983, 0.8384317] 1.44913\n",
            "174 [0.61068857, 0.85859656] 1.4692851\n",
            "175 [0.6473962, 0.8797116] 1.5271078\n",
            "176 [0.5777124, 0.91805077] 1.4957632\n",
            "177 [0.579331, 0.8639403] 1.4432713\n",
            "178 [0.6555719, 0.9275703] 1.5831422\n",
            "179 [0.6295919, 0.7928269] 1.4224188\n",
            "180 [0.65977204, 0.8265785] 1.4863505\n",
            "181 [0.6223284, 0.884403] 1.5067314\n",
            "182 [0.60575336, 0.9119519] 1.5177052\n",
            "183 [0.5974773, 0.8389686] 1.436446\n",
            "184 [0.6251422, 0.8728514] 1.4979936\n",
            "185 [0.6938121, 0.8227552] 1.5165672\n",
            "186 [0.7203415, 0.730965] 1.4513066\n",
            "187 [0.688371, 0.8076655] 1.4960365\n",
            "188 [0.60703063, 0.83619577] 1.4432263\n",
            "189 [0.5768489, 0.88852566] 1.4653746\n",
            "190 [0.66938335, 0.8478017] 1.517185\n",
            "191 [0.62806094, 0.7905262] 1.4185872\n",
            "192 [0.71679795, 0.79846317] 1.5152612\n",
            "193 [0.65345407, 0.7625673] 1.4160213\n",
            "194 [0.6048695, 0.8197843] 1.4246538\n",
            "195 [0.6357771, 0.8034552] 1.4392323\n",
            "196 [0.6272914, 0.81531143] 1.4426029\n",
            "197 [0.6086013, 0.8592019] 1.4678032\n",
            "198 [0.6235124, 0.806956] 1.4304683\n",
            "199 [0.6191228, 0.80993885] 1.4290617\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m1YlDtnLDVoc"
      },
      "source": [
        "S = preprocess_support(A)\n",
        "X = tf.math.l2_normalize(toTensor(X), axis=1)\n",
        "embs = model.embed(X, S) # node embeddings"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tKtnEIBADWX7"
      },
      "source": [
        "# node clustering using the KMeans algorithm\n",
        "from sklearn.cluster import KMeans\n",
        "y_pred = KMeans(n_clusters=Y.shape[1]).fit(embs).predict(embs)\n",
        "y_true = np.argmax(Y, axis=1)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y5InZim4DWco",
        "outputId": "1ba53fd3-d966-4c17-dde1-dd5b3f82d729"
      },
      "source": [
        "# result\n",
        "from sklearn.metrics import adjusted_mutual_info_score\n",
        "print(adjusted_mutual_info_score(y_true, y_pred))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.42917532706246153\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}