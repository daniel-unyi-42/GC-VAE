{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "polynomial.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "zzzB2uJwBC-m",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9554da84-d6a6-43be-a92b-16d39eee9696"
      },
      "source": [
        "!sudo apt-get install libmetis-dev\n",
        "!pip install metis\n",
        "import metis\n",
        "import random\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from scipy import sparse\n",
        "import scipy.io as sio"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "libmetis-dev is already the newest version (5.1.0.dfsg-5).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 30 not upgraded.\n",
            "Requirement already satisfied: metis in /usr/local/lib/python3.7/dist-packages (0.2a5)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p-5rUhczBJdV"
      },
      "source": [
        "# hyperparameters\n",
        "hidden = 512 # number of hidden units in the encoder layer\n",
        "latent = 256 # dimension of the latent variables\n",
        "learning_rate = 0.01\n",
        "epochs = 200\n",
        "nparts = 1500 # number of partitions\n",
        "batch_size = 20 # number of clusters per batch\n",
        "K = 3 # degree of polynomial filter"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RflJi-R3BOkG"
      },
      "source": [
        "filename = '/content/drive/MyDrive/GRAPH DATA/reddit.mat' # dataset"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ABjlQkpsDFgx"
      },
      "source": [
        "mat_dict = sio.loadmat(filename)\n",
        "A = mat_dict['A'].ceil()\n",
        "X = mat_dict['X']\n",
        "Y = mat_dict['Y']\n",
        "train_mask = mat_dict['train_mask'].squeeze().astype(bool)\n",
        "val_mask = mat_dict['val_mask'].squeeze().astype(bool)\n",
        "test_mask = mat_dict['test_mask'].squeeze().astype(bool)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a2GhIMjvDNzG",
        "outputId": "7cd9afa4-e4e1-4156-a077-4e272b289b58"
      },
      "source": [
        "def cluster_graph(A, nparts):\n",
        "  if nparts == 1:\n",
        "    edge_cuts, parts = 0, [0, ] * A.shape[0]\n",
        "  else:\n",
        "    edge_cuts, parts = metis.part_graph([neighbors for neighbors in A.tolil().rows], nparts=nparts)\n",
        "  print('Number of edge cuts: %d.' % edge_cuts)\n",
        "  cluster_dict = {}\n",
        "  for index, part in enumerate(parts):\n",
        "    if part not in cluster_dict:\n",
        "      cluster_dict[part] = []\n",
        "    cluster_dict[part].append(index)\n",
        "  return cluster_dict\n",
        "\n",
        "# the clustering algorithm (METIS)\n",
        "cluster_dict = cluster_graph(A, nparts)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of edge cuts: 9609639.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FstzOA7oDgoL"
      },
      "source": [
        "def preprocess_support(A):\n",
        "  N = A.shape[1]\n",
        "  D = sparse.csr_matrix(A.sum(axis=1))\n",
        "  norm = D.power(-0.5)\n",
        "  L = sparse.eye(N, dtype='float32') - A.multiply(norm).T.multiply(norm)\n",
        "  max_eigval = sparse.linalg.eigsh(L, k=1, return_eigenvectors=False)[0]\n",
        "  L_ = 2.0 / max_eigval * L - sparse.eye(N, dtype='float32')\n",
        "  return L_\n",
        "\n",
        "def toTensorSparse(S):\n",
        "  return tf.constant(S.todense())\n",
        "\n",
        "def toTensor(T):\n",
        "  return tf.constant(T)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OVOs3s5TDrHJ"
      },
      "source": [
        "# layer classes\n",
        "\n",
        "class bilinear_layer:\n",
        "\n",
        "  def __init__(self, indim, outdim):\n",
        "    pass\n",
        "\n",
        "  def __call__(self, tensor):\n",
        "    return tf.linalg.matmul(tensor, tf.transpose(tensor))\n",
        "\n",
        "# unused\n",
        "class FC_layer:\n",
        "\n",
        "  def __init__(self, indim, outdim):\n",
        "    initial_value = tf.initializers.he_normal()((indim, outdim,))\n",
        "    self.weight = tf.Variable(initial_value=initial_value, trainable=True)\n",
        "\n",
        "  def __call__(self, tensor):\n",
        "    return tf.linalg.matmul(tensor, self.weight)\n",
        "\n",
        "class GC_layer:\n",
        "\n",
        "  def __init__(self, indim, outdim):\n",
        "    global K\n",
        "    initial_value = tf.initializers.he_normal()((indim, outdim,))\n",
        "    self.weight = tf.Variable(initial_value=initial_value, trainable=True)\n",
        "    delta = np.zeros((K + 1, outdim), dtype='float32')\n",
        "    for o in range(outdim):\n",
        "      delta[0, o] = 1.0\n",
        "    self.coeffs = tf.Variable(initial_value=delta, trainable=True)\n",
        "\n",
        "  def __call__(self, tensor, support, embed=False):\n",
        "    global K\n",
        "    if embed: # numpy pipeline\n",
        "      transform = tensor.numpy().dot(self.weight.numpy())\n",
        "      # Legendre polynomials\n",
        "      basis = [transform]\n",
        "      if (K > 0):\n",
        "        basis.append(support.dot(transform))\n",
        "      if (K > 1):\n",
        "        for k in range(2, K + 1):\n",
        "          basis.append((2.0 * k - 1.0) / k * support.dot(basis[k-1]) - (k - 1.0) / k * basis[k-2])\n",
        "      # linear combination\n",
        "      result = np.zeros(transform.shape)\n",
        "      for coeff, base in zip(self.coeffs.numpy(), basis):\n",
        "        result += base * coeff\n",
        "      return result\n",
        "    else: # tensorflow pipeline\n",
        "      transform = tf.linalg.matmul(tensor, self.weight)\n",
        "      # Legendre polynomials\n",
        "      basis = [transform]\n",
        "      if (K > 0):\n",
        "        basis.append(tf.linalg.matmul(support, transform))\n",
        "      if (K > 1):\n",
        "        for k in range(2, K + 1):\n",
        "          basis.append((2.0 * k - 1.0) / k * tf.linalg.matmul(support, basis[k-1]) - (k - 1.0) / k * basis[k-2])\n",
        "      # linear combination\n",
        "      result = tf.zeros(transform.shape)\n",
        "      for k in range(K + 1):\n",
        "        result += self.coeffs[k] * basis[k]\n",
        "      return result"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qwEu7WB9EWAI"
      },
      "source": [
        "# our model class (for the paper \"Scalable Graph Variational Autoencoders\")\n",
        "\n",
        "class Model:\n",
        "\n",
        "  def __init__(self, size_tuple, optimizer, nonlinear):\n",
        "    self.sources = [] # variables to optimize\n",
        "    self.build(size_tuple) # builds the model by stacking layers on each other\n",
        "    self.optimizer = optimizer\n",
        "    self.nonlinear = nonlinear\n",
        "    self.Z_mean = None # mean embedding layer\n",
        "    self.Z_var = None # variance embedding layer\n",
        "    self.noise = None # the noise sample\n",
        "    self.sample = None # self.Z_mean + self.Z_var * self.noise\n",
        "    self.A_gamma = None # the reconstructions\n",
        "  \n",
        "  def build(self, size_tuple):\n",
        "    X_dim, hidden, latent = size_tuple\n",
        "    self.enc_layer = GC_layer(X_dim, hidden)\n",
        "    self.enc_mean_layer = GC_layer(hidden, latent)\n",
        "    self.enc_var_layer = GC_layer(hidden, latent)\n",
        "    self.A_dec_gamma_layer = bilinear_layer(latent, latent)\n",
        "    # filling the source array with weights\n",
        "    layers = [self.enc_layer, self.enc_mean_layer, self.enc_var_layer]\n",
        "    for layer in layers:\n",
        "      self.sources.append(layer.weight)\n",
        "      self.sources.append(layer.coeffs)\n",
        "  \n",
        "  # forward propagation in the encoder\n",
        "  def encode(self, X, S):\n",
        "    enc = self.nonlinear(self.enc_layer(X, S))\n",
        "    enc_mean = self.enc_mean_layer(enc, S)\n",
        "    enc_var = tf.math.exp(self.enc_var_layer(enc, S))\n",
        "    return enc_mean, enc_var\n",
        "\n",
        "  # returns only the node embeddings\n",
        "  def embed(self, X, S):\n",
        "    enc = self.nonlinear(self.enc_layer(X, S, embed=True))\n",
        "    enc_mean = self.enc_mean_layer(enc, S, embed=True)\n",
        "    return enc_mean\n",
        "\n",
        "  # forward propagation in the decoder\n",
        "  def decode(self, sample):\n",
        "    A_dec_gamma = self.A_dec_gamma_layer(sample)\n",
        "    return A_dec_gamma\n",
        "\n",
        "  def predict(self, X, S):\n",
        "    self.Z_mean, self.Z_var = self.encode(X, S)\n",
        "    self.noise = tf.random.normal(self.Z_var.shape)\n",
        "    self.sample = self.Z_mean + self.Z_var * self.noise # reparameterization trick\n",
        "    self.A_gamma = self.decode(self.sample)\n",
        "\n",
        "  def train(self, X, A, cluster_dict, batch_size, epochs):\n",
        "    for epoch in range(epochs):\n",
        "      # only a subgraph is used in the training process\n",
        "      samples = random.sample(cluster_dict.keys(), batch_size)\n",
        "      nodes = sum([cluster_dict[sample] for sample in samples], [])\n",
        "      S_batch = toTensorSparse(preprocess_support(A[nodes].T[nodes]))\n",
        "      A_batch = toTensor(A.T[nodes].T[nodes].todense())\n",
        "      X_batch = tf.math.l2_normalize(toTensor(X[nodes]), axis=1)\n",
        "      # optimization\n",
        "      with tf.GradientTape() as tape:\n",
        "        self.predict(X_batch, S_batch)\n",
        "        losses = self.loss(A_batch, X_batch)\n",
        "        loss_ = tf.reduce_sum(losses)\n",
        "      print(epoch, [loss.numpy() for loss in losses], loss_.numpy())\n",
        "      grads = tape.gradient(loss_, self.sources)\n",
        "      self.optimizer.apply_gradients(zip(grads, self.sources))\n",
        "\n",
        "  # Kullback–Leibler divergence\n",
        "  def KL_Divergence(self):\n",
        "    loss = 0.5 * tf.reduce_mean(self.Z_mean**2.0 + self.Z_var**2.0 - 2.0 * tf.math.log(self.Z_var) - 1.0)\n",
        "    return loss\n",
        "\n",
        "  # reconstruction loss\n",
        "  def re_A_loss(self, A):\n",
        "    density = tf.reduce_sum(A) / tf.size(A, out_type=tf.float32)\n",
        "    pos_weight = (1.0 - density) / density\n",
        "    loss = -0.5 * tf.reduce_mean(1.0 / (1.0 - density) * tf.nn.weighted_cross_entropy_with_logits(labels=A, logits=self.A_gamma, pos_weight=pos_weight))\n",
        "    return -loss\n",
        "\n",
        "  # list of all loss functions\n",
        "  def loss(self, A, X):\n",
        "    return self.KL_Divergence(), self.re_A_loss(A)\n"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0zuy3DUXLK4J",
        "outputId": "29488c07-9856-4f62-9867-99e23efa4f1e"
      },
      "source": [
        "size_tuple = (X.shape[1], hidden, latent)\n",
        "optimizer = tf.optimizers.Adam(learning_rate=learning_rate)\n",
        "nonlinear = tf.nn.relu\n",
        "\n",
        "model = Model(size_tuple, optimizer, nonlinear)\n",
        "\n",
        "print('Training...')\n",
        "model.train(X, A, cluster_dict, batch_size, epochs)\n"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training...\n",
            "0 [0.0051376955, 6.520226] 6.525364\n",
            "1 [0.027653715, 5.017509] 5.0451627\n",
            "2 [0.109020725, 3.3549967] 3.4640174\n",
            "3 [0.3321186, 2.01661] 2.3487284\n",
            "4 [0.697624, 1.6535718] 2.3511958\n",
            "5 [1.0100601, 1.4767085] 2.4867687\n",
            "6 [1.2009507, 1.0275496] 2.2285004\n",
            "7 [1.1872467, 0.882879] 2.0701256\n",
            "8 [1.1127493, 0.8707761] 1.9835255\n",
            "9 [1.0165356, 0.8938532] 1.9103888\n",
            "10 [0.86723447, 0.88719606] 1.7544305\n",
            "11 [0.710264, 0.962594] 1.672858\n",
            "12 [0.5993235, 1.1049657] 1.7042892\n",
            "13 [0.50550866, 1.2503694] 1.7558781\n",
            "14 [0.4586492, 1.35502] 1.8136692\n",
            "15 [0.45413637, 1.3559142] 1.8100506\n",
            "16 [0.48488876, 1.274455] 1.7593437\n",
            "17 [0.53360116, 1.1820498] 1.7156509\n",
            "18 [0.608522, 1.0834162] 1.6919382\n",
            "19 [0.68783253, 0.9825391] 1.6703717\n",
            "20 [0.77941525, 0.90234995] 1.6817652\n",
            "21 [0.843075, 0.8656452] 1.7087202\n",
            "22 [0.8571064, 0.84708875] 1.7041951\n",
            "23 [0.86449516, 0.835911] 1.7004061\n",
            "24 [0.8564016, 0.8371655] 1.693567\n",
            "25 [0.79637, 0.87500024] 1.6713703\n",
            "26 [0.73373586, 0.9233536] 1.6570895\n",
            "27 [0.67990917, 0.97452325] 1.6544324\n",
            "28 [0.6151342, 1.041469] 1.6566031\n",
            "29 [0.5763927, 1.1136652] 1.690058\n",
            "30 [0.5743977, 1.1109706] 1.6853683\n",
            "31 [0.5983179, 1.0903956] 1.6887136\n",
            "32 [0.6408311, 1.0214478] 1.6622789\n",
            "33 [0.70446306, 0.9505451] 1.6550081\n",
            "34 [0.72698045, 0.9281528] 1.6551332\n",
            "35 [0.7677017, 0.8860538] 1.6537554\n",
            "36 [0.7921134, 0.8710891] 1.6632025\n",
            "37 [0.79123235, 0.8685368] 1.659769\n",
            "38 [0.744137, 0.9034595] 1.6475965\n",
            "39 [0.74005127, 0.9077779] 1.6478292\n",
            "40 [0.6869714, 0.95426285] 1.6412343\n",
            "41 [0.6673995, 0.9653595] 1.6327591\n",
            "42 [0.65179414, 0.98252344] 1.6343176\n",
            "43 [0.65294427, 0.97708976] 1.630034\n",
            "44 [0.6385017, 0.98219967] 1.6207013\n",
            "45 [0.6391634, 0.98406035] 1.6232238\n",
            "46 [0.68372566, 0.93315804] 1.6168838\n",
            "47 [0.6830395, 0.9052361] 1.5882757\n",
            "48 [0.6963131, 0.8894712] 1.5857842\n",
            "49 [0.71742094, 0.9008342] 1.6182551\n",
            "50 [0.7129974, 0.87313557] 1.586133\n",
            "51 [0.69620883, 0.9088911] 1.6050999\n",
            "52 [0.7061333, 0.8778396] 1.5839729\n",
            "53 [0.68160963, 0.91230565] 1.5939152\n",
            "54 [0.6690648, 0.89891803] 1.5679829\n",
            "55 [0.62434864, 0.92405206] 1.5484006\n",
            "56 [0.62108874, 0.94642943] 1.5675182\n",
            "57 [0.5866793, 0.9958017] 1.5824809\n",
            "58 [0.587628, 0.9524903] 1.5401183\n",
            "59 [0.6252176, 0.9530074] 1.578225\n",
            "60 [0.68263453, 0.87296015] 1.5555947\n",
            "61 [0.6840391, 0.84531915] 1.5293583\n",
            "62 [0.69261587, 0.8142128] 1.5068287\n",
            "63 [0.6969866, 0.8491622] 1.5461488\n",
            "64 [0.6887475, 0.87310606] 1.5618536\n",
            "65 [0.6477024, 0.8759329] 1.5236353\n",
            "66 [0.6261129, 0.8884901] 1.5146029\n",
            "67 [0.67291796, 0.95253676] 1.6254547\n",
            "68 [0.6486757, 0.89957297] 1.5482486\n",
            "69 [0.6221219, 0.88475573] 1.5068777\n",
            "70 [0.6271043, 0.94650745] 1.5736117\n",
            "71 [0.6347339, 0.95751125] 1.5922451\n",
            "72 [0.65530676, 0.9193135] 1.5746202\n",
            "73 [0.688681, 0.85422474] 1.5429058\n",
            "74 [0.7045968, 0.8629773] 1.5675741\n",
            "75 [0.6992634, 0.8233549] 1.5226183\n",
            "76 [0.7074645, 0.80482113] 1.5122857\n",
            "77 [0.6474152, 0.8533168] 1.500732\n",
            "78 [0.6320784, 0.9259303] 1.5580087\n",
            "79 [0.6135441, 0.9106863] 1.5242305\n",
            "80 [0.63209075, 0.8895099] 1.5216007\n",
            "81 [0.63727254, 0.8789623] 1.5162349\n",
            "82 [0.6392364, 0.8574392] 1.4966756\n",
            "83 [0.66147476, 0.8291525] 1.4906273\n",
            "84 [0.6500156, 0.8301747] 1.4801903\n",
            "85 [0.64865285, 0.88770396] 1.5363568\n",
            "86 [0.63171273, 0.8850577] 1.5167704\n",
            "87 [0.6453654, 0.9352629] 1.5806284\n",
            "88 [0.62165886, 0.87609416] 1.497753\n",
            "89 [0.60646206, 0.87083447] 1.4772966\n",
            "90 [0.63577604, 0.9024359] 1.538212\n",
            "91 [0.6424034, 0.8783158] 1.5207193\n",
            "92 [0.63170016, 0.8460689] 1.4777691\n",
            "93 [0.62734634, 0.8182391] 1.4455855\n",
            "94 [0.6551877, 0.809767] 1.4649547\n",
            "95 [0.6434447, 0.8722107] 1.5156554\n",
            "96 [0.6305392, 0.8428844] 1.4734236\n",
            "97 [0.6842548, 0.87192804] 1.5561829\n",
            "98 [0.6435744, 0.8128009] 1.4563754\n",
            "99 [0.6161925, 0.93376356] 1.5499561\n",
            "100 [0.61759347, 0.99228895] 1.6098824\n",
            "101 [0.6184928, 0.87179214] 1.4902849\n",
            "102 [0.5808889, 0.88695455] 1.4678435\n",
            "103 [0.58143026, 0.910084] 1.4915142\n",
            "104 [0.6104439, 0.8980933] 1.5085372\n",
            "105 [0.6569337, 0.8831253] 1.5400591\n",
            "106 [0.6656218, 0.8026248] 1.4682467\n",
            "107 [0.6917593, 0.8334542] 1.5252135\n",
            "108 [0.69098425, 0.8792222] 1.5702064\n",
            "109 [0.6784895, 0.87833226] 1.5568218\n",
            "110 [0.6519615, 0.9019301] 1.5538917\n",
            "111 [0.59719986, 0.85916996] 1.4563699\n",
            "112 [0.57533246, 0.9302695] 1.5056019\n",
            "113 [0.57648957, 0.9373915] 1.5138811\n",
            "114 [0.6060681, 0.87747043] 1.4835385\n",
            "115 [0.6396912, 0.8675393] 1.5072305\n",
            "116 [0.6846635, 0.8628653] 1.5475287\n",
            "117 [0.70282865, 0.7442906] 1.4471192\n",
            "118 [0.71712273, 0.7390902] 1.456213\n",
            "119 [0.69005865, 0.7723309] 1.4623895\n",
            "120 [0.6948102, 0.79421794] 1.4890282\n",
            "121 [0.6449044, 0.8348636] 1.479768\n",
            "122 [0.60043716, 0.8693924] 1.4698296\n",
            "123 [0.5976394, 0.9487801] 1.5464195\n",
            "124 [0.6029455, 0.9197177] 1.5226632\n",
            "125 [0.60333335, 0.8801887] 1.483522\n",
            "126 [0.6293241, 0.80081713] 1.4301412\n",
            "127 [0.6515975, 0.8435773] 1.4951749\n",
            "128 [0.69088, 0.75630903] 1.4471891\n",
            "129 [0.6844887, 0.775978] 1.4604667\n",
            "130 [0.685506, 0.7929853] 1.4784913\n",
            "131 [0.6578129, 0.8170256] 1.4748385\n",
            "132 [0.62538254, 0.82534105] 1.4507236\n",
            "133 [0.59632486, 0.91272783] 1.5090528\n",
            "134 [0.5829106, 0.8580125] 1.4409231\n",
            "135 [0.57175887, 0.90049183] 1.4722507\n",
            "136 [0.6109234, 0.86011606] 1.4710395\n",
            "137 [0.6525549, 0.84049916] 1.4930542\n",
            "138 [0.64025277, 0.80728316] 1.447536\n",
            "139 [0.6594379, 0.81288046] 1.4723184\n",
            "140 [0.6543879, 0.8238529] 1.4782407\n",
            "141 [0.63552934, 0.8559489] 1.4914782\n",
            "142 [0.627317, 0.8313947] 1.4587116\n",
            "143 [0.5990165, 0.8712241] 1.4702406\n",
            "144 [0.61251116, 0.8483905] 1.4609017\n",
            "145 [0.6404431, 0.8383486] 1.4787917\n",
            "146 [0.66440254, 0.86962795] 1.5340304\n",
            "147 [0.63522243, 0.77740765] 1.4126301\n",
            "148 [0.61858594, 0.83490413] 1.45349\n",
            "149 [0.63815415, 0.8441482] 1.4823024\n",
            "150 [0.6282218, 0.78069156] 1.4089134\n",
            "151 [0.6159923, 0.832492] 1.4484843\n",
            "152 [0.6416622, 0.8097696] 1.4514318\n",
            "153 [0.627787, 0.8169163] 1.4447033\n",
            "154 [0.62933767, 0.8444141] 1.4737518\n",
            "155 [0.6471252, 0.8313726] 1.4784977\n",
            "156 [0.6599996, 0.7845355] 1.4445351\n",
            "157 [0.6669937, 0.75652796] 1.4235216\n",
            "158 [0.6591396, 0.8148722] 1.4740118\n",
            "159 [0.61592627, 0.8255488] 1.4414752\n",
            "160 [0.5846528, 0.8531496] 1.4378023\n",
            "161 [0.58517987, 0.8643695] 1.4495494\n",
            "162 [0.60067743, 0.85687685] 1.4575543\n",
            "163 [0.64346945, 0.7977863] 1.4412558\n",
            "164 [0.6763376, 0.78321767] 1.4595553\n",
            "165 [0.6556118, 0.77132493] 1.4269367\n",
            "166 [0.71112126, 0.9628088] 1.67393\n",
            "167 [0.6394619, 0.89863366] 1.5380955\n",
            "168 [0.59695214, 0.81385803] 1.4108102\n",
            "169 [0.54903173, 0.97743374] 1.5264654\n",
            "170 [0.5599828, 0.98524004] 1.5452228\n",
            "171 [0.61439055, 0.9856769] 1.6000674\n",
            "172 [0.6342853, 0.87732404] 1.5116093\n",
            "173 [0.6569513, 0.8035483] 1.4604995\n",
            "174 [0.679085, 0.78150344] 1.4605885\n",
            "175 [0.716836, 0.8181466] 1.5349826\n",
            "176 [0.7239196, 0.83166105] 1.5555806\n",
            "177 [0.6807612, 0.83684796] 1.5176091\n",
            "178 [0.5798802, 0.9084247] 1.4883049\n",
            "179 [0.5849221, 0.93463856] 1.5195606\n",
            "180 [0.58111906, 0.93561715] 1.5167363\n",
            "181 [0.58704084, 0.8917028] 1.4787436\n",
            "182 [0.6016993, 0.8422014] 1.4439007\n",
            "183 [0.65709126, 0.8633401] 1.5204313\n",
            "184 [0.67884344, 0.8093272] 1.4881706\n",
            "185 [0.71441454, 0.80411494] 1.5185294\n",
            "186 [0.72289836, 0.76838446] 1.4912828\n",
            "187 [0.7074129, 0.75943035] 1.4668432\n",
            "188 [0.68910563, 0.7870923] 1.476198\n",
            "189 [0.626984, 0.84499645] 1.4719805\n",
            "190 [0.62260824, 0.8967478] 1.519356\n",
            "191 [0.5719361, 0.9007609] 1.472697\n",
            "192 [0.58567965, 0.9144412] 1.5001209\n",
            "193 [0.5969916, 0.85213333] 1.4491249\n",
            "194 [0.6294752, 0.8701916] 1.4996667\n",
            "195 [0.625405, 0.8038976] 1.4293027\n",
            "196 [0.6331982, 0.81745815] 1.4506564\n",
            "197 [0.6463222, 0.80526495] 1.4515872\n",
            "198 [0.67717266, 0.8070502] 1.4842229\n",
            "199 [0.70377696, 0.76036704] 1.464144\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vYGXD_yz_dqA"
      },
      "source": [
        "S = preprocess_support(A)\n",
        "X = tf.math.l2_normalize(toTensor(X), axis=1)\n",
        "embs = model.embed(X, S) # node embeddings"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h-k0TCif_gIe"
      },
      "source": [
        "# node clustering using the KMeans algorithm\n",
        "from sklearn.cluster import KMeans\n",
        "y_pred = KMeans(n_clusters=Y.shape[1]).fit(embs).predict(embs)\n",
        "y_true = np.argmax(Y, axis=1)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dVdTJClV_h9A",
        "outputId": "5428363c-48f9-4715-ffee-73eebe95ff77"
      },
      "source": [
        "# result\n",
        "from sklearn.metrics import adjusted_mutual_info_score\n",
        "print(adjusted_mutual_info_score(y_true, y_pred))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.4113758211028403\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}