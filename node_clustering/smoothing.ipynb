{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "smoothing.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zzzB2uJwBC-m",
        "outputId": "f2d2c60e-95bf-4eec-b26c-44565e63cfe1"
      },
      "source": [
        "!sudo apt-get install libmetis-dev\n",
        "!pip install metis\n",
        "import metis\n",
        "import random\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from scipy import sparse\n",
        "import scipy.io as sio"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "libmetis-dev is already the newest version (5.1.0.dfsg-5).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 30 not upgraded.\n",
            "Requirement already satisfied: metis in /usr/local/lib/python3.7/dist-packages (0.2a5)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p-5rUhczBJdV"
      },
      "source": [
        "# hyperparameters\n",
        "hidden = 512 # number of hidden units in the encoder layer\n",
        "latent = 256 # dimension of the latent variables\n",
        "learning_rate = 0.01\n",
        "epochs = 200\n",
        "nparts = 1500 # number of partitions\n",
        "batch_size = 20 # number of clusters per batch"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RflJi-R3BOkG"
      },
      "source": [
        "filename = '/content/drive/MyDrive/GRAPH DATA/reddit.mat' # dataset"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ABjlQkpsDFgx"
      },
      "source": [
        "mat_dict = sio.loadmat(filename)\n",
        "A = mat_dict['A'].ceil()\n",
        "X = mat_dict['X']\n",
        "Y = mat_dict['Y']\n",
        "train_mask = mat_dict['train_mask'].squeeze().astype(bool)\n",
        "val_mask = mat_dict['val_mask'].squeeze().astype(bool)\n",
        "test_mask = mat_dict['test_mask'].squeeze().astype(bool)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a2GhIMjvDNzG",
        "outputId": "97713170-3e7e-4dc9-f2ac-f9b9b612b583"
      },
      "source": [
        "def cluster_graph(A, nparts):\n",
        "  if nparts == 1:\n",
        "    edge_cuts, parts = 0, [0, ] * A.shape[0]\n",
        "  else:\n",
        "    edge_cuts, parts = metis.part_graph([neighbors for neighbors in A.tolil().rows], nparts=nparts)\n",
        "  print('Number of edge cuts: %d.' % edge_cuts)\n",
        "  cluster_dict = {}\n",
        "  for index, part in enumerate(parts):\n",
        "    if part not in cluster_dict:\n",
        "      cluster_dict[part] = []\n",
        "    cluster_dict[part].append(index)\n",
        "  return cluster_dict\n",
        "\n",
        "# the clustering algorithm (METIS)\n",
        "cluster_dict = cluster_graph(A, nparts)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of edge cuts: 9609639.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FstzOA7oDgoL"
      },
      "source": [
        "def preprocess_support(A):\n",
        "  N = A.shape[1]\n",
        "  A_I = A + sparse.eye(N, dtype='float32')\n",
        "  D_I = sparse.csr_matrix(A_I.sum(axis=1))\n",
        "  norm = D_I.power(-0.5)\n",
        "  return A_I.multiply(norm).T.multiply(norm)\n",
        "\n",
        "def toTensorSparse(S):\n",
        "  return tf.constant(S.todense())\n",
        "\n",
        "def toTensor(T):\n",
        "  return tf.constant(T)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OVOs3s5TDrHJ"
      },
      "source": [
        "# layer classes\n",
        "\n",
        "class bilinear_layer:\n",
        "\n",
        "  def __init__(self, indim, outdim):\n",
        "    pass\n",
        "\n",
        "  def __call__(self, tensor):\n",
        "    return tf.linalg.matmul(tensor, tf.transpose(tensor))\n",
        "\n",
        "# unused\n",
        "class FC_layer:\n",
        "\n",
        "  def __init__(self, indim, outdim):\n",
        "    initial_value = tf.initializers.he_normal()((indim, outdim,))\n",
        "    self.weight = tf.Variable(initial_value=initial_value, trainable=True)\n",
        "\n",
        "  def __call__(self, tensor):\n",
        "    return tf.linalg.matmul(tensor, self.weight)\n",
        "\n",
        "class GC_layer:\n",
        "\n",
        "  def __init__(self, indim, outdim):\n",
        "    initial_value = tf.initializers.he_normal()((indim, outdim,))\n",
        "    self.weight = tf.Variable(initial_value=initial_value, trainable=True)\n",
        "\n",
        "  def __call__(self, tensor, support, embed=False):\n",
        "    if embed: # numpy pipeline\n",
        "      return support.dot(tensor.numpy().dot(self.weight.numpy()))\n",
        "    else: # tensorflow pipeline\n",
        "      return tf.linalg.matmul(support, tf.linalg.matmul(tensor, self.weight))\n"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qwEu7WB9EWAI"
      },
      "source": [
        "# our model class (for the paper \"Scalable Graph Variational Autoencoders\")\n",
        "\n",
        "class Model:\n",
        "\n",
        "  def __init__(self, size_tuple, optimizer, nonlinear):\n",
        "    self.sources = [] # variables to optimize\n",
        "    self.build(size_tuple) # builds the model by stacking layers on each other\n",
        "    self.optimizer = optimizer\n",
        "    self.nonlinear = nonlinear\n",
        "    self.Z_mean = None # mean embedding layer\n",
        "    self.Z_var = None # variance embedding layer\n",
        "    self.noise = None # the noise sample\n",
        "    self.sample = None # self.Z_mean + self.Z_var * self.noise\n",
        "    self.A_gamma = None # the reconstructions\n",
        "  \n",
        "  def build(self, size_tuple):\n",
        "    X_dim, hidden, latent = size_tuple\n",
        "    self.enc_layer = GC_layer(X_dim, hidden)\n",
        "    self.enc_mean_layer = GC_layer(hidden, latent)\n",
        "    self.enc_var_layer = GC_layer(hidden, latent)\n",
        "    self.A_dec_gamma_layer = bilinear_layer(latent, latent)\n",
        "    # filling the source array with weights\n",
        "    layers = [self.enc_layer, self.enc_mean_layer, self.enc_var_layer]\n",
        "    for layer in layers:\n",
        "      self.sources.append(layer.weight)\n",
        "  \n",
        "  # forward propagation in the encoder\n",
        "  def encode(self, X, S):\n",
        "    enc = self.nonlinear(self.enc_layer(X, S))\n",
        "    enc_mean = self.enc_mean_layer(enc, S)\n",
        "    enc_var = tf.math.exp(self.enc_var_layer(enc, S))\n",
        "    return enc_mean, enc_var\n",
        "\n",
        "  # returns only the node embeddings\n",
        "  def embed(self, X, S):\n",
        "    enc = self.nonlinear(self.enc_layer(X, S, embed=True))\n",
        "    enc_mean = self.enc_mean_layer(enc, S, embed=True)\n",
        "    return enc_mean\n",
        "\n",
        "  # forward propagation in the decoder\n",
        "  def decode(self, sample):\n",
        "    A_dec_gamma = self.A_dec_gamma_layer(sample)\n",
        "    return A_dec_gamma\n",
        "\n",
        "  def predict(self, X, S):\n",
        "    self.Z_mean, self.Z_var = self.encode(X, S)\n",
        "    self.noise = tf.random.normal(self.Z_var.shape)\n",
        "    self.sample = self.Z_mean + self.Z_var * self.noise # reparameterization trick\n",
        "    self.A_gamma = self.decode(self.sample)\n",
        "\n",
        "  def train(self, X, A, cluster_dict, batch_size, epochs):\n",
        "    for epoch in range(epochs):\n",
        "      # only a subgraph is used in the training process\n",
        "      samples = random.sample(cluster_dict.keys(), batch_size)\n",
        "      nodes = sum([cluster_dict[sample] for sample in samples], [])\n",
        "      S_batch = toTensorSparse(preprocess_support(A[nodes].T[nodes]))\n",
        "      A_batch = toTensor(A.T[nodes].T[nodes].todense())\n",
        "      X_batch = tf.math.l2_normalize(toTensor(X[nodes]), axis=1)\n",
        "      # optimization\n",
        "      with tf.GradientTape() as tape:\n",
        "        self.predict(X_batch, S_batch)\n",
        "        losses = self.loss(A_batch, X_batch)\n",
        "        loss_ = tf.reduce_sum(losses)\n",
        "      print(epoch, [loss.numpy() for loss in losses], loss_.numpy())\n",
        "      grads = tape.gradient(loss_, self.sources)\n",
        "      self.optimizer.apply_gradients(zip(grads, self.sources))\n",
        "\n",
        "  # Kullbackâ€“Leibler divergence\n",
        "  def KL_Divergence(self):\n",
        "    loss = 0.5 * tf.reduce_mean(self.Z_mean**2.0 + self.Z_var**2.0 - 2.0 * tf.math.log(self.Z_var) - 1.0)\n",
        "    return loss\n",
        "\n",
        "  # reconstruction loss\n",
        "  def re_A_loss(self, A):\n",
        "    density = tf.reduce_sum(A) / tf.size(A, out_type=tf.float32)\n",
        "    pos_weight = (1.0 - density) / density\n",
        "    loss = -0.5 * tf.reduce_mean(1.0 / (1.0 - density) * tf.nn.weighted_cross_entropy_with_logits(labels=A, logits=self.A_gamma, pos_weight=pos_weight))\n",
        "    return -loss\n",
        "\n",
        "  # list of all loss functions\n",
        "  def loss(self, A, X):\n",
        "    return self.KL_Divergence(), self.re_A_loss(A)\n"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0zuy3DUXLK4J",
        "outputId": "480a0383-b302-407e-cf12-e0b06f3d1a6f"
      },
      "source": [
        "size_tuple = (X.shape[1], hidden, latent)\n",
        "optimizer = tf.optimizers.Adam(learning_rate=learning_rate)\n",
        "nonlinear = tf.nn.relu\n",
        "\n",
        "model = Model(size_tuple, optimizer, nonlinear)\n",
        "\n",
        "print('Training...')\n",
        "model.train(X, A, cluster_dict, batch_size, epochs)\n"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training...\n",
            "0 [0.0047069215, 6.589947] 6.594654\n",
            "1 [0.021607213, 5.0971265] 5.118734\n",
            "2 [0.07974266, 3.5981739] 3.6779165\n",
            "3 [0.23128651, 2.096803] 2.3280895\n",
            "4 [0.4854414, 1.4227942] 1.9082355\n",
            "5 [0.8196651, 1.0161958] 1.8358608\n",
            "6 [1.1136945, 1.6832622] 2.7969568\n",
            "7 [1.191991, 0.92631495] 2.118306\n",
            "8 [1.2690568, 1.5022756] 2.7713323\n",
            "9 [1.264591, 0.9664697] 2.2310607\n",
            "10 [1.148089, 1.0598824] 2.2079716\n",
            "11 [1.0869949, 0.83456075] 1.9215556\n",
            "12 [0.9579296, 0.8477148] 1.8056444\n",
            "13 [0.7978228, 0.88878906] 1.6866119\n",
            "14 [0.750253, 0.9027708] 1.6530238\n",
            "15 [0.59232956, 0.991692] 1.5840216\n",
            "16 [0.50038934, 1.1714028] 1.6717921\n",
            "17 [0.41803837, 1.3114457] 1.7294841\n",
            "18 [0.3988082, 1.4030156] 1.8018239\n",
            "19 [0.41451365, 1.3779553] 1.792469\n",
            "20 [0.4267816, 1.3047972] 1.7315788\n",
            "21 [0.46418837, 1.2414572] 1.7056456\n",
            "22 [0.52551746, 1.1128404] 1.6383579\n",
            "23 [0.6161648, 1.0142229] 1.6303877\n",
            "24 [0.6882699, 0.92302805] 1.611298\n",
            "25 [0.69762677, 0.90551037] 1.6031371\n",
            "26 [0.7391903, 0.88411605] 1.6233063\n",
            "27 [0.78177667, 0.816186] 1.5979626\n",
            "28 [0.77711487, 0.7826572] 1.559772\n",
            "29 [0.77409816, 0.79583615] 1.5699344\n",
            "30 [0.7886286, 0.81864524] 1.6072738\n",
            "31 [0.76223254, 0.8196535] 1.581886\n",
            "32 [0.76126444, 0.8397595] 1.6010239\n",
            "33 [0.7231265, 0.84367716] 1.5668037\n",
            "34 [0.6815084, 0.8963771] 1.5778855\n",
            "35 [0.6652923, 0.8778714] 1.5431638\n",
            "36 [0.62609893, 0.9104032] 1.5365021\n",
            "37 [0.60450804, 0.96503276] 1.5695407\n",
            "38 [0.60109377, 0.922902] 1.5239958\n",
            "39 [0.60885483, 0.9429701] 1.5518249\n",
            "40 [0.59915394, 0.92015076] 1.5193048\n",
            "41 [0.60634756, 0.9542561] 1.5606036\n",
            "42 [0.6012722, 0.95693344] 1.5582056\n",
            "43 [0.62319595, 0.90462106] 1.527817\n",
            "44 [0.6263664, 0.86959434] 1.4959607\n",
            "45 [0.6376988, 0.88228065] 1.5199795\n",
            "46 [0.6510325, 0.8966838] 1.5477164\n",
            "47 [0.66488695, 0.86498594] 1.5298729\n",
            "48 [0.69706434, 0.84607685] 1.5431411\n",
            "49 [0.69657785, 0.80119014] 1.4977679\n",
            "50 [0.6980999, 0.78632087] 1.4844208\n",
            "51 [0.6737638, 0.83245504] 1.5062189\n",
            "52 [0.65891963, 0.8692693] 1.528189\n",
            "53 [0.64321816, 0.8681118] 1.5113299\n",
            "54 [0.6371613, 0.8508059] 1.4879673\n",
            "55 [0.63125044, 0.89228755] 1.523538\n",
            "56 [0.62981075, 0.89260805] 1.5224187\n",
            "57 [0.6315027, 0.8353071] 1.4668097\n",
            "58 [0.62443703, 0.87237084] 1.4968078\n",
            "59 [0.6478469, 0.9146127] 1.5624596\n",
            "60 [0.6166231, 0.8888256] 1.5054487\n",
            "61 [0.6184899, 0.88281476] 1.5013046\n",
            "62 [0.614515, 0.8566393] 1.4711543\n",
            "63 [0.6359034, 0.8907893] 1.5266926\n",
            "64 [0.6582634, 0.9383262] 1.5965896\n",
            "65 [0.6312187, 0.82776076] 1.4589794\n",
            "66 [0.6349206, 0.87069607] 1.5056167\n",
            "67 [0.6197586, 0.9169158] 1.5366744\n",
            "68 [0.6444455, 0.8939767] 1.5384221\n",
            "69 [0.6587037, 0.8573347] 1.5160384\n",
            "70 [0.6641083, 0.88248545] 1.5465937\n",
            "71 [0.64724094, 0.8527496] 1.4999905\n",
            "72 [0.6473873, 0.82926893] 1.4766562\n",
            "73 [0.62959224, 0.8469047] 1.4764969\n",
            "74 [0.6152507, 0.8719603] 1.487211\n",
            "75 [0.6116913, 0.8590695] 1.4707608\n",
            "76 [0.6741185, 0.8996675] 1.573786\n",
            "77 [0.62592006, 0.87915075] 1.5050708\n",
            "78 [0.650161, 0.8616163] 1.5117774\n",
            "79 [0.6401704, 0.8313676] 1.4715381\n",
            "80 [0.6501343, 0.87052464] 1.520659\n",
            "81 [0.6212091, 1.0098922] 1.6311014\n",
            "82 [0.6501258, 0.82790655] 1.4780324\n",
            "83 [0.6374493, 0.867844] 1.5052934\n",
            "84 [0.6474295, 0.85447025] 1.5018997\n",
            "85 [0.6219636, 0.9324706] 1.5544343\n",
            "86 [0.65332556, 0.94788146] 1.601207\n",
            "87 [0.61126244, 0.862209] 1.4734714\n",
            "88 [0.57748914, 0.9573265] 1.5348155\n",
            "89 [0.6313454, 0.8406488] 1.4719942\n",
            "90 [0.62884885, 0.89474016] 1.523589\n",
            "91 [0.6260124, 0.9115053] 1.5375177\n",
            "92 [0.6540694, 0.85737944] 1.5114489\n",
            "93 [0.6678833, 0.8745366] 1.5424199\n",
            "94 [0.6452338, 0.8543835] 1.4996173\n",
            "95 [0.65184486, 0.8265494] 1.4783943\n",
            "96 [0.6462391, 0.8917158] 1.5379549\n",
            "97 [0.6465312, 0.9115301] 1.5580614\n",
            "98 [0.63674253, 0.83565676] 1.4723992\n",
            "99 [0.6307326, 0.9057523] 1.536485\n",
            "100 [0.631812, 0.8852034] 1.5170155\n",
            "101 [0.615036, 0.87664294] 1.491679\n",
            "102 [0.60352606, 0.9064993] 1.5100254\n",
            "103 [0.6216238, 0.87893426] 1.5005581\n",
            "104 [0.6567538, 0.84388965] 1.5006435\n",
            "105 [0.6601273, 0.878215] 1.5383422\n",
            "106 [0.6674189, 0.88531166] 1.5527306\n",
            "107 [0.65182656, 0.856468] 1.5082946\n",
            "108 [0.6514531, 0.85855156] 1.5100046\n",
            "109 [0.6386895, 0.903779] 1.5424685\n",
            "110 [0.62534505, 0.8915513] 1.5168964\n",
            "111 [0.6369007, 0.8700551] 1.5069559\n",
            "112 [0.63011664, 0.849572] 1.4796886\n",
            "113 [0.65231615, 0.8252734] 1.4775896\n",
            "114 [0.664031, 0.84367615] 1.5077071\n",
            "115 [0.6560377, 0.8889229] 1.5449606\n",
            "116 [0.6384925, 0.7980721] 1.4365647\n",
            "117 [0.65137535, 0.7873342] 1.4387095\n",
            "118 [0.6254748, 0.8253617] 1.4508365\n",
            "119 [0.6481303, 0.88812685] 1.5362571\n",
            "120 [0.6182128, 0.89249814] 1.510711\n",
            "121 [0.61769265, 0.84125364] 1.4589462\n",
            "122 [0.59785646, 0.8905501] 1.4884065\n",
            "123 [0.59909517, 0.9259496] 1.5250447\n",
            "124 [0.633868, 0.8557709] 1.4896388\n",
            "125 [0.6084609, 0.85277164] 1.4612325\n",
            "126 [0.6264306, 0.8295226] 1.4559531\n",
            "127 [0.62868464, 0.8240725] 1.4527571\n",
            "128 [0.633402, 0.86550725] 1.4989092\n",
            "129 [0.62875545, 0.87248385] 1.5012393\n",
            "130 [0.61231166, 0.86070126] 1.4730129\n",
            "131 [0.61227643, 0.8592702] 1.4715466\n",
            "132 [0.6102336, 0.88782156] 1.4980552\n",
            "133 [0.62850124, 0.8494432] 1.4779444\n",
            "134 [0.6154689, 0.85602736] 1.4714963\n",
            "135 [0.64659363, 0.8096678] 1.4562614\n",
            "136 [0.65416974, 0.8056939] 1.4598637\n",
            "137 [0.6541241, 0.8474067] 1.5015308\n",
            "138 [0.6613178, 0.9023897] 1.5637076\n",
            "139 [0.6586228, 0.78061956] 1.4392424\n",
            "140 [0.65525514, 0.8104112] 1.4656663\n",
            "141 [0.6274418, 0.8347262] 1.462168\n",
            "142 [0.61348885, 0.8362884] 1.4497772\n",
            "143 [0.64234215, 0.8942052] 1.5365474\n",
            "144 [0.62387234, 0.8180997] 1.441972\n",
            "145 [0.63798845, 0.8444834] 1.4824718\n",
            "146 [0.62162215, 0.90313214] 1.5247543\n",
            "147 [0.59752625, 0.9895012] 1.5870274\n",
            "148 [0.6037381, 0.8588486] 1.4625866\n",
            "149 [0.6179379, 0.8448143] 1.4627522\n",
            "150 [0.61762345, 0.85450935] 1.4721328\n",
            "151 [0.63942105, 0.8255646] 1.4649856\n",
            "152 [0.641355, 0.8027042] 1.4440591\n",
            "153 [0.6351743, 0.86353356] 1.4987078\n",
            "154 [0.64844376, 0.86410224] 1.5125461\n",
            "155 [0.6355362, 0.84397995] 1.4795161\n",
            "156 [0.63606304, 0.8306693] 1.4667323\n",
            "157 [0.6279104, 0.8154078] 1.4433181\n",
            "158 [0.6279224, 0.82276577] 1.4506881\n",
            "159 [0.6469211, 0.83309597] 1.4800171\n",
            "160 [0.6198802, 0.8226678] 1.442548\n",
            "161 [0.6281636, 0.8533421] 1.4815056\n",
            "162 [0.6341278, 0.8507384] 1.4848661\n",
            "163 [0.646145, 0.7972178] 1.4433627\n",
            "164 [0.6455255, 0.7746062] 1.4201317\n",
            "165 [0.6530142, 0.8184546] 1.4714688\n",
            "166 [0.64245534, 0.81130004] 1.4537554\n",
            "167 [0.63976717, 0.8385791] 1.4783463\n",
            "168 [0.61855894, 0.86013967] 1.4786986\n",
            "169 [0.6047578, 0.8842804] 1.4890382\n",
            "170 [0.6150543, 0.8976896] 1.512744\n",
            "171 [0.5931847, 0.892829] 1.4860137\n",
            "172 [0.6001074, 0.8633228] 1.4634302\n",
            "173 [0.6520131, 0.843154] 1.4951671\n",
            "174 [0.6685918, 0.83683264] 1.5054245\n",
            "175 [0.650487, 0.8440979] 1.4945849\n",
            "176 [0.63301533, 0.88399386] 1.5170093\n",
            "177 [0.5979119, 0.8427913] 1.4407032\n",
            "178 [0.59546584, 0.92663515] 1.5221009\n",
            "179 [0.6132568, 0.93000555] 1.5432624\n",
            "180 [0.56529695, 0.9558214] 1.5211184\n",
            "181 [0.5903754, 0.87494284] 1.4653182\n",
            "182 [0.63429254, 0.80364984] 1.4379424\n",
            "183 [0.66719806, 0.81179214] 1.4789902\n",
            "184 [0.6900954, 0.7538102] 1.4439056\n",
            "185 [0.7309178, 0.7965475] 1.5274653\n",
            "186 [0.71506935, 0.7796071] 1.4946765\n",
            "187 [0.68448514, 0.81730527] 1.5017904\n",
            "188 [0.64299303, 0.8691885] 1.5121815\n",
            "189 [0.5926648, 0.86442155] 1.4570863\n",
            "190 [0.540914, 1.0078721] 1.5487862\n",
            "191 [0.56078213, 0.9059855] 1.4667675\n",
            "192 [0.5893225, 0.8959743] 1.4852967\n",
            "193 [0.62927705, 0.85928] 1.4885571\n",
            "194 [0.68578064, 0.8074286] 1.4932092\n",
            "195 [0.7132663, 0.73375505] 1.4470214\n",
            "196 [0.7039349, 0.78989846] 1.4938333\n",
            "197 [0.7008149, 0.7580831] 1.4588981\n",
            "198 [0.6630868, 0.81372267] 1.4768095\n",
            "199 [0.6264561, 0.8130096] 1.4394658\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hxpze7Au9Ool"
      },
      "source": [
        "S = preprocess_support(A)\n",
        "X = tf.math.l2_normalize(toTensor(X), axis=1)\n",
        "embs = model.embed(X, S) # node embeddings"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X2MMEiuO9cf_"
      },
      "source": [
        "# node clustering using the KMeans algorithm\n",
        "from sklearn.cluster import KMeans\n",
        "y_pred = KMeans(n_clusters=Y.shape[1]).fit(embs).predict(embs)\n",
        "y_true = np.argmax(Y, axis=1)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DSzmXSij9jVG",
        "outputId": "b2e2d92a-79ff-47af-c6fc-56aa08ba43fe"
      },
      "source": [
        "# result\n",
        "from sklearn.metrics import adjusted_mutual_info_score\n",
        "print(adjusted_mutual_info_score(y_true, y_pred))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.44621126475467937\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}